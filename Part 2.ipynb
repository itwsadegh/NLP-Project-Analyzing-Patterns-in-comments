{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9vyzTKv2fP-"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xrFqINlWn6L",
        "outputId": "c3950116-a38f-40c0-d7f8-5570f0e27fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.25.2)\n",
            "Collecting boto3 (from pytorch-transformers)\n",
            "  Downloading boto3-1.34.102-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (0.1.99)\n",
            "Collecting sacremoses (from pytorch-transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting botocore<1.35.0,>=1.34.102 (from boto3->pytorch-transformers)\n",
            "  Downloading botocore-1.34.102-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.102->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.102->boto3->pytorch-transformers) (1.16.0)\n",
            "Installing collected packages: sacremoses, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.34.102 botocore-1.34.102 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-transformers-1.2.0 s3transfer-0.10.1 sacremoses-0.1.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "!pip install psutil\n",
        "!pip install pytorch-transformers\n",
        "!pip install transformers\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install torch torchvision  torchaudio -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX9nk8Ndfr3i",
        "outputId": "dc9bb0ca-3d35-4d6b-e0f6-edf1b42b86d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-kmH86lfyBN",
        "outputId": "1fdf2c22-f0c9-4bb1-fd7a-53088330490e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkEdOk1Rh2eN"
      },
      "source": [
        "## Load and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr2Zgp_QWNFP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('data_supervised.csv')\n",
        "#df = pd.read_csv('df_lemmatized.csv',keep_default_na=False)\n",
        "gender_label = pd.read_csv('target_supervised.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vs5Q-l5_g0tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3_f9onlWRir",
        "outputId": "0543b2c4-d177-492f-894f-f026f7d44b0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from string import punctuation\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def initial_clean(text):\n",
        "      # lowercasing\n",
        "      text = str(text).lower()\n",
        "      # fix contractions\n",
        "      text = contractions.fix(text)\n",
        "      # remove urls\n",
        "      url_pattern = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
        "      text = re.sub(url_pattern, \"\", text)\n",
        "      # remove punctuations\n",
        "      ##text  = \"\".join([char if char not in string.punctuation else \" \" for char in text])\n",
        "      return text\n",
        "\n",
        "#The goal of lemmatization is to normalize words so that variations of the same word\n",
        "#are treated as the same word for analysis purposes.\n",
        "\n",
        "def lemmatize(text_tokens):\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in text_tokens if token.isalpha() and token not in stopword ]\n",
        "    return tokens\n",
        "\n",
        "def clean(text):\n",
        "    if text==\"\":\n",
        "        return text\n",
        "    text = initial_clean(text)\n",
        "    # tokenization\n",
        "    text_tokens = word_tokenize(text)\n",
        "    text_tokens = lemmatize(text_tokens)\n",
        "    text = ' '.join(text_tokens)\n",
        "    # remove extra spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "df['cleaned_comment'] = df['body'].apply(lambda row: clean(row))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEkC18YqpshA",
        "outputId": "786e1c6e-5ded-4bbb-abce-3ea89981edf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 296042 entries, 0 to 296041\n",
            "Data columns (total 4 columns):\n",
            " #   Column           Non-Null Count   Dtype  \n",
            "---  ------           --------------   -----  \n",
            " 0   author           296042 non-null  object \n",
            " 1   subreddit        296042 non-null  object \n",
            " 2   created_utc      296042 non-null  float64\n",
            " 3   cleaned_comment  296042 non-null  object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 9.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df.drop(columns=['body'],inplace=True)\n",
        "df.to_csv('df_lemmatized.csv', index=False)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzgenptsY9PN"
      },
      "source": [
        "## Section4.2 - Subreddit Matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOfkCIe7cbfc"
      },
      "outputs": [],
      "source": [
        "grouped_comments= df.groupby('author')['cleaned_comment'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "grouped_comments_labeled = pd.merge(grouped_comments, gender_label ,on=\"author\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txFHgxeDWRZ5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def get_subreddit_matrix(df, grouped_comments):\n",
        "  # Create a list of unique users and subreddits\n",
        "  unique_authors = grouped_comments['author']\n",
        "  unique_subreddits = df['subreddit'].unique()\n",
        "\n",
        "  author_row_map = {author: idx for idx, author in enumerate(unique_authors)}\n",
        "  # Initialize a user-subreddit matrix\n",
        "  author_subreddit_matrix = np.zeros((len(unique_authors), len(unique_subreddits)))\n",
        "\n",
        "  # Populate the matrix with the number of times each user has posted in each subreddit\n",
        "  for _, row in df.iterrows():\n",
        "      author_index = author_row_map[row['author']]\n",
        "      subreddit_index = np.where(unique_subreddits == row['subreddit'])[0][0]\n",
        "      author_subreddit_matrix[author_index, subreddit_index] += 1\n",
        "\n",
        "  # Normalize each row of the matrix\n",
        "  author_subreddit_matrix_normalized = author_subreddit_matrix / author_subreddit_matrix.sum(axis=1, keepdims=True)\n",
        "  return author_row_map, author_subreddit_matrix_normalized\n",
        "\n",
        "author_row_map, author_subreddit_matrix_normalized = get_subreddit_matrix(df,grouped_comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLLDX37G512X",
        "outputId": "11992e18-f5f9-485a-e6ee-8d61ab08027f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "806\n"
          ]
        }
      ],
      "source": [
        "# Determine the number of components required\n",
        "svd = TruncatedSVD(n_components=1000)\n",
        "svd.fit(author_subreddit_matrix_normalized)\n",
        "\n",
        "# Access the explained variance ratio for each component\n",
        "explained_variance_ratio = svd.explained_variance_ratio_\n",
        "\n",
        "# Compute the cumulative explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "n_components = np.argmax(cumulative_variance_ratio > 0.95) + 5\n",
        "print(n_components)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8aYr5nb6EM8",
        "outputId": "50f5649c-6e95-4661-aa00-79c88223f53f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9503154189596873"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svd = TruncatedSVD(n_components=n_components)\n",
        "author_subreddit_matrix_reduced = svd.fit_transform(author_subreddit_matrix_normalized)\n",
        "svd.explained_variance_ratio_.cumsum()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeKIp6oAlG4_"
      },
      "outputs": [],
      "source": [
        "#np.save('author_subreddit_matrix_reduced.npy',author_subreddit_matrix_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukNnQPDZ-Tk1"
      },
      "outputs": [],
      "source": [
        "# Load saved matrix\n",
        "author_row_map = {author: idx for idx, author in enumerate(grouped_comments['author'])}\n",
        "author_subreddit_matrix_reduced = np.load('author_subreddit_matrix_reduced.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngH08qCUY7lm"
      },
      "source": [
        "## Section4.3 Encode and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az5YKZg3SKP0",
        "outputId": "ae9e14b2-6d88-4c5b-8a3b-3d79bc5f0b23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [22:16<00:00,  1.34s/it]\n",
            "100%|██████████| 1000/1000 [21:47<00:00,  1.31s/it]\n",
            "100%|██████████| 1000/1000 [21:35<00:00,  1.30s/it]\n",
            "100%|██████████| 1000/1000 [21:34<00:00,  1.29s/it]\n",
            "100%|██████████| 1000/1000 [21:33<00:00,  1.29s/it]\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "def embed_text(df, max_length=512):\n",
        "    #tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "    #model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    encoded_texts = []\n",
        "    for text in tqdm(df['cleaned_comment']):\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True\n",
        "          )\n",
        "        with torch.no_grad():\n",
        "          outputs = model(**inputs)\n",
        "        # embedding of cls\n",
        "        #sentence_embedding = outputs[0][:,0,:].numpy()\n",
        "        # mean pooling\n",
        "        sentence_embedding = mean_pooling(outputs, inputs['attention_mask'])\n",
        "        encoded_texts.append(sentence_embedding)\n",
        "        del inputs, outputs\n",
        "        gc.collect()\n",
        "    return encoded_texts\n",
        "\n",
        "sentence_embeddings_all = []\n",
        "for i in range(0, 5000, 1000):\n",
        "  sentence_embeddings = np.array(embed_text(grouped_comments_labeled[i:i+1000]))\n",
        "  np.savez_compressed(f'sentence_embeddings_{i}.npz',sentence_embeddings)\n",
        "  sentence_embeddings_all.append(sentence_embeddings)\n",
        "\n",
        "#NOTE: Due to RAM limitation we've splitted the data, embedded each split and then concatenated them all\n",
        "#NOTE: DistillBert is smaller and faster version of BERT model with 97% of BERT's performance\n",
        "#NOTE: we've tried both mean_pooling and [cls] embedding to encode the text. The former one outperformed the latter one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q23YGqqmbD5T"
      },
      "outputs": [],
      "source": [
        "#embeddings_all = np.concatenate(tuple(sentence_embeddings_all))\n",
        "#embeddings_all = embeddings_all.squeeze()\n",
        "#np.savez_compressed(f'embeddings_all.npz',embeddings_all)\n",
        "embeddings_all=np.load('embeddings_all.npz')['arr_0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he3T_HUHgwoz",
        "outputId": "33374700-3f2d-4f7a-e4c6-2d853853a43e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 768)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_all.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QsbND3-Txwq"
      },
      "outputs": [],
      "source": [
        "input_matrix = np.concatenate((author_subreddit_matrix_reduced,embeddings_all),axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsAWaiNmzp5Y"
      },
      "source": [
        "### Split Train Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aGMj0bMfSgb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "combined_data = list(zip(grouped_comments_labeled.values, input_matrix))\n",
        "\n",
        "# Split the combined data into training and testing sets\n",
        "train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Unzip the training and testing sets into separate DataFrames and TF-IDF matrices\n",
        "train_df, X_train = zip(*train_data)\n",
        "train_df = pd.DataFrame(train_df, columns=grouped_comments_labeled.columns)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_df['gender'])\n",
        "\n",
        "test_df, X_test = zip(*test_data)\n",
        "test_df = pd.DataFrame(test_df, columns=grouped_comments_labeled.columns)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(test_df['gender'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXChTK2I0l9I"
      },
      "source": [
        "## Section4.4 Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbX61mwHdGr9"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIynm0UjbecJ",
        "outputId": "e7d2b6d3-ee7b-46a1-e366-c1af506d9a09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import  cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "logit = LogisticRegression(C=1, max_iter=100,n_jobs=4)\n",
        "logit.fit(X_train, y_train)\n",
        "cv_scores = cross_val_score(logit, X_train, y_train, cv=5, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQh_vXxhdsR3",
        "outputId": "09d17548-5ca9-4c8a-f503-1acc6d29d1cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8424817872753764"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = logit.predict(X_test)\n",
        "roc_auc_score(y_test, logit.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05LXgAmxiNGj",
        "outputId": "1b689c67-5a6c-425a-bdaf-a67624a7291f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Memory used before clearing: 6597.01953125 MB\n",
            "Memory used after clearing: 6597.05859375 MB\n"
          ]
        }
      ],
      "source": [
        "!pip install psutil\n",
        "\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "\n",
        "def clear_memory():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Memory used before clearing:\", process.memory_info().rss / 1024 / 1024, \"MB\")\n",
        "    gc.collect()\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Memory used after clearing:\", process.memory_info().rss / 1024 / 1024, \"MB\")\n",
        "\n",
        "# Call this function whenever you want to clear memory\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlIqB3SLap1Z"
      },
      "source": [
        "### Base"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import keras_tuner\n",
        "\n",
        "from keras import regularizers\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam,SGD,RMSprop\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "METRICS = [\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "# Balancing class weights\n",
        "initial_bias = np.log([sum(y_train)/(len(y_train)-sum(y_train))])\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "        Dense(128, activation='relu',input_shape=(input_matrix.shape[1],)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid',bias_initializer=keras.initializers.Constant(initial_bias))\n",
        "    ])\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(5e-5), loss='binary_crossentropy', metrics=[keras.metrics.AUC(name='auc')])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-d4_guGRb-oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obumeWkeXIDf",
        "outputId": "d1a65b50-2092-47e4-8d1d-78342037832d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 1.6474 - accuracy: 0.7267 - auc: 0.5235 - prc: 0.2881\n",
            "Epoch: 1\n",
            "Train Loss: 1.6460988521575928\n",
            "Val Loss: 1.4016119241714478\n",
            "Train Accuracy: 0.7269444465637207\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 10s 14ms/step - loss: 1.6461 - accuracy: 0.7269 - auc: 0.5224 - prc: 0.2865 - val_loss: 1.4016 - val_accuracy: 0.7800 - val_auc: 0.5784 - val_prc: 0.3167\n",
            "Epoch 2/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 1.5710 - accuracy: 0.6904 - auc: 0.5119 - prc: 0.2824\n",
            "Epoch: 2\n",
            "Train Loss: 1.5700352191925049\n",
            "Val Loss: 1.4258160591125488\n",
            "Train Accuracy: 0.690833330154419\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 1.5700 - accuracy: 0.6908 - auc: 0.5114 - prc: 0.2813 - val_loss: 1.4258 - val_accuracy: 0.7800 - val_auc: 0.5831 - val_prc: 0.3426\n",
            "Epoch 3/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 1.5259 - accuracy: 0.6527 - auc: 0.5374 - prc: 0.2952\n",
            "Epoch: 3\n",
            "Train Loss: 1.524178147315979\n",
            "Val Loss: 1.4372930526733398\n",
            "Train Accuracy: 0.6527777910232544\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 6s 14ms/step - loss: 1.5242 - accuracy: 0.6528 - auc: 0.5376 - prc: 0.2938 - val_loss: 1.4373 - val_accuracy: 0.7850 - val_auc: 0.5966 - val_prc: 0.3574\n",
            "Epoch 4/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 1.4888 - accuracy: 0.6194 - auc: 0.5560 - prc: 0.3019\n",
            "Epoch: 4\n",
            "Train Loss: 1.4895156621932983\n",
            "Val Loss: 1.4330118894577026\n",
            "Train Accuracy: 0.6183333396911621\n",
            "Val Accuracy: 0.7149999737739563\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 1.4895 - accuracy: 0.6183 - auc: 0.5550 - prc: 0.3017 - val_loss: 1.4330 - val_accuracy: 0.7150 - val_auc: 0.6151 - val_prc: 0.3896\n",
            "Epoch 5/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 1.4718 - accuracy: 0.5882 - auc: 0.5441 - prc: 0.3087\n",
            "Epoch: 5\n",
            "Train Loss: 1.4702138900756836\n",
            "Val Loss: 1.4164294004440308\n",
            "Train Accuracy: 0.5891666412353516\n",
            "Val Accuracy: 0.7250000238418579\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 1.4702 - accuracy: 0.5892 - auc: 0.5450 - prc: 0.3086 - val_loss: 1.4164 - val_accuracy: 0.7250 - val_auc: 0.6379 - val_prc: 0.4160\n",
            "Epoch 6/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.4452 - accuracy: 0.6020 - auc: 0.5608 - prc: 0.3161\n",
            "Epoch: 6\n",
            "Train Loss: 1.4456967115402222\n",
            "Val Loss: 1.404698371887207\n",
            "Train Accuracy: 0.601111114025116\n",
            "Val Accuracy: 0.6700000166893005\n",
            "450/450 [==============================] - 5s 10ms/step - loss: 1.4457 - accuracy: 0.6011 - auc: 0.5596 - prc: 0.3154 - val_loss: 1.4047 - val_accuracy: 0.6700 - val_auc: 0.6466 - val_prc: 0.4251\n",
            "Epoch 7/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.4209 - accuracy: 0.6062 - auc: 0.5737 - prc: 0.3272\n",
            "Epoch: 7\n",
            "Train Loss: 1.419724702835083\n",
            "Val Loss: 1.3837876319885254\n",
            "Train Accuracy: 0.6072221994400024\n",
            "Val Accuracy: 0.6875\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 1.4197 - accuracy: 0.6072 - auc: 0.5749 - prc: 0.3272 - val_loss: 1.3838 - val_accuracy: 0.6875 - val_auc: 0.6606 - val_prc: 0.4290\n",
            "Epoch 8/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.4032 - accuracy: 0.5942 - auc: 0.5668 - prc: 0.3321\n",
            "Epoch: 8\n",
            "Train Loss: 1.4038350582122803\n",
            "Val Loss: 1.3622277975082397\n",
            "Train Accuracy: 0.5933333039283752\n",
            "Val Accuracy: 0.7124999761581421\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 1.4038 - accuracy: 0.5933 - auc: 0.5653 - prc: 0.3312 - val_loss: 1.3622 - val_accuracy: 0.7125 - val_auc: 0.6725 - val_prc: 0.4424\n",
            "Epoch 9/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 1.3834 - accuracy: 0.6055 - auc: 0.5760 - prc: 0.3315\n",
            "Epoch: 9\n",
            "Train Loss: 1.3827821016311646\n",
            "Val Loss: 1.3468016386032104\n",
            "Train Accuracy: 0.6058333516120911\n",
            "Val Accuracy: 0.6725000143051147\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 1.3828 - accuracy: 0.6058 - auc: 0.5761 - prc: 0.3311 - val_loss: 1.3468 - val_accuracy: 0.6725 - val_auc: 0.6848 - val_prc: 0.4522\n",
            "Epoch 10/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 1.3639 - accuracy: 0.5985 - auc: 0.5747 - prc: 0.3444\n",
            "Epoch: 10\n",
            "Train Loss: 1.3629525899887085\n",
            "Val Loss: 1.330847978591919\n",
            "Train Accuracy: 0.5986111164093018\n",
            "Val Accuracy: 0.6474999785423279\n",
            "450/450 [==============================] - 6s 13ms/step - loss: 1.3630 - accuracy: 0.5986 - auc: 0.5748 - prc: 0.3435 - val_loss: 1.3308 - val_accuracy: 0.6475 - val_auc: 0.6892 - val_prc: 0.4484\n",
            "Epoch 11/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.3411 - accuracy: 0.6037 - auc: 0.5849 - prc: 0.3434\n",
            "Epoch: 11\n",
            "Train Loss: 1.3418530225753784\n",
            "Val Loss: 1.3092689514160156\n",
            "Train Accuracy: 0.6038888692855835\n",
            "Val Accuracy: 0.6625000238418579\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 1.3419 - accuracy: 0.6039 - auc: 0.5843 - prc: 0.3458 - val_loss: 1.3093 - val_accuracy: 0.6625 - val_auc: 0.6974 - val_prc: 0.4514\n",
            "Epoch 12/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.3189 - accuracy: 0.5969 - auc: 0.5984 - prc: 0.3619\n",
            "Epoch: 12\n",
            "Train Loss: 1.3188626766204834\n",
            "Val Loss: 1.2844549417495728\n",
            "Train Accuracy: 0.5969444513320923\n",
            "Val Accuracy: 0.6974999904632568\n",
            "450/450 [==============================] - 5s 12ms/step - loss: 1.3189 - accuracy: 0.5969 - auc: 0.5984 - prc: 0.3619 - val_loss: 1.2845 - val_accuracy: 0.6975 - val_auc: 0.7025 - val_prc: 0.4466\n",
            "Epoch 13/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 1.3007 - accuracy: 0.6205 - auc: 0.6093 - prc: 0.3602\n",
            "Epoch: 13\n",
            "Train Loss: 1.3006815910339355\n",
            "Val Loss: 1.266108751296997\n",
            "Train Accuracy: 0.6202777624130249\n",
            "Val Accuracy: 0.7024999856948853\n",
            "450/450 [==============================] - 6s 13ms/step - loss: 1.3007 - accuracy: 0.6203 - auc: 0.6094 - prc: 0.3607 - val_loss: 1.2661 - val_accuracy: 0.7025 - val_auc: 0.7063 - val_prc: 0.4556\n",
            "Epoch 14/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 1.2755 - accuracy: 0.6312 - auc: 0.6283 - prc: 0.3927\n",
            "Epoch: 14\n",
            "Train Loss: 1.2771745920181274\n",
            "Val Loss: 1.2562369108200073\n",
            "Train Accuracy: 0.6286110877990723\n",
            "Val Accuracy: 0.6575000286102295\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 1.2772 - accuracy: 0.6286 - auc: 0.6263 - prc: 0.3887 - val_loss: 1.2562 - val_accuracy: 0.6575 - val_auc: 0.7096 - val_prc: 0.4572\n",
            "Epoch 15/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 1.2609 - accuracy: 0.6323 - auc: 0.6313 - prc: 0.3746\n",
            "Epoch: 15\n",
            "Train Loss: 1.2608284950256348\n",
            "Val Loss: 1.2404061555862427\n",
            "Train Accuracy: 0.6316666603088379\n",
            "Val Accuracy: 0.6399999856948853\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 1.2608 - accuracy: 0.6317 - auc: 0.6306 - prc: 0.3727 - val_loss: 1.2404 - val_accuracy: 0.6400 - val_auc: 0.7145 - val_prc: 0.4604\n",
            "Epoch 16/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 1.2530 - accuracy: 0.6152 - auc: 0.6042 - prc: 0.3693\n",
            "Epoch: 16\n",
            "Train Loss: 1.2525978088378906\n",
            "Val Loss: 1.214807152748108\n",
            "Train Accuracy: 0.6150000095367432\n",
            "Val Accuracy: 0.6924999952316284\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 1.2526 - accuracy: 0.6150 - auc: 0.6044 - prc: 0.3686 - val_loss: 1.2148 - val_accuracy: 0.6925 - val_auc: 0.7200 - val_prc: 0.4515\n",
            "Epoch 17/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.2309 - accuracy: 0.6368 - auc: 0.6235 - prc: 0.3841\n",
            "Epoch: 17\n",
            "Train Loss: 1.2301167249679565\n",
            "Val Loss: 1.1996235847473145\n",
            "Train Accuracy: 0.6366666555404663\n",
            "Val Accuracy: 0.6899999976158142\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 1.2301 - accuracy: 0.6367 - auc: 0.6246 - prc: 0.3840 - val_loss: 1.1996 - val_accuracy: 0.6900 - val_auc: 0.7242 - val_prc: 0.4588\n",
            "Epoch 18/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 1.2171 - accuracy: 0.6258 - auc: 0.6195 - prc: 0.3717\n",
            "Epoch: 18\n",
            "Train Loss: 1.2179458141326904\n",
            "Val Loss: 1.1791003942489624\n",
            "Train Accuracy: 0.6255555748939514\n",
            "Val Accuracy: 0.699999988079071\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.2179 - accuracy: 0.6256 - auc: 0.6186 - prc: 0.3709 - val_loss: 1.1791 - val_accuracy: 0.7000 - val_auc: 0.7253 - val_prc: 0.4471\n",
            "Epoch 19/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.1889 - accuracy: 0.6485 - auc: 0.6576 - prc: 0.4113\n",
            "Epoch: 19\n",
            "Train Loss: 1.1890267133712769\n",
            "Val Loss: 1.1637530326843262\n",
            "Train Accuracy: 0.6474999785423279\n",
            "Val Accuracy: 0.6949999928474426\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.1890 - accuracy: 0.6475 - auc: 0.6577 - prc: 0.4092 - val_loss: 1.1638 - val_accuracy: 0.6950 - val_auc: 0.7262 - val_prc: 0.4422\n",
            "Epoch 20/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.1711 - accuracy: 0.6506 - auc: 0.6640 - prc: 0.4322\n",
            "Epoch: 20\n",
            "Train Loss: 1.1711150407791138\n",
            "Val Loss: 1.1528140306472778\n",
            "Train Accuracy: 0.6505555510520935\n",
            "Val Accuracy: 0.6825000047683716\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.1711 - accuracy: 0.6506 - auc: 0.6640 - prc: 0.4322 - val_loss: 1.1528 - val_accuracy: 0.6825 - val_auc: 0.7344 - val_prc: 0.4662\n",
            "Epoch 21/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 1.1621 - accuracy: 0.6509 - auc: 0.6566 - prc: 0.4046\n",
            "Epoch: 21\n",
            "Train Loss: 1.1618014574050903\n",
            "Val Loss: 1.1378170251846313\n",
            "Train Accuracy: 0.6511111259460449\n",
            "Val Accuracy: 0.6800000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.1618 - accuracy: 0.6511 - auc: 0.6571 - prc: 0.4053 - val_loss: 1.1378 - val_accuracy: 0.6800 - val_auc: 0.7374 - val_prc: 0.4677\n",
            "Epoch 22/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.1340 - accuracy: 0.6555 - auc: 0.6837 - prc: 0.4557\n",
            "Epoch: 22\n",
            "Train Loss: 1.135323405265808\n",
            "Val Loss: 1.1227682828903198\n",
            "Train Accuracy: 0.6555555462837219\n",
            "Val Accuracy: 0.6775000095367432\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 1.1353 - accuracy: 0.6556 - auc: 0.6838 - prc: 0.4583 - val_loss: 1.1228 - val_accuracy: 0.6775 - val_auc: 0.7396 - val_prc: 0.4625\n",
            "Epoch 23/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 1.1343 - accuracy: 0.6543 - auc: 0.6593 - prc: 0.4191\n",
            "Epoch: 23\n",
            "Train Loss: 1.1344553232192993\n",
            "Val Loss: 1.1060025691986084\n",
            "Train Accuracy: 0.6547222137451172\n",
            "Val Accuracy: 0.6899999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 1.1345 - accuracy: 0.6547 - auc: 0.6594 - prc: 0.4188 - val_loss: 1.1060 - val_accuracy: 0.6900 - val_auc: 0.7409 - val_prc: 0.4593\n",
            "Epoch 24/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 1.1112 - accuracy: 0.6604 - auc: 0.6835 - prc: 0.4499\n",
            "Epoch: 24\n",
            "Train Loss: 1.110528588294983\n",
            "Val Loss: 1.0945051908493042\n",
            "Train Accuracy: 0.6616666913032532\n",
            "Val Accuracy: 0.6825000047683716\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.1105 - accuracy: 0.6617 - auc: 0.6850 - prc: 0.4539 - val_loss: 1.0945 - val_accuracy: 0.6825 - val_auc: 0.7444 - val_prc: 0.4715\n",
            "Epoch 25/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 1.1002 - accuracy: 0.6688 - auc: 0.6818 - prc: 0.4440\n",
            "Epoch: 25\n",
            "Train Loss: 1.1003979444503784\n",
            "Val Loss: 1.0815842151641846\n",
            "Train Accuracy: 0.6683333516120911\n",
            "Val Accuracy: 0.675000011920929\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.1004 - accuracy: 0.6683 - auc: 0.6812 - prc: 0.4421 - val_loss: 1.0816 - val_accuracy: 0.6750 - val_auc: 0.7456 - val_prc: 0.4699\n",
            "Epoch 26/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 1.0798 - accuracy: 0.6673 - auc: 0.7012 - prc: 0.4734\n",
            "Epoch: 26\n",
            "Train Loss: 1.0803282260894775\n",
            "Val Loss: 1.0709623098373413\n",
            "Train Accuracy: 0.6663888692855835\n",
            "Val Accuracy: 0.6800000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.0803 - accuracy: 0.6664 - auc: 0.6995 - prc: 0.4700 - val_loss: 1.0710 - val_accuracy: 0.6800 - val_auc: 0.7479 - val_prc: 0.4751\n",
            "Epoch 27/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 1.0745 - accuracy: 0.6690 - auc: 0.6927 - prc: 0.4442\n",
            "Epoch: 27\n",
            "Train Loss: 1.0734527111053467\n",
            "Val Loss: 1.057843804359436\n",
            "Train Accuracy: 0.6686111092567444\n",
            "Val Accuracy: 0.675000011920929\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 1.0735 - accuracy: 0.6686 - auc: 0.6934 - prc: 0.4447 - val_loss: 1.0578 - val_accuracy: 0.6750 - val_auc: 0.7513 - val_prc: 0.4868\n",
            "Epoch 28/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 1.0555 - accuracy: 0.6767 - auc: 0.7085 - prc: 0.4563\n",
            "Epoch: 28\n",
            "Train Loss: 1.0567917823791504\n",
            "Val Loss: 1.0377823114395142\n",
            "Train Accuracy: 0.676111102104187\n",
            "Val Accuracy: 0.6949999928474426\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 1.0568 - accuracy: 0.6761 - auc: 0.7077 - prc: 0.4565 - val_loss: 1.0378 - val_accuracy: 0.6950 - val_auc: 0.7552 - val_prc: 0.4881\n",
            "Epoch 29/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 1.0489 - accuracy: 0.6723 - auc: 0.7015 - prc: 0.4518\n",
            "Epoch: 29\n",
            "Train Loss: 1.0487780570983887\n",
            "Val Loss: 1.0251415967941284\n",
            "Train Accuracy: 0.6725000143051147\n",
            "Val Accuracy: 0.6924999952316284\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.0488 - accuracy: 0.6725 - auc: 0.7019 - prc: 0.4520 - val_loss: 1.0251 - val_accuracy: 0.6925 - val_auc: 0.7576 - val_prc: 0.4930\n",
            "Epoch 30/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 1.0315 - accuracy: 0.6869 - auc: 0.7162 - prc: 0.4750\n",
            "Epoch: 30\n",
            "Train Loss: 1.0315572023391724\n",
            "Val Loss: 1.0107728242874146\n",
            "Train Accuracy: 0.6877777576446533\n",
            "Val Accuracy: 0.7024999856948853\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.0316 - accuracy: 0.6878 - auc: 0.7166 - prc: 0.4747 - val_loss: 1.0108 - val_accuracy: 0.7025 - val_auc: 0.7597 - val_prc: 0.4943\n",
            "Epoch 31/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 1.0133 - accuracy: 0.6837 - auc: 0.7270 - prc: 0.5006\n",
            "Epoch: 31\n",
            "Train Loss: 1.01480233669281\n",
            "Val Loss: 0.9976359009742737\n",
            "Train Accuracy: 0.6819444298744202\n",
            "Val Accuracy: 0.7074999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.0148 - accuracy: 0.6819 - auc: 0.7254 - prc: 0.4986 - val_loss: 0.9976 - val_accuracy: 0.7075 - val_auc: 0.7624 - val_prc: 0.4957\n",
            "Epoch 32/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 1.0038 - accuracy: 0.6963 - auc: 0.7293 - prc: 0.4925\n",
            "Epoch: 32\n",
            "Train Loss: 1.0045127868652344\n",
            "Val Loss: 0.9781697988510132\n",
            "Train Accuracy: 0.6958333253860474\n",
            "Val Accuracy: 0.7200000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.0045 - accuracy: 0.6958 - auc: 0.7285 - prc: 0.4920 - val_loss: 0.9782 - val_accuracy: 0.7200 - val_auc: 0.7653 - val_prc: 0.4978\n",
            "Epoch 33/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.9919 - accuracy: 0.7004 - auc: 0.7357 - prc: 0.5035\n",
            "Epoch: 33\n",
            "Train Loss: 0.9918251037597656\n",
            "Val Loss: 0.9774575233459473\n",
            "Train Accuracy: 0.7005555629730225\n",
            "Val Accuracy: 0.7174999713897705\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.9918 - accuracy: 0.7006 - auc: 0.7362 - prc: 0.5047 - val_loss: 0.9775 - val_accuracy: 0.7175 - val_auc: 0.7680 - val_prc: 0.5072\n",
            "Epoch 34/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.9870 - accuracy: 0.6909 - auc: 0.7284 - prc: 0.4838\n",
            "Epoch: 34\n",
            "Train Loss: 0.9886994957923889\n",
            "Val Loss: 0.9733471870422363\n",
            "Train Accuracy: 0.6891666650772095\n",
            "Val Accuracy: 0.6974999904632568\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.9887 - accuracy: 0.6892 - auc: 0.7251 - prc: 0.4811 - val_loss: 0.9733 - val_accuracy: 0.6975 - val_auc: 0.7691 - val_prc: 0.5116\n",
            "Epoch 35/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.9777 - accuracy: 0.6923 - auc: 0.7332 - prc: 0.4925\n",
            "Epoch: 35\n",
            "Train Loss: 0.9773077368736267\n",
            "Val Loss: 0.9667072296142578\n",
            "Train Accuracy: 0.691944420337677\n",
            "Val Accuracy: 0.6875\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.9773 - accuracy: 0.6919 - auc: 0.7320 - prc: 0.4923 - val_loss: 0.9667 - val_accuracy: 0.6875 - val_auc: 0.7699 - val_prc: 0.5118\n",
            "Epoch 36/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.9597 - accuracy: 0.7017 - auc: 0.7445 - prc: 0.5163\n",
            "Epoch: 36\n",
            "Train Loss: 0.959690511226654\n",
            "Val Loss: 0.9472936987876892\n",
            "Train Accuracy: 0.7016666531562805\n",
            "Val Accuracy: 0.7200000286102295\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.9597 - accuracy: 0.7017 - auc: 0.7445 - prc: 0.5163 - val_loss: 0.9473 - val_accuracy: 0.7200 - val_auc: 0.7719 - val_prc: 0.5129\n",
            "Epoch 37/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.9526 - accuracy: 0.7006 - auc: 0.7448 - prc: 0.5207\n",
            "Epoch: 37\n",
            "Train Loss: 0.9517276883125305\n",
            "Val Loss: 0.9412898421287537\n",
            "Train Accuracy: 0.7002778053283691\n",
            "Val Accuracy: 0.7099999785423279\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.9517 - accuracy: 0.7003 - auc: 0.7444 - prc: 0.5178 - val_loss: 0.9413 - val_accuracy: 0.7100 - val_auc: 0.7741 - val_prc: 0.5166\n",
            "Epoch 38/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.9345 - accuracy: 0.7112 - auc: 0.7642 - prc: 0.5393\n",
            "Epoch: 38\n",
            "Train Loss: 0.9339358806610107\n",
            "Val Loss: 0.9364286661148071\n",
            "Train Accuracy: 0.7111111283302307\n",
            "Val Accuracy: 0.6974999904632568\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.9339 - accuracy: 0.7111 - auc: 0.7647 - prc: 0.5396 - val_loss: 0.9364 - val_accuracy: 0.6975 - val_auc: 0.7750 - val_prc: 0.5194\n",
            "Epoch 39/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.9327 - accuracy: 0.7107 - auc: 0.7515 - prc: 0.5107\n",
            "Epoch: 39\n",
            "Train Loss: 0.9322007298469543\n",
            "Val Loss: 0.9177942872047424\n",
            "Train Accuracy: 0.7105555534362793\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.9322 - accuracy: 0.7106 - auc: 0.7516 - prc: 0.5122 - val_loss: 0.9178 - val_accuracy: 0.7275 - val_auc: 0.7774 - val_prc: 0.5207\n",
            "Epoch 40/300\n",
            "439/450 [============================>.] - ETA: 0s - loss: 0.9231 - accuracy: 0.7073 - auc: 0.7565 - prc: 0.5226\n",
            "Epoch: 40\n",
            "Train Loss: 0.922310471534729\n",
            "Val Loss: 0.9055433869361877\n",
            "Train Accuracy: 0.7080555558204651\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.9223 - accuracy: 0.7081 - auc: 0.7564 - prc: 0.5196 - val_loss: 0.9055 - val_accuracy: 0.7275 - val_auc: 0.7790 - val_prc: 0.5222\n",
            "Epoch 41/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.9052 - accuracy: 0.7263 - auc: 0.7711 - prc: 0.5563\n",
            "Epoch: 41\n",
            "Train Loss: 0.9050395488739014\n",
            "Val Loss: 0.9031608700752258\n",
            "Train Accuracy: 0.7269444465637207\n",
            "Val Accuracy: 0.7200000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.9050 - accuracy: 0.7269 - auc: 0.7713 - prc: 0.5569 - val_loss: 0.9032 - val_accuracy: 0.7200 - val_auc: 0.7810 - val_prc: 0.5303\n",
            "Epoch 42/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.8963 - accuracy: 0.7252 - auc: 0.7768 - prc: 0.5372\n",
            "Epoch: 42\n",
            "Train Loss: 0.8963072896003723\n",
            "Val Loss: 0.8896802663803101\n",
            "Train Accuracy: 0.7252777814865112\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8963 - accuracy: 0.7253 - auc: 0.7771 - prc: 0.5389 - val_loss: 0.8897 - val_accuracy: 0.7275 - val_auc: 0.7820 - val_prc: 0.5323\n",
            "Epoch 43/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.8842 - accuracy: 0.7256 - auc: 0.7804 - prc: 0.5639\n",
            "Epoch: 43\n",
            "Train Loss: 0.884863555431366\n",
            "Val Loss: 0.8887327313423157\n",
            "Train Accuracy: 0.7266666889190674\n",
            "Val Accuracy: 0.7124999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8849 - accuracy: 0.7267 - auc: 0.7809 - prc: 0.5649 - val_loss: 0.8887 - val_accuracy: 0.7125 - val_auc: 0.7829 - val_prc: 0.5383\n",
            "Epoch 44/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.8836 - accuracy: 0.7250 - auc: 0.7725 - prc: 0.5472\n",
            "Epoch: 44\n",
            "Train Loss: 0.8836025595664978\n",
            "Val Loss: 0.8793846368789673\n",
            "Train Accuracy: 0.7250000238418579\n",
            "Val Accuracy: 0.7124999761581421\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.8836 - accuracy: 0.7250 - auc: 0.7725 - prc: 0.5472 - val_loss: 0.8794 - val_accuracy: 0.7125 - val_auc: 0.7848 - val_prc: 0.5415\n",
            "Epoch 45/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.8748 - accuracy: 0.7360 - auc: 0.7809 - prc: 0.5460\n",
            "Epoch: 45\n",
            "Train Loss: 0.8749921917915344\n",
            "Val Loss: 0.8821439146995544\n",
            "Train Accuracy: 0.7350000143051147\n",
            "Val Accuracy: 0.6924999952316284\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.8750 - accuracy: 0.7350 - auc: 0.7805 - prc: 0.5451 - val_loss: 0.8821 - val_accuracy: 0.6925 - val_auc: 0.7856 - val_prc: 0.5439\n",
            "Epoch 46/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.8665 - accuracy: 0.7261 - auc: 0.7769 - prc: 0.5493\n",
            "Epoch: 46\n",
            "Train Loss: 0.8674029111862183\n",
            "Val Loss: 0.8544661998748779\n",
            "Train Accuracy: 0.726111114025116\n",
            "Val Accuracy: 0.7300000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8674 - accuracy: 0.7261 - auc: 0.7768 - prc: 0.5509 - val_loss: 0.8545 - val_accuracy: 0.7300 - val_auc: 0.7870 - val_prc: 0.5401\n",
            "Epoch 47/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.8532 - accuracy: 0.7463 - auc: 0.7925 - prc: 0.5686\n",
            "Epoch: 47\n",
            "Train Loss: 0.8521841168403625\n",
            "Val Loss: 0.8593435883522034\n",
            "Train Accuracy: 0.7461110949516296\n",
            "Val Accuracy: 0.7074999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8522 - accuracy: 0.7461 - auc: 0.7934 - prc: 0.5691 - val_loss: 0.8593 - val_accuracy: 0.7075 - val_auc: 0.7872 - val_prc: 0.5434\n",
            "Epoch 48/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.8481 - accuracy: 0.7302 - auc: 0.7880 - prc: 0.5700\n",
            "Epoch: 48\n",
            "Train Loss: 0.8478870391845703\n",
            "Val Loss: 0.8419353365898132\n",
            "Train Accuracy: 0.730555534362793\n",
            "Val Accuracy: 0.7174999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8479 - accuracy: 0.7306 - auc: 0.7883 - prc: 0.5695 - val_loss: 0.8419 - val_accuracy: 0.7175 - val_auc: 0.7892 - val_prc: 0.5441\n",
            "Epoch 49/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.8434 - accuracy: 0.7357 - auc: 0.7890 - prc: 0.5662\n",
            "Epoch: 49\n",
            "Train Loss: 0.8426999449729919\n",
            "Val Loss: 0.8481147885322571\n",
            "Train Accuracy: 0.7366666793823242\n",
            "Val Accuracy: 0.699999988079071\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8427 - accuracy: 0.7367 - auc: 0.7891 - prc: 0.5660 - val_loss: 0.8481 - val_accuracy: 0.7000 - val_auc: 0.7900 - val_prc: 0.5541\n",
            "Epoch 50/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.8345 - accuracy: 0.7360 - auc: 0.7921 - prc: 0.5772\n",
            "Epoch: 50\n",
            "Train Loss: 0.8345847129821777\n",
            "Val Loss: 0.8399825096130371\n",
            "Train Accuracy: 0.7358333468437195\n",
            "Val Accuracy: 0.7074999809265137\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.8346 - accuracy: 0.7358 - auc: 0.7919 - prc: 0.5767 - val_loss: 0.8400 - val_accuracy: 0.7075 - val_auc: 0.7913 - val_prc: 0.5546\n",
            "Epoch 51/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.8258 - accuracy: 0.7447 - auc: 0.7991 - prc: 0.5709\n",
            "Epoch: 51\n",
            "Train Loss: 0.8258004188537598\n",
            "Val Loss: 0.8365741968154907\n",
            "Train Accuracy: 0.7452777624130249\n",
            "Val Accuracy: 0.699999988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.8258 - accuracy: 0.7453 - auc: 0.7994 - prc: 0.5727 - val_loss: 0.8366 - val_accuracy: 0.7000 - val_auc: 0.7918 - val_prc: 0.5552\n",
            "Epoch 52/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.8224 - accuracy: 0.7276 - auc: 0.7950 - prc: 0.5803\n",
            "Epoch: 52\n",
            "Train Loss: 0.8217832446098328\n",
            "Val Loss: 0.8175104260444641\n",
            "Train Accuracy: 0.7277777791023254\n",
            "Val Accuracy: 0.7225000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8218 - accuracy: 0.7278 - auc: 0.7955 - prc: 0.5808 - val_loss: 0.8175 - val_accuracy: 0.7225 - val_auc: 0.7939 - val_prc: 0.5577\n",
            "Epoch 53/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.8116 - accuracy: 0.7404 - auc: 0.8026 - prc: 0.5855\n",
            "Epoch: 53\n",
            "Train Loss: 0.812644898891449\n",
            "Val Loss: 0.8118699789047241\n",
            "Train Accuracy: 0.7391666769981384\n",
            "Val Accuracy: 0.7174999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8126 - accuracy: 0.7392 - auc: 0.8016 - prc: 0.5848 - val_loss: 0.8119 - val_accuracy: 0.7175 - val_auc: 0.7944 - val_prc: 0.5549\n",
            "Epoch 54/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.8031 - accuracy: 0.7449 - auc: 0.8061 - prc: 0.5999\n",
            "Epoch: 54\n",
            "Train Loss: 0.8029439449310303\n",
            "Val Loss: 0.8185948133468628\n",
            "Train Accuracy: 0.745555579662323\n",
            "Val Accuracy: 0.7049999833106995\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8029 - accuracy: 0.7456 - auc: 0.8068 - prc: 0.6014 - val_loss: 0.8186 - val_accuracy: 0.7050 - val_auc: 0.7954 - val_prc: 0.5637\n",
            "Epoch 55/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.8027 - accuracy: 0.7436 - auc: 0.8025 - prc: 0.5814\n",
            "Epoch: 55\n",
            "Train Loss: 0.802736222743988\n",
            "Val Loss: 0.8055804967880249\n",
            "Train Accuracy: 0.7430555820465088\n",
            "Val Accuracy: 0.7149999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8027 - accuracy: 0.7431 - auc: 0.8022 - prc: 0.5807 - val_loss: 0.8056 - val_accuracy: 0.7150 - val_auc: 0.7967 - val_prc: 0.5665\n",
            "Epoch 56/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.7908 - accuracy: 0.7497 - auc: 0.8108 - prc: 0.6032\n",
            "Epoch: 56\n",
            "Train Loss: 0.7905257344245911\n",
            "Val Loss: 0.7961902022361755\n",
            "Train Accuracy: 0.7502777576446533\n",
            "Val Accuracy: 0.7225000262260437\n",
            "450/450 [==============================] - 7s 16ms/step - loss: 0.7905 - accuracy: 0.7503 - auc: 0.8114 - prc: 0.6063 - val_loss: 0.7962 - val_accuracy: 0.7225 - val_auc: 0.7977 - val_prc: 0.5684\n",
            "Epoch 57/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.7885 - accuracy: 0.7489 - auc: 0.8077 - prc: 0.5997\n",
            "Epoch: 57\n",
            "Train Loss: 0.7888124585151672\n",
            "Val Loss: 0.7964877486228943\n",
            "Train Accuracy: 0.7494444251060486\n",
            "Val Accuracy: 0.7149999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7888 - accuracy: 0.7494 - auc: 0.8083 - prc: 0.6021 - val_loss: 0.7965 - val_accuracy: 0.7150 - val_auc: 0.7988 - val_prc: 0.5707\n",
            "Epoch 58/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.7837 - accuracy: 0.7536 - auc: 0.8093 - prc: 0.5957\n",
            "Epoch: 58\n",
            "Train Loss: 0.7842879295349121\n",
            "Val Loss: 0.8019096255302429\n",
            "Train Accuracy: 0.7530555725097656\n",
            "Val Accuracy: 0.6974999904632568\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7843 - accuracy: 0.7531 - auc: 0.8086 - prc: 0.5924 - val_loss: 0.8019 - val_accuracy: 0.6975 - val_auc: 0.7998 - val_prc: 0.5748\n",
            "Epoch 59/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.7769 - accuracy: 0.7450 - auc: 0.8118 - prc: 0.5941\n",
            "Epoch: 59\n",
            "Train Loss: 0.7776970863342285\n",
            "Val Loss: 0.7773230671882629\n",
            "Train Accuracy: 0.7444444298744202\n",
            "Val Accuracy: 0.7325000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7777 - accuracy: 0.7444 - auc: 0.8114 - prc: 0.5949 - val_loss: 0.7773 - val_accuracy: 0.7325 - val_auc: 0.8008 - val_prc: 0.5717\n",
            "Epoch 60/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.7736 - accuracy: 0.7571 - auc: 0.8131 - prc: 0.5985\n",
            "Epoch: 60\n",
            "Train Loss: 0.7733104825019836\n",
            "Val Loss: 0.7814026474952698\n",
            "Train Accuracy: 0.7563889026641846\n",
            "Val Accuracy: 0.7200000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7733 - accuracy: 0.7564 - auc: 0.8131 - prc: 0.5970 - val_loss: 0.7814 - val_accuracy: 0.7200 - val_auc: 0.8024 - val_prc: 0.5763\n",
            "Epoch 61/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.7587 - accuracy: 0.7699 - auc: 0.8251 - prc: 0.6179\n",
            "Epoch: 61\n",
            "Train Loss: 0.7581960558891296\n",
            "Val Loss: 0.773637056350708\n",
            "Train Accuracy: 0.7708333134651184\n",
            "Val Accuracy: 0.7225000262260437\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.7582 - accuracy: 0.7708 - auc: 0.8255 - prc: 0.6185 - val_loss: 0.7736 - val_accuracy: 0.7225 - val_auc: 0.8029 - val_prc: 0.5809\n",
            "Epoch 62/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.7569 - accuracy: 0.7556 - auc: 0.8211 - prc: 0.6172\n",
            "Epoch: 62\n",
            "Train Loss: 0.7569435834884644\n",
            "Val Loss: 0.7733455896377563\n",
            "Train Accuracy: 0.7555555701255798\n",
            "Val Accuracy: 0.7174999713897705\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.7569 - accuracy: 0.7556 - auc: 0.8211 - prc: 0.6172 - val_loss: 0.7733 - val_accuracy: 0.7175 - val_auc: 0.8047 - val_prc: 0.5836\n",
            "Epoch 63/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.7543 - accuracy: 0.7641 - auc: 0.8208 - prc: 0.6149\n",
            "Epoch: 63\n",
            "Train Loss: 0.7540829181671143\n",
            "Val Loss: 0.7731629014015198\n",
            "Train Accuracy: 0.7633333206176758\n",
            "Val Accuracy: 0.7124999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7541 - accuracy: 0.7633 - auc: 0.8210 - prc: 0.6151 - val_loss: 0.7732 - val_accuracy: 0.7125 - val_auc: 0.8047 - val_prc: 0.5853\n",
            "Epoch 64/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.7543 - accuracy: 0.7565 - auc: 0.8175 - prc: 0.6153\n",
            "Epoch: 64\n",
            "Train Loss: 0.7534424066543579\n",
            "Val Loss: 0.760099470615387\n",
            "Train Accuracy: 0.7566666603088379\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7534 - accuracy: 0.7567 - auc: 0.8178 - prc: 0.6144 - val_loss: 0.7601 - val_accuracy: 0.7275 - val_auc: 0.8061 - val_prc: 0.5870\n",
            "Epoch 65/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.7474 - accuracy: 0.7643 - auc: 0.8218 - prc: 0.6191\n",
            "Epoch: 65\n",
            "Train Loss: 0.7472019195556641\n",
            "Val Loss: 0.7579872608184814\n",
            "Train Accuracy: 0.7649999856948853\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7472 - accuracy: 0.7650 - auc: 0.8226 - prc: 0.6218 - val_loss: 0.7580 - val_accuracy: 0.7275 - val_auc: 0.8065 - val_prc: 0.5877\n",
            "Epoch 66/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.7470 - accuracy: 0.7581 - auc: 0.8163 - prc: 0.6162\n",
            "Epoch: 66\n",
            "Train Loss: 0.7470722794532776\n",
            "Val Loss: 0.7638385891914368\n",
            "Train Accuracy: 0.758055567741394\n",
            "Val Accuracy: 0.7124999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7471 - accuracy: 0.7581 - auc: 0.8162 - prc: 0.6160 - val_loss: 0.7638 - val_accuracy: 0.7125 - val_auc: 0.8077 - val_prc: 0.5918\n",
            "Epoch 67/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.7278 - accuracy: 0.7597 - auc: 0.8338 - prc: 0.6487\n",
            "Epoch: 67\n",
            "Train Loss: 0.7277846336364746\n",
            "Val Loss: 0.7385477423667908\n",
            "Train Accuracy: 0.7597222328186035\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.7278 - accuracy: 0.7597 - auc: 0.8338 - prc: 0.6487 - val_loss: 0.7385 - val_accuracy: 0.7425 - val_auc: 0.8084 - val_prc: 0.5883\n",
            "Epoch 68/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.7286 - accuracy: 0.7611 - auc: 0.8286 - prc: 0.6251\n",
            "Epoch: 68\n",
            "Train Loss: 0.7299920916557312\n",
            "Val Loss: 0.7391586303710938\n",
            "Train Accuracy: 0.7608333230018616\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.7300 - accuracy: 0.7608 - auc: 0.8280 - prc: 0.6258 - val_loss: 0.7392 - val_accuracy: 0.7425 - val_auc: 0.8089 - val_prc: 0.5884\n",
            "Epoch 69/300\n",
            "439/450 [============================>.] - ETA: 0s - loss: 0.7278 - accuracy: 0.7631 - auc: 0.8279 - prc: 0.6370\n",
            "Epoch: 69\n",
            "Train Loss: 0.7274302840232849\n",
            "Val Loss: 0.7396916747093201\n",
            "Train Accuracy: 0.7622222304344177\n",
            "Val Accuracy: 0.7300000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7274 - accuracy: 0.7622 - auc: 0.8268 - prc: 0.6320 - val_loss: 0.7397 - val_accuracy: 0.7300 - val_auc: 0.8092 - val_prc: 0.5905\n",
            "Epoch 70/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.7218 - accuracy: 0.7565 - auc: 0.8305 - prc: 0.6382\n",
            "Epoch: 70\n",
            "Train Loss: 0.7219696044921875\n",
            "Val Loss: 0.7317327260971069\n",
            "Train Accuracy: 0.7558333277702332\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7220 - accuracy: 0.7558 - auc: 0.8299 - prc: 0.6377 - val_loss: 0.7317 - val_accuracy: 0.7425 - val_auc: 0.8102 - val_prc: 0.5909\n",
            "Epoch 71/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.7151 - accuracy: 0.7744 - auc: 0.8336 - prc: 0.6422\n",
            "Epoch: 71\n",
            "Train Loss: 0.7156228423118591\n",
            "Val Loss: 0.7365355491638184\n",
            "Train Accuracy: 0.7730555534362793\n",
            "Val Accuracy: 0.7300000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7156 - accuracy: 0.7731 - auc: 0.8331 - prc: 0.6422 - val_loss: 0.7365 - val_accuracy: 0.7300 - val_auc: 0.8103 - val_prc: 0.5971\n",
            "Epoch 72/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.7089 - accuracy: 0.7706 - auc: 0.8356 - prc: 0.6614\n",
            "Epoch: 72\n",
            "Train Loss: 0.7088761925697327\n",
            "Val Loss: 0.7237292528152466\n",
            "Train Accuracy: 0.7705555558204651\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7089 - accuracy: 0.7706 - auc: 0.8356 - prc: 0.6614 - val_loss: 0.7237 - val_accuracy: 0.7425 - val_auc: 0.8110 - val_prc: 0.5952\n",
            "Epoch 73/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.7075 - accuracy: 0.7679 - auc: 0.8347 - prc: 0.6528\n",
            "Epoch: 73\n",
            "Train Loss: 0.7075130343437195\n",
            "Val Loss: 0.7320471405982971\n",
            "Train Accuracy: 0.7683333158493042\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.7075 - accuracy: 0.7683 - auc: 0.8346 - prc: 0.6524 - val_loss: 0.7320 - val_accuracy: 0.7275 - val_auc: 0.8112 - val_prc: 0.5992\n",
            "Epoch 74/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.7073 - accuracy: 0.7631 - auc: 0.8339 - prc: 0.6436\n",
            "Epoch: 74\n",
            "Train Loss: 0.7068204283714294\n",
            "Val Loss: 0.7125506401062012\n",
            "Train Accuracy: 0.7630555629730225\n",
            "Val Accuracy: 0.7524999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.7068 - accuracy: 0.7631 - auc: 0.8339 - prc: 0.6422 - val_loss: 0.7126 - val_accuracy: 0.7525 - val_auc: 0.8123 - val_prc: 0.5967\n",
            "Epoch 75/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.7062 - accuracy: 0.7677 - auc: 0.8326 - prc: 0.6400\n",
            "Epoch: 75\n",
            "Train Loss: 0.7056299448013306\n",
            "Val Loss: 0.713941752910614\n",
            "Train Accuracy: 0.7674999833106995\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.7056 - accuracy: 0.7675 - auc: 0.8331 - prc: 0.6406 - val_loss: 0.7139 - val_accuracy: 0.7425 - val_auc: 0.8135 - val_prc: 0.5997\n",
            "Epoch 76/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.6982 - accuracy: 0.7666 - auc: 0.8378 - prc: 0.6610\n",
            "Epoch: 76\n",
            "Train Loss: 0.6983036994934082\n",
            "Val Loss: 0.7199413180351257\n",
            "Train Accuracy: 0.7663888931274414\n",
            "Val Accuracy: 0.7325000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6983 - accuracy: 0.7664 - auc: 0.8373 - prc: 0.6602 - val_loss: 0.7199 - val_accuracy: 0.7325 - val_auc: 0.8134 - val_prc: 0.6017\n",
            "Epoch 77/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.7797 - auc: 0.8416 - prc: 0.6643\n",
            "Epoch: 77\n",
            "Train Loss: 0.6910340785980225\n",
            "Val Loss: 0.7134409546852112\n",
            "Train Accuracy: 0.7797222137451172\n",
            "Val Accuracy: 0.7400000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.7797 - auc: 0.8416 - prc: 0.6643 - val_loss: 0.7134 - val_accuracy: 0.7400 - val_auc: 0.8138 - val_prc: 0.6036\n",
            "Epoch 78/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.7706 - auc: 0.8370 - prc: 0.6427\n",
            "Epoch: 78\n",
            "Train Loss: 0.6947810649871826\n",
            "Val Loss: 0.7113848328590393\n",
            "Train Accuracy: 0.7705555558204651\n",
            "Val Accuracy: 0.7325000166893005\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.6948 - accuracy: 0.7706 - auc: 0.8370 - prc: 0.6427 - val_loss: 0.7114 - val_accuracy: 0.7325 - val_auc: 0.8143 - val_prc: 0.6066\n",
            "Epoch 79/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.7708 - auc: 0.8355 - prc: 0.6384\n",
            "Epoch: 79\n",
            "Train Loss: 0.692409873008728\n",
            "Val Loss: 0.7032116651535034\n",
            "Train Accuracy: 0.7708333134651184\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.6924 - accuracy: 0.7708 - auc: 0.8359 - prc: 0.6394 - val_loss: 0.7032 - val_accuracy: 0.7425 - val_auc: 0.8159 - val_prc: 0.6064\n",
            "Epoch 80/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.7694 - auc: 0.8433 - prc: 0.6638\n",
            "Epoch: 80\n",
            "Train Loss: 0.6827154755592346\n",
            "Val Loss: 0.6984815001487732\n",
            "Train Accuracy: 0.7691666483879089\n",
            "Val Accuracy: 0.7475000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6827 - accuracy: 0.7692 - auc: 0.8428 - prc: 0.6623 - val_loss: 0.6985 - val_accuracy: 0.7475 - val_auc: 0.8160 - val_prc: 0.6067\n",
            "Epoch 81/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.7743 - auc: 0.8401 - prc: 0.6500\n",
            "Epoch: 81\n",
            "Train Loss: 0.6821826100349426\n",
            "Val Loss: 0.701164960861206\n",
            "Train Accuracy: 0.7749999761581421\n",
            "Val Accuracy: 0.7400000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6822 - accuracy: 0.7750 - auc: 0.8408 - prc: 0.6498 - val_loss: 0.7012 - val_accuracy: 0.7400 - val_auc: 0.8170 - val_prc: 0.6115\n",
            "Epoch 82/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.6767 - accuracy: 0.7744 - auc: 0.8426 - prc: 0.6599\n",
            "Epoch: 82\n",
            "Train Loss: 0.6763264536857605\n",
            "Val Loss: 0.6983070373535156\n",
            "Train Accuracy: 0.7744444608688354\n",
            "Val Accuracy: 0.7400000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6763 - accuracy: 0.7744 - auc: 0.8435 - prc: 0.6629 - val_loss: 0.6983 - val_accuracy: 0.7400 - val_auc: 0.8176 - val_prc: 0.6147\n",
            "Epoch 83/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.7744 - auc: 0.8416 - prc: 0.6581\n",
            "Epoch: 83\n",
            "Train Loss: 0.6766648888587952\n",
            "Val Loss: 0.6916530728340149\n",
            "Train Accuracy: 0.7747222185134888\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6767 - accuracy: 0.7747 - auc: 0.8416 - prc: 0.6583 - val_loss: 0.6917 - val_accuracy: 0.7425 - val_auc: 0.8175 - val_prc: 0.6113\n",
            "Epoch 84/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.6691 - accuracy: 0.7781 - auc: 0.8477 - prc: 0.6584\n",
            "Epoch: 84\n",
            "Train Loss: 0.6692869663238525\n",
            "Val Loss: 0.6774826049804688\n",
            "Train Accuracy: 0.7774999737739563\n",
            "Val Accuracy: 0.762499988079071\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.6693 - accuracy: 0.7775 - auc: 0.8473 - prc: 0.6569 - val_loss: 0.6775 - val_accuracy: 0.7625 - val_auc: 0.8186 - val_prc: 0.6137\n",
            "Epoch 85/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.6719 - accuracy: 0.7829 - auc: 0.8438 - prc: 0.6640\n",
            "Epoch: 85\n",
            "Train Loss: 0.6711806058883667\n",
            "Val Loss: 0.6840727925300598\n",
            "Train Accuracy: 0.7819444537162781\n",
            "Val Accuracy: 0.7475000023841858\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.6712 - accuracy: 0.7819 - auc: 0.8433 - prc: 0.6624 - val_loss: 0.6841 - val_accuracy: 0.7475 - val_auc: 0.8191 - val_prc: 0.6178\n",
            "Epoch 86/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.6646 - accuracy: 0.7777 - auc: 0.8462 - prc: 0.6698\n",
            "Epoch: 86\n",
            "Train Loss: 0.664287805557251\n",
            "Val Loss: 0.671857476234436\n",
            "Train Accuracy: 0.7780555486679077\n",
            "Val Accuracy: 0.762499988079071\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6643 - accuracy: 0.7781 - auc: 0.8468 - prc: 0.6705 - val_loss: 0.6719 - val_accuracy: 0.7625 - val_auc: 0.8198 - val_prc: 0.6158\n",
            "Epoch 87/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.6682 - accuracy: 0.7771 - auc: 0.8435 - prc: 0.6649\n",
            "Epoch: 87\n",
            "Train Loss: 0.6663615703582764\n",
            "Val Loss: 0.6901565790176392\n",
            "Train Accuracy: 0.778333306312561\n",
            "Val Accuracy: 0.7350000143051147\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6664 - accuracy: 0.7783 - auc: 0.8446 - prc: 0.6649 - val_loss: 0.6902 - val_accuracy: 0.7350 - val_auc: 0.8196 - val_prc: 0.6209\n",
            "Epoch 88/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.6572 - accuracy: 0.7789 - auc: 0.8497 - prc: 0.6711\n",
            "Epoch: 88\n",
            "Train Loss: 0.6579440832138062\n",
            "Val Loss: 0.6828665137290955\n",
            "Train Accuracy: 0.7797222137451172\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6579 - accuracy: 0.7797 - auc: 0.8495 - prc: 0.6707 - val_loss: 0.6829 - val_accuracy: 0.7425 - val_auc: 0.8201 - val_prc: 0.6226\n",
            "Epoch 89/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.6622 - accuracy: 0.7734 - auc: 0.8441 - prc: 0.6483\n",
            "Epoch: 89\n",
            "Train Loss: 0.6625581979751587\n",
            "Val Loss: 0.6763992309570312\n",
            "Train Accuracy: 0.773888885974884\n",
            "Val Accuracy: 0.7475000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6626 - accuracy: 0.7739 - auc: 0.8442 - prc: 0.6506 - val_loss: 0.6764 - val_accuracy: 0.7475 - val_auc: 0.8212 - val_prc: 0.6249\n",
            "Epoch 90/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6470 - accuracy: 0.7897 - auc: 0.8567 - prc: 0.6761\n",
            "Epoch: 90\n",
            "Train Loss: 0.6470091938972473\n",
            "Val Loss: 0.6725461483001709\n",
            "Train Accuracy: 0.789722204208374\n",
            "Val Accuracy: 0.7475000023841858\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.6470 - accuracy: 0.7897 - auc: 0.8567 - prc: 0.6761 - val_loss: 0.6725 - val_accuracy: 0.7475 - val_auc: 0.8219 - val_prc: 0.6267\n",
            "Epoch 91/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.6537 - accuracy: 0.7796 - auc: 0.8471 - prc: 0.6673\n",
            "Epoch: 91\n",
            "Train Loss: 0.6541083455085754\n",
            "Val Loss: 0.6787979006767273\n",
            "Train Accuracy: 0.7797222137451172\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.6541 - accuracy: 0.7797 - auc: 0.8467 - prc: 0.6672 - val_loss: 0.6788 - val_accuracy: 0.7425 - val_auc: 0.8220 - val_prc: 0.6282\n",
            "Epoch 92/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.6442 - accuracy: 0.7837 - auc: 0.8545 - prc: 0.6910\n",
            "Epoch: 92\n",
            "Train Loss: 0.6440232396125793\n",
            "Val Loss: 0.6680548787117004\n",
            "Train Accuracy: 0.7833333611488342\n",
            "Val Accuracy: 0.75\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6440 - accuracy: 0.7833 - auc: 0.8545 - prc: 0.6911 - val_loss: 0.6681 - val_accuracy: 0.7500 - val_auc: 0.8233 - val_prc: 0.6290\n",
            "Epoch 93/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.6423 - accuracy: 0.7899 - auc: 0.8547 - prc: 0.6808\n",
            "Epoch: 93\n",
            "Train Loss: 0.6422512531280518\n",
            "Val Loss: 0.6732238531112671\n",
            "Train Accuracy: 0.789722204208374\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6423 - accuracy: 0.7897 - auc: 0.8549 - prc: 0.6805 - val_loss: 0.6732 - val_accuracy: 0.7425 - val_auc: 0.8232 - val_prc: 0.6319\n",
            "Epoch 94/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.6405 - accuracy: 0.7829 - auc: 0.8534 - prc: 0.6756\n",
            "Epoch: 94\n",
            "Train Loss: 0.6398110389709473\n",
            "Val Loss: 0.676440954208374\n",
            "Train Accuracy: 0.7844444513320923\n",
            "Val Accuracy: 0.7350000143051147\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6398 - accuracy: 0.7844 - auc: 0.8550 - prc: 0.6811 - val_loss: 0.6764 - val_accuracy: 0.7350 - val_auc: 0.8232 - val_prc: 0.6327\n",
            "Epoch 95/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.6371 - accuracy: 0.7790 - auc: 0.8548 - prc: 0.6822\n",
            "Epoch: 95\n",
            "Train Loss: 0.6375935673713684\n",
            "Val Loss: 0.6511278748512268\n",
            "Train Accuracy: 0.7788888812065125\n",
            "Val Accuracy: 0.7649999856948853\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6376 - accuracy: 0.7789 - auc: 0.8547 - prc: 0.6823 - val_loss: 0.6511 - val_accuracy: 0.7650 - val_auc: 0.8250 - val_prc: 0.6323\n",
            "Epoch 96/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.7922 - auc: 0.8571 - prc: 0.6835\n",
            "Epoch: 96\n",
            "Train Loss: 0.6333903074264526\n",
            "Val Loss: 0.6613128185272217\n",
            "Train Accuracy: 0.7922222018241882\n",
            "Val Accuracy: 0.7524999976158142\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.6334 - accuracy: 0.7922 - auc: 0.8571 - prc: 0.6835 - val_loss: 0.6613 - val_accuracy: 0.7525 - val_auc: 0.8256 - val_prc: 0.6380\n",
            "Epoch 97/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.6372 - accuracy: 0.7812 - auc: 0.8532 - prc: 0.6800\n",
            "Epoch: 97\n",
            "Train Loss: 0.6379798054695129\n",
            "Val Loss: 0.6537590026855469\n",
            "Train Accuracy: 0.7799999713897705\n",
            "Val Accuracy: 0.762499988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.6380 - accuracy: 0.7800 - auc: 0.8523 - prc: 0.6784 - val_loss: 0.6538 - val_accuracy: 0.7625 - val_auc: 0.8260 - val_prc: 0.6376\n",
            "Epoch 98/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.6306 - accuracy: 0.7901 - auc: 0.8587 - prc: 0.6831\n",
            "Epoch: 98\n",
            "Train Loss: 0.6306742429733276\n",
            "Val Loss: 0.6588087677955627\n",
            "Train Accuracy: 0.789722204208374\n",
            "Val Accuracy: 0.7599999904632568\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.7897 - auc: 0.8585 - prc: 0.6834 - val_loss: 0.6588 - val_accuracy: 0.7600 - val_auc: 0.8264 - val_prc: 0.6409\n",
            "Epoch 99/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.7887 - auc: 0.8624 - prc: 0.6994\n",
            "Epoch: 99\n",
            "Train Loss: 0.62211012840271\n",
            "Val Loss: 0.66656893491745\n",
            "Train Accuracy: 0.7883333563804626\n",
            "Val Accuracy: 0.737500011920929\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6221 - accuracy: 0.7883 - auc: 0.8623 - prc: 0.6994 - val_loss: 0.6666 - val_accuracy: 0.7375 - val_auc: 0.8258 - val_prc: 0.6410\n",
            "Epoch 100/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.6278 - accuracy: 0.7863 - auc: 0.8575 - prc: 0.6817\n",
            "Epoch: 100\n",
            "Train Loss: 0.6269745826721191\n",
            "Val Loss: 0.6536002159118652\n",
            "Train Accuracy: 0.7866666913032532\n",
            "Val Accuracy: 0.762499988079071\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6270 - accuracy: 0.7867 - auc: 0.8579 - prc: 0.6817 - val_loss: 0.6536 - val_accuracy: 0.7625 - val_auc: 0.8268 - val_prc: 0.6405\n",
            "Epoch 101/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.6212 - accuracy: 0.7876 - auc: 0.8611 - prc: 0.6869\n",
            "Epoch: 101\n",
            "Train Loss: 0.6209730505943298\n",
            "Val Loss: 0.6463032364845276\n",
            "Train Accuracy: 0.7875000238418579\n",
            "Val Accuracy: 0.7599999904632568\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.6210 - accuracy: 0.7875 - auc: 0.8611 - prc: 0.6868 - val_loss: 0.6463 - val_accuracy: 0.7600 - val_auc: 0.8272 - val_prc: 0.6395\n",
            "Epoch 102/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.6209 - accuracy: 0.7895 - auc: 0.8585 - prc: 0.6920\n",
            "Epoch: 102\n",
            "Train Loss: 0.6213598847389221\n",
            "Val Loss: 0.6401045322418213\n",
            "Train Accuracy: 0.7891666889190674\n",
            "Val Accuracy: 0.762499988079071\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.6214 - accuracy: 0.7892 - auc: 0.8581 - prc: 0.6916 - val_loss: 0.6401 - val_accuracy: 0.7625 - val_auc: 0.8275 - val_prc: 0.6408\n",
            "Epoch 103/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.6165 - accuracy: 0.7869 - auc: 0.8598 - prc: 0.6982\n",
            "Epoch: 103\n",
            "Train Loss: 0.6160598993301392\n",
            "Val Loss: 0.6438515186309814\n",
            "Train Accuracy: 0.7877777814865112\n",
            "Val Accuracy: 0.7599999904632568\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6161 - accuracy: 0.7878 - auc: 0.8602 - prc: 0.7000 - val_loss: 0.6439 - val_accuracy: 0.7600 - val_auc: 0.8279 - val_prc: 0.6435\n",
            "Epoch 104/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.6169 - accuracy: 0.7837 - auc: 0.8590 - prc: 0.7006\n",
            "Epoch: 104\n",
            "Train Loss: 0.617526113986969\n",
            "Val Loss: 0.642108678817749\n",
            "Train Accuracy: 0.7833333611488342\n",
            "Val Accuracy: 0.7599999904632568\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6175 - accuracy: 0.7833 - auc: 0.8586 - prc: 0.6990 - val_loss: 0.6421 - val_accuracy: 0.7600 - val_auc: 0.8289 - val_prc: 0.6450\n",
            "Epoch 105/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6090 - accuracy: 0.7992 - auc: 0.8655 - prc: 0.7062\n",
            "Epoch: 105\n",
            "Train Loss: 0.6090039014816284\n",
            "Val Loss: 0.6557093858718872\n",
            "Train Accuracy: 0.7991666793823242\n",
            "Val Accuracy: 0.7475000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6090 - accuracy: 0.7992 - auc: 0.8655 - prc: 0.7062 - val_loss: 0.6557 - val_accuracy: 0.7475 - val_auc: 0.8278 - val_prc: 0.6480\n",
            "Epoch 106/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.6069 - accuracy: 0.7849 - auc: 0.8655 - prc: 0.7023\n",
            "Epoch: 106\n",
            "Train Loss: 0.6069216132164001\n",
            "Val Loss: 0.6326313614845276\n",
            "Train Accuracy: 0.7863888740539551\n",
            "Val Accuracy: 0.762499988079071\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6069 - accuracy: 0.7864 - auc: 0.8656 - prc: 0.7037 - val_loss: 0.6326 - val_accuracy: 0.7625 - val_auc: 0.8302 - val_prc: 0.6495\n",
            "Epoch 107/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.6108 - accuracy: 0.7876 - auc: 0.8612 - prc: 0.6984\n",
            "Epoch: 107\n",
            "Train Loss: 0.6113530993461609\n",
            "Val Loss: 0.6280827522277832\n",
            "Train Accuracy: 0.7869444489479065\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.7869 - auc: 0.8604 - prc: 0.6973 - val_loss: 0.6281 - val_accuracy: 0.7700 - val_auc: 0.8304 - val_prc: 0.6486\n",
            "Epoch 108/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.6079 - accuracy: 0.7988 - auc: 0.8630 - prc: 0.6936\n",
            "Epoch: 108\n",
            "Train Loss: 0.6093189120292664\n",
            "Val Loss: 0.6374247074127197\n",
            "Train Accuracy: 0.7986111044883728\n",
            "Val Accuracy: 0.7649999856948853\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.6093 - accuracy: 0.7986 - auc: 0.8620 - prc: 0.6931 - val_loss: 0.6374 - val_accuracy: 0.7650 - val_auc: 0.8302 - val_prc: 0.6490\n",
            "Epoch 109/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.6118 - accuracy: 0.7837 - auc: 0.8576 - prc: 0.6956\n",
            "Epoch: 109\n",
            "Train Loss: 0.6117763519287109\n",
            "Val Loss: 0.641249418258667\n",
            "Train Accuracy: 0.7838888764381409\n",
            "Val Accuracy: 0.7649999856948853\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6118 - accuracy: 0.7839 - auc: 0.8577 - prc: 0.6962 - val_loss: 0.6412 - val_accuracy: 0.7650 - val_auc: 0.8299 - val_prc: 0.6518\n",
            "Epoch 110/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.6000 - accuracy: 0.7929 - auc: 0.8657 - prc: 0.7058\n",
            "Epoch: 110\n",
            "Train Loss: 0.6006145477294922\n",
            "Val Loss: 0.6399852633476257\n",
            "Train Accuracy: 0.7916666865348816\n",
            "Val Accuracy: 0.7649999856948853\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6006 - accuracy: 0.7917 - auc: 0.8652 - prc: 0.7048 - val_loss: 0.6400 - val_accuracy: 0.7650 - val_auc: 0.8305 - val_prc: 0.6560\n",
            "Epoch 111/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.8016 - auc: 0.8695 - prc: 0.7185\n",
            "Epoch: 111\n",
            "Train Loss: 0.5953220725059509\n",
            "Val Loss: 0.6292540431022644\n",
            "Train Accuracy: 0.8013888597488403\n",
            "Val Accuracy: 0.7649999856948853\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5953 - accuracy: 0.8014 - auc: 0.8684 - prc: 0.7174 - val_loss: 0.6293 - val_accuracy: 0.7650 - val_auc: 0.8318 - val_prc: 0.6557\n",
            "Epoch 112/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.5969 - accuracy: 0.7901 - auc: 0.8664 - prc: 0.7109\n",
            "Epoch: 112\n",
            "Train Loss: 0.5977200269699097\n",
            "Val Loss: 0.6380930542945862\n",
            "Train Accuracy: 0.7894444465637207\n",
            "Val Accuracy: 0.7649999856948853\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5977 - accuracy: 0.7894 - auc: 0.8657 - prc: 0.7092 - val_loss: 0.6381 - val_accuracy: 0.7650 - val_auc: 0.8308 - val_prc: 0.6599\n",
            "Epoch 113/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.7914 - auc: 0.8677 - prc: 0.7034\n",
            "Epoch: 113\n",
            "Train Loss: 0.5944945812225342\n",
            "Val Loss: 0.6100501418113708\n",
            "Train Accuracy: 0.7913888692855835\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5945 - accuracy: 0.7914 - auc: 0.8677 - prc: 0.7034 - val_loss: 0.6101 - val_accuracy: 0.7750 - val_auc: 0.8339 - val_prc: 0.6606\n",
            "Epoch 114/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5913 - accuracy: 0.8016 - auc: 0.8687 - prc: 0.7168\n",
            "Epoch: 114\n",
            "Train Loss: 0.59126216173172\n",
            "Val Loss: 0.6193376183509827\n",
            "Train Accuracy: 0.801111102104187\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5913 - accuracy: 0.8011 - auc: 0.8686 - prc: 0.7164 - val_loss: 0.6193 - val_accuracy: 0.7700 - val_auc: 0.8331 - val_prc: 0.6618\n",
            "Epoch 115/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.7937 - auc: 0.8702 - prc: 0.7143\n",
            "Epoch: 115\n",
            "Train Loss: 0.5875864028930664\n",
            "Val Loss: 0.6198034882545471\n",
            "Train Accuracy: 0.7936111092567444\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5876 - accuracy: 0.7936 - auc: 0.8699 - prc: 0.7149 - val_loss: 0.6198 - val_accuracy: 0.7700 - val_auc: 0.8336 - val_prc: 0.6665\n",
            "Epoch 116/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.7970 - auc: 0.8700 - prc: 0.7199\n",
            "Epoch: 116\n",
            "Train Loss: 0.5859163999557495\n",
            "Val Loss: 0.6092143058776855\n",
            "Train Accuracy: 0.7975000143051147\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5859 - accuracy: 0.7975 - auc: 0.8708 - prc: 0.7220 - val_loss: 0.6092 - val_accuracy: 0.7725 - val_auc: 0.8344 - val_prc: 0.6627\n",
            "Epoch 117/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.5863 - accuracy: 0.7971 - auc: 0.8694 - prc: 0.7114\n",
            "Epoch: 117\n",
            "Train Loss: 0.5865347385406494\n",
            "Val Loss: 0.6196367144584656\n",
            "Train Accuracy: 0.7963888645172119\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5865 - accuracy: 0.7964 - auc: 0.8691 - prc: 0.7077 - val_loss: 0.6196 - val_accuracy: 0.7700 - val_auc: 0.8341 - val_prc: 0.6652\n",
            "Epoch 118/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.7914 - auc: 0.8680 - prc: 0.7048\n",
            "Epoch: 118\n",
            "Train Loss: 0.5876054763793945\n",
            "Val Loss: 0.6154482364654541\n",
            "Train Accuracy: 0.7913888692855835\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5876 - accuracy: 0.7914 - auc: 0.8680 - prc: 0.7048 - val_loss: 0.6154 - val_accuracy: 0.7725 - val_auc: 0.8340 - val_prc: 0.6662\n",
            "Epoch 119/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5777 - accuracy: 0.8038 - auc: 0.8735 - prc: 0.7278\n",
            "Epoch: 119\n",
            "Train Loss: 0.5795847177505493\n",
            "Val Loss: 0.6119285821914673\n",
            "Train Accuracy: 0.8025000095367432\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5796 - accuracy: 0.8025 - auc: 0.8724 - prc: 0.7244 - val_loss: 0.6119 - val_accuracy: 0.7725 - val_auc: 0.8343 - val_prc: 0.6666\n",
            "Epoch 120/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.5781 - accuracy: 0.8023 - auc: 0.8723 - prc: 0.7309\n",
            "Epoch: 120\n",
            "Train Loss: 0.5780190825462341\n",
            "Val Loss: 0.6056625843048096\n",
            "Train Accuracy: 0.8022222518920898\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5780 - accuracy: 0.8022 - auc: 0.8724 - prc: 0.7310 - val_loss: 0.6057 - val_accuracy: 0.7700 - val_auc: 0.8352 - val_prc: 0.6703\n",
            "Epoch 121/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5820 - accuracy: 0.7981 - auc: 0.8701 - prc: 0.7101\n",
            "Epoch: 121\n",
            "Train Loss: 0.5799567103385925\n",
            "Val Loss: 0.6298137903213501\n",
            "Train Accuracy: 0.7988888621330261\n",
            "Val Accuracy: 0.75\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5800 - accuracy: 0.7989 - auc: 0.8710 - prc: 0.7102 - val_loss: 0.6298 - val_accuracy: 0.7500 - val_auc: 0.8336 - val_prc: 0.6742\n",
            "Epoch 122/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5759 - accuracy: 0.8048 - auc: 0.8737 - prc: 0.7109\n",
            "Epoch: 122\n",
            "Train Loss: 0.5752015113830566\n",
            "Val Loss: 0.6020498871803284\n",
            "Train Accuracy: 0.804722249507904\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5752 - accuracy: 0.8047 - auc: 0.8743 - prc: 0.7135 - val_loss: 0.6020 - val_accuracy: 0.7750 - val_auc: 0.8354 - val_prc: 0.6689\n",
            "Epoch 123/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7953 - auc: 0.8687 - prc: 0.7159\n",
            "Epoch: 123\n",
            "Train Loss: 0.5782865881919861\n",
            "Val Loss: 0.608812153339386\n",
            "Train Accuracy: 0.7952777743339539\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5783 - accuracy: 0.7953 - auc: 0.8687 - prc: 0.7159 - val_loss: 0.6088 - val_accuracy: 0.7700 - val_auc: 0.8349 - val_prc: 0.6704\n",
            "Epoch 124/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.8055 - auc: 0.8718 - prc: 0.7136\n",
            "Epoch: 124\n",
            "Train Loss: 0.5734447836875916\n",
            "Val Loss: 0.6148629784584045\n",
            "Train Accuracy: 0.8063889145851135\n",
            "Val Accuracy: 0.7674999833106995\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.5734 - accuracy: 0.8064 - auc: 0.8728 - prc: 0.7158 - val_loss: 0.6149 - val_accuracy: 0.7675 - val_auc: 0.8344 - val_prc: 0.6721\n",
            "Epoch 125/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.8074 - auc: 0.8777 - prc: 0.7266\n",
            "Epoch: 125\n",
            "Train Loss: 0.5644120573997498\n",
            "Val Loss: 0.5914303660392761\n",
            "Train Accuracy: 0.808055579662323\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.5644 - accuracy: 0.8081 - auc: 0.8779 - prc: 0.7310 - val_loss: 0.5914 - val_accuracy: 0.7725 - val_auc: 0.8360 - val_prc: 0.6728\n",
            "Epoch 126/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.8051 - auc: 0.8784 - prc: 0.7409\n",
            "Epoch: 126\n",
            "Train Loss: 0.5629227757453918\n",
            "Val Loss: 0.6033177375793457\n",
            "Train Accuracy: 0.8041666746139526\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5629 - accuracy: 0.8042 - auc: 0.8782 - prc: 0.7406 - val_loss: 0.6033 - val_accuracy: 0.7700 - val_auc: 0.8358 - val_prc: 0.6753\n",
            "Epoch 127/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.8015 - auc: 0.8789 - prc: 0.7346\n",
            "Epoch: 127\n",
            "Train Loss: 0.5611586570739746\n",
            "Val Loss: 0.5965203642845154\n",
            "Train Accuracy: 0.8016666769981384\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5612 - accuracy: 0.8017 - auc: 0.8795 - prc: 0.7359 - val_loss: 0.5965 - val_accuracy: 0.7775 - val_auc: 0.8367 - val_prc: 0.6758\n",
            "Epoch 128/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.8058 - auc: 0.8751 - prc: 0.7264\n",
            "Epoch: 128\n",
            "Train Loss: 0.5670886635780334\n",
            "Val Loss: 0.6069896817207336\n",
            "Train Accuracy: 0.804722249507904\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5671 - accuracy: 0.8047 - auc: 0.8743 - prc: 0.7259 - val_loss: 0.6070 - val_accuracy: 0.7700 - val_auc: 0.8363 - val_prc: 0.6822\n",
            "Epoch 129/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.8010 - auc: 0.8738 - prc: 0.7308\n",
            "Epoch: 129\n",
            "Train Loss: 0.565685510635376\n",
            "Val Loss: 0.5929057598114014\n",
            "Train Accuracy: 0.800000011920929\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5657 - accuracy: 0.8000 - auc: 0.8732 - prc: 0.7287 - val_loss: 0.5929 - val_accuracy: 0.7775 - val_auc: 0.8373 - val_prc: 0.6777\n",
            "Epoch 130/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5567 - accuracy: 0.8039 - auc: 0.8799 - prc: 0.7369\n",
            "Epoch: 130\n",
            "Train Loss: 0.5564950704574585\n",
            "Val Loss: 0.5746082067489624\n",
            "Train Accuracy: 0.804722249507904\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5565 - accuracy: 0.8047 - auc: 0.8801 - prc: 0.7374 - val_loss: 0.5746 - val_accuracy: 0.7875 - val_auc: 0.8383 - val_prc: 0.6774\n",
            "Epoch 131/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.8111 - auc: 0.8801 - prc: 0.7365\n",
            "Epoch: 131\n",
            "Train Loss: 0.5558423399925232\n",
            "Val Loss: 0.5906795263290405\n",
            "Train Accuracy: 0.8111110925674438\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.5558 - accuracy: 0.8111 - auc: 0.8801 - prc: 0.7365 - val_loss: 0.5907 - val_accuracy: 0.7775 - val_auc: 0.8376 - val_prc: 0.6812\n",
            "Epoch 132/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.5542 - accuracy: 0.8045 - auc: 0.8804 - prc: 0.7378\n",
            "Epoch: 132\n",
            "Train Loss: 0.5539948344230652\n",
            "Val Loss: 0.5843474268913269\n",
            "Train Accuracy: 0.804444432258606\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5540 - accuracy: 0.8044 - auc: 0.8802 - prc: 0.7379 - val_loss: 0.5843 - val_accuracy: 0.7775 - val_auc: 0.8381 - val_prc: 0.6796\n",
            "Epoch 133/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.5493 - accuracy: 0.8093 - auc: 0.8828 - prc: 0.7382\n",
            "Epoch: 133\n",
            "Train Loss: 0.5498338341712952\n",
            "Val Loss: 0.5943771004676819\n",
            "Train Accuracy: 0.809166669845581\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5498 - accuracy: 0.8092 - auc: 0.8827 - prc: 0.7391 - val_loss: 0.5944 - val_accuracy: 0.7725 - val_auc: 0.8386 - val_prc: 0.6870\n",
            "Epoch 134/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5546 - accuracy: 0.8038 - auc: 0.8784 - prc: 0.7342\n",
            "Epoch: 134\n",
            "Train Loss: 0.5552793741226196\n",
            "Val Loss: 0.5764890909194946\n",
            "Train Accuracy: 0.8038889169692993\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5553 - accuracy: 0.8039 - auc: 0.8782 - prc: 0.7354 - val_loss: 0.5765 - val_accuracy: 0.7825 - val_auc: 0.8390 - val_prc: 0.6844\n",
            "Epoch 135/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5519 - accuracy: 0.8033 - auc: 0.8796 - prc: 0.7372\n",
            "Epoch: 135\n",
            "Train Loss: 0.5519043207168579\n",
            "Val Loss: 0.5712540149688721\n",
            "Train Accuracy: 0.8033333420753479\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5519 - accuracy: 0.8033 - auc: 0.8796 - prc: 0.7372 - val_loss: 0.5713 - val_accuracy: 0.7825 - val_auc: 0.8398 - val_prc: 0.6845\n",
            "Epoch 136/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.5475 - accuracy: 0.8087 - auc: 0.8818 - prc: 0.7448\n",
            "Epoch: 136\n",
            "Train Loss: 0.5478499531745911\n",
            "Val Loss: 0.585234522819519\n",
            "Train Accuracy: 0.8086110949516296\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5478 - accuracy: 0.8086 - auc: 0.8816 - prc: 0.7439 - val_loss: 0.5852 - val_accuracy: 0.7775 - val_auc: 0.8396 - val_prc: 0.6872\n",
            "Epoch 137/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5496 - accuracy: 0.8022 - auc: 0.8810 - prc: 0.7361\n",
            "Epoch: 137\n",
            "Train Loss: 0.5494917631149292\n",
            "Val Loss: 0.5812258124351501\n",
            "Train Accuracy: 0.8025000095367432\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5495 - accuracy: 0.8025 - auc: 0.8811 - prc: 0.7378 - val_loss: 0.5812 - val_accuracy: 0.7775 - val_auc: 0.8400 - val_prc: 0.6884\n",
            "Epoch 138/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.5455 - accuracy: 0.8098 - auc: 0.8824 - prc: 0.7371\n",
            "Epoch: 138\n",
            "Train Loss: 0.5461825728416443\n",
            "Val Loss: 0.5871293544769287\n",
            "Train Accuracy: 0.8100000023841858\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5462 - accuracy: 0.8100 - auc: 0.8820 - prc: 0.7365 - val_loss: 0.5871 - val_accuracy: 0.7775 - val_auc: 0.8397 - val_prc: 0.6892\n",
            "Epoch 139/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.8003 - auc: 0.8804 - prc: 0.7503\n",
            "Epoch: 139\n",
            "Train Loss: 0.544865071773529\n",
            "Val Loss: 0.5715574026107788\n",
            "Train Accuracy: 0.8005555272102356\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5449 - accuracy: 0.8006 - auc: 0.8815 - prc: 0.7528 - val_loss: 0.5716 - val_accuracy: 0.7825 - val_auc: 0.8404 - val_prc: 0.6903\n",
            "Epoch 140/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.5412 - accuracy: 0.8065 - auc: 0.8851 - prc: 0.7388\n",
            "Epoch: 140\n",
            "Train Loss: 0.5411171913146973\n",
            "Val Loss: 0.572380781173706\n",
            "Train Accuracy: 0.8066666722297668\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.5411 - accuracy: 0.8067 - auc: 0.8854 - prc: 0.7396 - val_loss: 0.5724 - val_accuracy: 0.7825 - val_auc: 0.8406 - val_prc: 0.6915\n",
            "Epoch 141/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5448 - accuracy: 0.8089 - auc: 0.8804 - prc: 0.7444\n",
            "Epoch: 141\n",
            "Train Loss: 0.5448035001754761\n",
            "Val Loss: 0.5775052905082703\n",
            "Train Accuracy: 0.8088889122009277\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.5448 - accuracy: 0.8089 - auc: 0.8804 - prc: 0.7444 - val_loss: 0.5775 - val_accuracy: 0.7775 - val_auc: 0.8407 - val_prc: 0.6919\n",
            "Epoch 142/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.5437 - accuracy: 0.8043 - auc: 0.8809 - prc: 0.7420\n",
            "Epoch: 142\n",
            "Train Loss: 0.5430091023445129\n",
            "Val Loss: 0.568631649017334\n",
            "Train Accuracy: 0.8041666746139526\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.5430 - accuracy: 0.8042 - auc: 0.8814 - prc: 0.7434 - val_loss: 0.5686 - val_accuracy: 0.7825 - val_auc: 0.8416 - val_prc: 0.6945\n",
            "Epoch 143/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5389 - accuracy: 0.8105 - auc: 0.8830 - prc: 0.7432\n",
            "Epoch: 143\n",
            "Train Loss: 0.5382791757583618\n",
            "Val Loss: 0.5672533512115479\n",
            "Train Accuracy: 0.8102777600288391\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5383 - accuracy: 0.8103 - auc: 0.8834 - prc: 0.7442 - val_loss: 0.5673 - val_accuracy: 0.7825 - val_auc: 0.8421 - val_prc: 0.6962\n",
            "Epoch 144/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5351 - accuracy: 0.8145 - auc: 0.8851 - prc: 0.7479\n",
            "Epoch: 144\n",
            "Train Loss: 0.5361034870147705\n",
            "Val Loss: 0.5764391422271729\n",
            "Train Accuracy: 0.8130555748939514\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5361 - accuracy: 0.8131 - auc: 0.8849 - prc: 0.7503 - val_loss: 0.5764 - val_accuracy: 0.7775 - val_auc: 0.8411 - val_prc: 0.6956\n",
            "Epoch 145/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5319 - accuracy: 0.8091 - auc: 0.8870 - prc: 0.7482\n",
            "Epoch: 145\n",
            "Train Loss: 0.5328016877174377\n",
            "Val Loss: 0.571356475353241\n",
            "Train Accuracy: 0.8088889122009277\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5328 - accuracy: 0.8089 - auc: 0.8868 - prc: 0.7493 - val_loss: 0.5714 - val_accuracy: 0.7800 - val_auc: 0.8416 - val_prc: 0.6950\n",
            "Epoch 146/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.5355 - accuracy: 0.8108 - auc: 0.8834 - prc: 0.7455\n",
            "Epoch: 146\n",
            "Train Loss: 0.5355339050292969\n",
            "Val Loss: 0.5609537363052368\n",
            "Train Accuracy: 0.8108333349227905\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5355 - accuracy: 0.8108 - auc: 0.8837 - prc: 0.7465 - val_loss: 0.5610 - val_accuracy: 0.7850 - val_auc: 0.8428 - val_prc: 0.6975\n",
            "Epoch 147/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5279 - accuracy: 0.8198 - auc: 0.8894 - prc: 0.7531\n",
            "Epoch: 147\n",
            "Train Loss: 0.5267381072044373\n",
            "Val Loss: 0.5666196346282959\n",
            "Train Accuracy: 0.8208333253860474\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5267 - accuracy: 0.8208 - auc: 0.8904 - prc: 0.7552 - val_loss: 0.5666 - val_accuracy: 0.7825 - val_auc: 0.8420 - val_prc: 0.6967\n",
            "Epoch 148/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.8061 - auc: 0.8833 - prc: 0.7493\n",
            "Epoch: 148\n",
            "Train Loss: 0.5347491502761841\n",
            "Val Loss: 0.5612496733665466\n",
            "Train Accuracy: 0.8052777647972107\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.5347 - accuracy: 0.8053 - auc: 0.8831 - prc: 0.7492 - val_loss: 0.5612 - val_accuracy: 0.7850 - val_auc: 0.8421 - val_prc: 0.6978\n",
            "Epoch 149/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.8142 - auc: 0.8879 - prc: 0.7590\n",
            "Epoch: 149\n",
            "Train Loss: 0.5267225503921509\n",
            "Val Loss: 0.5758213996887207\n",
            "Train Accuracy: 0.8141666650772095\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5267 - accuracy: 0.8142 - auc: 0.8879 - prc: 0.7590 - val_loss: 0.5758 - val_accuracy: 0.7750 - val_auc: 0.8420 - val_prc: 0.6991\n",
            "Epoch 150/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.5275 - accuracy: 0.8122 - auc: 0.8860 - prc: 0.7553\n",
            "Epoch: 150\n",
            "Train Loss: 0.5269008874893188\n",
            "Val Loss: 0.5606858730316162\n",
            "Train Accuracy: 0.8125\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5269 - accuracy: 0.8125 - auc: 0.8868 - prc: 0.7568 - val_loss: 0.5607 - val_accuracy: 0.7850 - val_auc: 0.8429 - val_prc: 0.6993\n",
            "Epoch 151/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.5270 - accuracy: 0.8056 - auc: 0.8866 - prc: 0.7541\n",
            "Epoch: 151\n",
            "Train Loss: 0.5278144478797913\n",
            "Val Loss: 0.5607351660728455\n",
            "Train Accuracy: 0.8066666722297668\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.5278 - accuracy: 0.8067 - auc: 0.8864 - prc: 0.7527 - val_loss: 0.5607 - val_accuracy: 0.7825 - val_auc: 0.8431 - val_prc: 0.7008\n",
            "Epoch 152/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.5266 - accuracy: 0.8135 - auc: 0.8869 - prc: 0.7513\n",
            "Epoch: 152\n",
            "Train Loss: 0.5263778567314148\n",
            "Val Loss: 0.5369120240211487\n",
            "Train Accuracy: 0.8136110901832581\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.5264 - accuracy: 0.8136 - auc: 0.8870 - prc: 0.7517 - val_loss: 0.5369 - val_accuracy: 0.8050 - val_auc: 0.8445 - val_prc: 0.6992\n",
            "Epoch 153/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.8103 - auc: 0.8882 - prc: 0.7547\n",
            "Epoch: 153\n",
            "Train Loss: 0.5241211652755737\n",
            "Val Loss: 0.5581903457641602\n",
            "Train Accuracy: 0.8102777600288391\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5241 - accuracy: 0.8103 - auc: 0.8882 - prc: 0.7547 - val_loss: 0.5582 - val_accuracy: 0.7825 - val_auc: 0.8439 - val_prc: 0.7033\n",
            "Epoch 154/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.5244 - accuracy: 0.8126 - auc: 0.8882 - prc: 0.7471\n",
            "Epoch: 154\n",
            "Train Loss: 0.5239165425300598\n",
            "Val Loss: 0.5518471598625183\n",
            "Train Accuracy: 0.8130555748939514\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5239 - accuracy: 0.8131 - auc: 0.8885 - prc: 0.7476 - val_loss: 0.5518 - val_accuracy: 0.7875 - val_auc: 0.8442 - val_prc: 0.7033\n",
            "Epoch 155/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5146 - accuracy: 0.8193 - auc: 0.8924 - prc: 0.7559\n",
            "Epoch: 155\n",
            "Train Loss: 0.515064001083374\n",
            "Val Loss: 0.5586555004119873\n",
            "Train Accuracy: 0.8191666603088379\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5151 - accuracy: 0.8192 - auc: 0.8926 - prc: 0.7578 - val_loss: 0.5587 - val_accuracy: 0.7875 - val_auc: 0.8440 - val_prc: 0.7035\n",
            "Epoch 156/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.5248 - accuracy: 0.8053 - auc: 0.8856 - prc: 0.7512\n",
            "Epoch: 156\n",
            "Train Loss: 0.5231912136077881\n",
            "Val Loss: 0.5528562664985657\n",
            "Train Accuracy: 0.8052777647972107\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5232 - accuracy: 0.8053 - auc: 0.8863 - prc: 0.7517 - val_loss: 0.5529 - val_accuracy: 0.7875 - val_auc: 0.8448 - val_prc: 0.7044\n",
            "Epoch 157/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5174 - accuracy: 0.8161 - auc: 0.8907 - prc: 0.7531\n",
            "Epoch: 157\n",
            "Train Loss: 0.5171842575073242\n",
            "Val Loss: 0.5518970489501953\n",
            "Train Accuracy: 0.8163889050483704\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5172 - accuracy: 0.8164 - auc: 0.8910 - prc: 0.7550 - val_loss: 0.5519 - val_accuracy: 0.7875 - val_auc: 0.8446 - val_prc: 0.7062\n",
            "Epoch 158/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.5179 - accuracy: 0.8126 - auc: 0.8903 - prc: 0.7524\n",
            "Epoch: 158\n",
            "Train Loss: 0.5175090432167053\n",
            "Val Loss: 0.5665887594223022\n",
            "Train Accuracy: 0.8119444251060486\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5175 - accuracy: 0.8119 - auc: 0.8901 - prc: 0.7518 - val_loss: 0.5666 - val_accuracy: 0.7725 - val_auc: 0.8442 - val_prc: 0.7064\n",
            "Epoch 159/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5155 - accuracy: 0.8077 - auc: 0.8900 - prc: 0.7621\n",
            "Epoch: 159\n",
            "Train Loss: 0.5151525139808655\n",
            "Val Loss: 0.536602258682251\n",
            "Train Accuracy: 0.8077777624130249\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.5152 - accuracy: 0.8078 - auc: 0.8904 - prc: 0.7626 - val_loss: 0.5366 - val_accuracy: 0.8025 - val_auc: 0.8456 - val_prc: 0.7045\n",
            "Epoch 160/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5170 - accuracy: 0.8156 - auc: 0.8887 - prc: 0.7543\n",
            "Epoch: 160\n",
            "Train Loss: 0.5172737240791321\n",
            "Val Loss: 0.551942765712738\n",
            "Train Accuracy: 0.815833330154419\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5173 - accuracy: 0.8158 - auc: 0.8890 - prc: 0.7558 - val_loss: 0.5519 - val_accuracy: 0.7850 - val_auc: 0.8452 - val_prc: 0.7081\n",
            "Epoch 161/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5101 - accuracy: 0.8195 - auc: 0.8938 - prc: 0.7585\n",
            "Epoch: 161\n",
            "Train Loss: 0.5099174380302429\n",
            "Val Loss: 0.5483765602111816\n",
            "Train Accuracy: 0.8191666603088379\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5099 - accuracy: 0.8192 - auc: 0.8935 - prc: 0.7569 - val_loss: 0.5484 - val_accuracy: 0.7850 - val_auc: 0.8462 - val_prc: 0.7096\n",
            "Epoch 162/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5086 - accuracy: 0.8129 - auc: 0.8936 - prc: 0.7646\n",
            "Epoch: 162\n",
            "Train Loss: 0.5097935199737549\n",
            "Val Loss: 0.5511157512664795\n",
            "Train Accuracy: 0.8113889098167419\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5098 - accuracy: 0.8114 - auc: 0.8928 - prc: 0.7621 - val_loss: 0.5511 - val_accuracy: 0.7825 - val_auc: 0.8460 - val_prc: 0.7105\n",
            "Epoch 163/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.8153 - auc: 0.8941 - prc: 0.7696\n",
            "Epoch: 163\n",
            "Train Loss: 0.5065229535102844\n",
            "Val Loss: 0.5533215403556824\n",
            "Train Accuracy: 0.8152777552604675\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.5065 - accuracy: 0.8153 - auc: 0.8941 - prc: 0.7696 - val_loss: 0.5533 - val_accuracy: 0.7800 - val_auc: 0.8461 - val_prc: 0.7092\n",
            "Epoch 164/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.5035 - accuracy: 0.8198 - auc: 0.8969 - prc: 0.7622\n",
            "Epoch: 164\n",
            "Train Loss: 0.5030868649482727\n",
            "Val Loss: 0.5227755904197693\n",
            "Train Accuracy: 0.8194444179534912\n",
            "Val Accuracy: 0.8100000023841858\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5031 - accuracy: 0.8194 - auc: 0.8971 - prc: 0.7629 - val_loss: 0.5228 - val_accuracy: 0.8100 - val_auc: 0.8470 - val_prc: 0.7062\n",
            "Epoch 165/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4996 - accuracy: 0.8238 - auc: 0.8980 - prc: 0.7753\n",
            "Epoch: 165\n",
            "Train Loss: 0.5007502436637878\n",
            "Val Loss: 0.5471639633178711\n",
            "Train Accuracy: 0.8233333230018616\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.5008 - accuracy: 0.8233 - auc: 0.8973 - prc: 0.7744 - val_loss: 0.5472 - val_accuracy: 0.7825 - val_auc: 0.8462 - val_prc: 0.7095\n",
            "Epoch 166/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.8161 - auc: 0.8928 - prc: 0.7661\n",
            "Epoch: 166\n",
            "Train Loss: 0.5066128373146057\n",
            "Val Loss: 0.5488392114639282\n",
            "Train Accuracy: 0.8161110877990723\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5066 - accuracy: 0.8161 - auc: 0.8928 - prc: 0.7661 - val_loss: 0.5488 - val_accuracy: 0.7800 - val_auc: 0.8462 - val_prc: 0.7111\n",
            "Epoch 167/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5033 - accuracy: 0.8126 - auc: 0.8943 - prc: 0.7704\n",
            "Epoch: 167\n",
            "Train Loss: 0.5023919939994812\n",
            "Val Loss: 0.5332529544830322\n",
            "Train Accuracy: 0.8133333325386047\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5024 - accuracy: 0.8133 - auc: 0.8947 - prc: 0.7708 - val_loss: 0.5333 - val_accuracy: 0.7950 - val_auc: 0.8470 - val_prc: 0.7109\n",
            "Epoch 168/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4989 - accuracy: 0.8196 - auc: 0.8973 - prc: 0.7761\n",
            "Epoch: 168\n",
            "Train Loss: 0.49818411469459534\n",
            "Val Loss: 0.5365307927131653\n",
            "Train Accuracy: 0.8199999928474426\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4982 - accuracy: 0.8200 - auc: 0.8976 - prc: 0.7761 - val_loss: 0.5365 - val_accuracy: 0.7875 - val_auc: 0.8473 - val_prc: 0.7109\n",
            "Epoch 169/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.8232 - auc: 0.8979 - prc: 0.7764\n",
            "Epoch: 169\n",
            "Train Loss: 0.4952603280544281\n",
            "Val Loss: 0.5424504280090332\n",
            "Train Accuracy: 0.8233333230018616\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4953 - accuracy: 0.8233 - auc: 0.8979 - prc: 0.7759 - val_loss: 0.5425 - val_accuracy: 0.7825 - val_auc: 0.8471 - val_prc: 0.7135\n",
            "Epoch 170/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5037 - accuracy: 0.8115 - auc: 0.8930 - prc: 0.7688\n",
            "Epoch: 170\n",
            "Train Loss: 0.5041083693504333\n",
            "Val Loss: 0.5506881475448608\n",
            "Train Accuracy: 0.8116666674613953\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.5041 - accuracy: 0.8117 - auc: 0.8925 - prc: 0.7670 - val_loss: 0.5507 - val_accuracy: 0.7775 - val_auc: 0.8473 - val_prc: 0.7163\n",
            "Epoch 171/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.8177 - auc: 0.8973 - prc: 0.7634\n",
            "Epoch: 171\n",
            "Train Loss: 0.49809786677360535\n",
            "Val Loss: 0.5175003409385681\n",
            "Train Accuracy: 0.816944420337677\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4981 - accuracy: 0.8169 - auc: 0.8966 - prc: 0.7628 - val_loss: 0.5175 - val_accuracy: 0.8075 - val_auc: 0.8484 - val_prc: 0.7106\n",
            "Epoch 172/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.5029 - accuracy: 0.8184 - auc: 0.8925 - prc: 0.7628\n",
            "Epoch: 172\n",
            "Train Loss: 0.5042853355407715\n",
            "Val Loss: 0.5482016801834106\n",
            "Train Accuracy: 0.8180555701255798\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.5043 - accuracy: 0.8181 - auc: 0.8920 - prc: 0.7632 - val_loss: 0.5482 - val_accuracy: 0.7725 - val_auc: 0.8471 - val_prc: 0.7162\n",
            "Epoch 173/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.8172 - auc: 0.8953 - prc: 0.7663\n",
            "Epoch: 173\n",
            "Train Loss: 0.4979614019393921\n",
            "Val Loss: 0.5387458205223083\n",
            "Train Accuracy: 0.8174999952316284\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.4980 - accuracy: 0.8175 - auc: 0.8955 - prc: 0.7667 - val_loss: 0.5387 - val_accuracy: 0.7825 - val_auc: 0.8479 - val_prc: 0.7146\n",
            "Epoch 174/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4871 - accuracy: 0.8224 - auc: 0.9024 - prc: 0.7844\n",
            "Epoch: 174\n",
            "Train Loss: 0.4869353771209717\n",
            "Val Loss: 0.5092372894287109\n",
            "Train Accuracy: 0.8219444155693054\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.4869 - accuracy: 0.8219 - auc: 0.9024 - prc: 0.7841 - val_loss: 0.5092 - val_accuracy: 0.8175 - val_auc: 0.8488 - val_prc: 0.7105\n",
            "Epoch 175/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4846 - accuracy: 0.8270 - auc: 0.9025 - prc: 0.7933\n",
            "Epoch: 175\n",
            "Train Loss: 0.48754391074180603\n",
            "Val Loss: 0.5304414629936218\n",
            "Train Accuracy: 0.8255555629730225\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4875 - accuracy: 0.8256 - auc: 0.9009 - prc: 0.7929 - val_loss: 0.5304 - val_accuracy: 0.7975 - val_auc: 0.8487 - val_prc: 0.7166\n",
            "Epoch 176/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4958 - accuracy: 0.8142 - auc: 0.8958 - prc: 0.7681\n",
            "Epoch: 176\n",
            "Train Loss: 0.4960767328739166\n",
            "Val Loss: 0.5237259864807129\n",
            "Train Accuracy: 0.8141666650772095\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4961 - accuracy: 0.8142 - auc: 0.8957 - prc: 0.7681 - val_loss: 0.5237 - val_accuracy: 0.8025 - val_auc: 0.8491 - val_prc: 0.7174\n",
            "Epoch 177/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4925 - accuracy: 0.8202 - auc: 0.8977 - prc: 0.7750\n",
            "Epoch: 177\n",
            "Train Loss: 0.4938770830631256\n",
            "Val Loss: 0.5369513630867004\n",
            "Train Accuracy: 0.8197222352027893\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4939 - accuracy: 0.8197 - auc: 0.8968 - prc: 0.7742 - val_loss: 0.5370 - val_accuracy: 0.7875 - val_auc: 0.8484 - val_prc: 0.7190\n",
            "Epoch 178/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4905 - accuracy: 0.8195 - auc: 0.8987 - prc: 0.7785\n",
            "Epoch: 178\n",
            "Train Loss: 0.4894581139087677\n",
            "Val Loss: 0.5291483402252197\n",
            "Train Accuracy: 0.8199999928474426\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4895 - accuracy: 0.8200 - auc: 0.8992 - prc: 0.7788 - val_loss: 0.5291 - val_accuracy: 0.7975 - val_auc: 0.8494 - val_prc: 0.7192\n",
            "Epoch 179/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4808 - accuracy: 0.8202 - auc: 0.9036 - prc: 0.7857\n",
            "Epoch: 179\n",
            "Train Loss: 0.4815802574157715\n",
            "Val Loss: 0.5319994688034058\n",
            "Train Accuracy: 0.8199999928474426\n",
            "Val Accuracy: 0.7900000214576721\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4816 - accuracy: 0.8200 - auc: 0.9033 - prc: 0.7850 - val_loss: 0.5320 - val_accuracy: 0.7900 - val_auc: 0.8489 - val_prc: 0.7195\n",
            "Epoch 180/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4889 - accuracy: 0.8267 - auc: 0.8988 - prc: 0.7784\n",
            "Epoch: 180\n",
            "Train Loss: 0.4888768792152405\n",
            "Val Loss: 0.5374632477760315\n",
            "Train Accuracy: 0.8266666531562805\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4889 - accuracy: 0.8267 - auc: 0.8988 - prc: 0.7784 - val_loss: 0.5375 - val_accuracy: 0.7875 - val_auc: 0.8487 - val_prc: 0.7203\n",
            "Epoch 181/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4864 - accuracy: 0.8210 - auc: 0.9005 - prc: 0.7849\n",
            "Epoch: 181\n",
            "Train Loss: 0.48639819025993347\n",
            "Val Loss: 0.5312116146087646\n",
            "Train Accuracy: 0.820277750492096\n",
            "Val Accuracy: 0.7900000214576721\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4864 - accuracy: 0.8203 - auc: 0.9002 - prc: 0.7836 - val_loss: 0.5312 - val_accuracy: 0.7900 - val_auc: 0.8494 - val_prc: 0.7203\n",
            "Epoch 182/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4832 - accuracy: 0.8178 - auc: 0.9009 - prc: 0.7900\n",
            "Epoch: 182\n",
            "Train Loss: 0.4827518165111542\n",
            "Val Loss: 0.5157694220542908\n",
            "Train Accuracy: 0.8177777528762817\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4828 - accuracy: 0.8178 - auc: 0.9011 - prc: 0.7897 - val_loss: 0.5158 - val_accuracy: 0.8025 - val_auc: 0.8508 - val_prc: 0.7197\n",
            "Epoch 183/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.8206 - auc: 0.8978 - prc: 0.7717\n",
            "Epoch: 183\n",
            "Train Loss: 0.4893819987773895\n",
            "Val Loss: 0.5173234343528748\n",
            "Train Accuracy: 0.820555567741394\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.4894 - accuracy: 0.8206 - auc: 0.8978 - prc: 0.7717 - val_loss: 0.5173 - val_accuracy: 0.8050 - val_auc: 0.8504 - val_prc: 0.7194\n",
            "Epoch 184/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4858 - accuracy: 0.8244 - auc: 0.8990 - prc: 0.7798\n",
            "Epoch: 184\n",
            "Train Loss: 0.48679429292678833\n",
            "Val Loss: 0.5191400051116943\n",
            "Train Accuracy: 0.823888897895813\n",
            "Val Accuracy: 0.800000011920929\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4868 - accuracy: 0.8239 - auc: 0.8982 - prc: 0.7785 - val_loss: 0.5191 - val_accuracy: 0.8000 - val_auc: 0.8498 - val_prc: 0.7212\n",
            "Epoch 185/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.8311 - auc: 0.9026 - prc: 0.7833\n",
            "Epoch: 185\n",
            "Train Loss: 0.4797177016735077\n",
            "Val Loss: 0.531786322593689\n",
            "Train Accuracy: 0.8311111330986023\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4797 - accuracy: 0.8311 - auc: 0.9026 - prc: 0.7833 - val_loss: 0.5318 - val_accuracy: 0.7875 - val_auc: 0.8495 - val_prc: 0.7237\n",
            "Epoch 186/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4738 - accuracy: 0.8201 - auc: 0.9053 - prc: 0.7900\n",
            "Epoch: 186\n",
            "Train Loss: 0.47544360160827637\n",
            "Val Loss: 0.5140576958656311\n",
            "Train Accuracy: 0.8194444179534912\n",
            "Val Accuracy: 0.800000011920929\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4754 - accuracy: 0.8194 - auc: 0.9044 - prc: 0.7896 - val_loss: 0.5141 - val_accuracy: 0.8000 - val_auc: 0.8509 - val_prc: 0.7232\n",
            "Epoch 187/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.8202 - auc: 0.8985 - prc: 0.7810\n",
            "Epoch: 187\n",
            "Train Loss: 0.4843413531780243\n",
            "Val Loss: 0.5298070311546326\n",
            "Train Accuracy: 0.820555567741394\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4843 - accuracy: 0.8206 - auc: 0.8990 - prc: 0.7822 - val_loss: 0.5298 - val_accuracy: 0.7875 - val_auc: 0.8504 - val_prc: 0.7248\n",
            "Epoch 188/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.8198 - auc: 0.9022 - prc: 0.7914\n",
            "Epoch: 188\n",
            "Train Loss: 0.47869980335235596\n",
            "Val Loss: 0.5152454376220703\n",
            "Train Accuracy: 0.8199999928474426\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4787 - accuracy: 0.8200 - auc: 0.9024 - prc: 0.7915 - val_loss: 0.5152 - val_accuracy: 0.8025 - val_auc: 0.8509 - val_prc: 0.7239\n",
            "Epoch 189/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.8258 - auc: 0.9051 - prc: 0.7907\n",
            "Epoch: 189\n",
            "Train Loss: 0.4736359417438507\n",
            "Val Loss: 0.5203394889831543\n",
            "Train Accuracy: 0.8252778053283691\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4736 - accuracy: 0.8253 - auc: 0.9053 - prc: 0.7919 - val_loss: 0.5203 - val_accuracy: 0.7950 - val_auc: 0.8510 - val_prc: 0.7248\n",
            "Epoch 190/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.8245 - auc: 0.8974 - prc: 0.7736\n",
            "Epoch: 190\n",
            "Train Loss: 0.4850711524486542\n",
            "Val Loss: 0.5210781693458557\n",
            "Train Accuracy: 0.824999988079071\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4851 - accuracy: 0.8250 - auc: 0.8978 - prc: 0.7745 - val_loss: 0.5211 - val_accuracy: 0.7950 - val_auc: 0.8507 - val_prc: 0.7236\n",
            "Epoch 191/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4702 - accuracy: 0.8283 - auc: 0.9071 - prc: 0.7972\n",
            "Epoch: 191\n",
            "Train Loss: 0.4700365960597992\n",
            "Val Loss: 0.5194170475006104\n",
            "Train Accuracy: 0.8286111354827881\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4700 - accuracy: 0.8286 - auc: 0.9071 - prc: 0.7968 - val_loss: 0.5194 - val_accuracy: 0.7975 - val_auc: 0.8509 - val_prc: 0.7254\n",
            "Epoch 192/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4713 - accuracy: 0.8272 - auc: 0.9052 - prc: 0.7911\n",
            "Epoch: 192\n",
            "Train Loss: 0.4712635576725006\n",
            "Val Loss: 0.5151853561401367\n",
            "Train Accuracy: 0.8272222280502319\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4713 - accuracy: 0.8272 - auc: 0.9052 - prc: 0.7911 - val_loss: 0.5152 - val_accuracy: 0.7950 - val_auc: 0.8516 - val_prc: 0.7269\n",
            "Epoch 193/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4696 - accuracy: 0.8199 - auc: 0.9059 - prc: 0.7937\n",
            "Epoch: 193\n",
            "Train Loss: 0.4691818356513977\n",
            "Val Loss: 0.5193207263946533\n",
            "Train Accuracy: 0.8199999928474426\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4692 - accuracy: 0.8200 - auc: 0.9061 - prc: 0.7941 - val_loss: 0.5193 - val_accuracy: 0.7950 - val_auc: 0.8513 - val_prc: 0.7264\n",
            "Epoch 194/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.4728 - accuracy: 0.8248 - auc: 0.9043 - prc: 0.7810\n",
            "Epoch: 194\n",
            "Train Loss: 0.4729432463645935\n",
            "Val Loss: 0.5149978399276733\n",
            "Train Accuracy: 0.824999988079071\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4729 - accuracy: 0.8250 - auc: 0.9049 - prc: 0.7834 - val_loss: 0.5150 - val_accuracy: 0.7950 - val_auc: 0.8519 - val_prc: 0.7277\n",
            "Epoch 195/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.8261 - auc: 0.9066 - prc: 0.7976\n",
            "Epoch: 195\n",
            "Train Loss: 0.4688378572463989\n",
            "Val Loss: 0.5109773278236389\n",
            "Train Accuracy: 0.8261111378669739\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4688 - accuracy: 0.8261 - auc: 0.9066 - prc: 0.7976 - val_loss: 0.5110 - val_accuracy: 0.8050 - val_auc: 0.8517 - val_prc: 0.7274\n",
            "Epoch 196/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.8266 - auc: 0.9063 - prc: 0.7938\n",
            "Epoch: 196\n",
            "Train Loss: 0.4676702320575714\n",
            "Val Loss: 0.5061042308807373\n",
            "Train Accuracy: 0.8263888955116272\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4677 - accuracy: 0.8264 - auc: 0.9061 - prc: 0.7938 - val_loss: 0.5061 - val_accuracy: 0.8050 - val_auc: 0.8517 - val_prc: 0.7273\n",
            "Epoch 197/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4699 - accuracy: 0.8255 - auc: 0.9048 - prc: 0.7916\n",
            "Epoch: 197\n",
            "Train Loss: 0.46844127774238586\n",
            "Val Loss: 0.5051873326301575\n",
            "Train Accuracy: 0.8266666531562805\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4684 - accuracy: 0.8267 - auc: 0.9058 - prc: 0.7926 - val_loss: 0.5052 - val_accuracy: 0.8075 - val_auc: 0.8520 - val_prc: 0.7289\n",
            "Epoch 198/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.8316 - auc: 0.9090 - prc: 0.7953\n",
            "Epoch: 198\n",
            "Train Loss: 0.4635039269924164\n",
            "Val Loss: 0.5001273155212402\n",
            "Train Accuracy: 0.8311111330986023\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4635 - accuracy: 0.8311 - auc: 0.9090 - prc: 0.7960 - val_loss: 0.5001 - val_accuracy: 0.8050 - val_auc: 0.8526 - val_prc: 0.7280\n",
            "Epoch 199/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.8314 - auc: 0.9070 - prc: 0.7898\n",
            "Epoch: 199\n",
            "Train Loss: 0.4674493074417114\n",
            "Val Loss: 0.5087558031082153\n",
            "Train Accuracy: 0.8313888907432556\n",
            "Val Accuracy: 0.800000011920929\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4674 - accuracy: 0.8314 - auc: 0.9065 - prc: 0.7877 - val_loss: 0.5088 - val_accuracy: 0.8000 - val_auc: 0.8518 - val_prc: 0.7278\n",
            "Epoch 200/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.4670 - accuracy: 0.8279 - auc: 0.9063 - prc: 0.7977\n",
            "Epoch: 200\n",
            "Train Loss: 0.4655001759529114\n",
            "Val Loss: 0.5161305069923401\n",
            "Train Accuracy: 0.8291666507720947\n",
            "Val Accuracy: 0.7925000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4655 - accuracy: 0.8292 - auc: 0.9071 - prc: 0.7988 - val_loss: 0.5161 - val_accuracy: 0.7925 - val_auc: 0.8520 - val_prc: 0.7300\n",
            "Epoch 201/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.8261 - auc: 0.9074 - prc: 0.7923\n",
            "Epoch: 201\n",
            "Train Loss: 0.46338653564453125\n",
            "Val Loss: 0.508168637752533\n",
            "Train Accuracy: 0.8272222280502319\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4634 - accuracy: 0.8272 - auc: 0.9079 - prc: 0.7926 - val_loss: 0.5082 - val_accuracy: 0.7975 - val_auc: 0.8522 - val_prc: 0.7293\n",
            "Epoch 202/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4591 - accuracy: 0.8330 - auc: 0.9095 - prc: 0.8043\n",
            "Epoch: 202\n",
            "Train Loss: 0.45938536524772644\n",
            "Val Loss: 0.49620041251182556\n",
            "Train Accuracy: 0.8327777981758118\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4594 - accuracy: 0.8328 - auc: 0.9093 - prc: 0.8042 - val_loss: 0.4962 - val_accuracy: 0.8200 - val_auc: 0.8532 - val_prc: 0.7305\n",
            "Epoch 203/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.4576 - accuracy: 0.8273 - auc: 0.9105 - prc: 0.7989\n",
            "Epoch: 203\n",
            "Train Loss: 0.45793864130973816\n",
            "Val Loss: 0.48875167965888977\n",
            "Train Accuracy: 0.8277778029441833\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4579 - accuracy: 0.8278 - auc: 0.9101 - prc: 0.7992 - val_loss: 0.4888 - val_accuracy: 0.8200 - val_auc: 0.8539 - val_prc: 0.7295\n",
            "Epoch 204/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4630 - accuracy: 0.8292 - auc: 0.9076 - prc: 0.7974\n",
            "Epoch: 204\n",
            "Train Loss: 0.46247074007987976\n",
            "Val Loss: 0.5012069344520569\n",
            "Train Accuracy: 0.8294444680213928\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4625 - accuracy: 0.8294 - auc: 0.9078 - prc: 0.7956 - val_loss: 0.5012 - val_accuracy: 0.8050 - val_auc: 0.8530 - val_prc: 0.7309\n",
            "Epoch 205/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4611 - accuracy: 0.8303 - auc: 0.9072 - prc: 0.7919\n",
            "Epoch: 205\n",
            "Train Loss: 0.460397332906723\n",
            "Val Loss: 0.5005581974983215\n",
            "Train Accuracy: 0.8308333158493042\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4604 - accuracy: 0.8308 - auc: 0.9077 - prc: 0.7925 - val_loss: 0.5006 - val_accuracy: 0.8075 - val_auc: 0.8539 - val_prc: 0.7311\n",
            "Epoch 206/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4618 - accuracy: 0.8282 - auc: 0.9073 - prc: 0.7898\n",
            "Epoch: 206\n",
            "Train Loss: 0.4624389708042145\n",
            "Val Loss: 0.4956762194633484\n",
            "Train Accuracy: 0.8274999856948853\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.4624 - accuracy: 0.8275 - auc: 0.9069 - prc: 0.7891 - val_loss: 0.4957 - val_accuracy: 0.8125 - val_auc: 0.8542 - val_prc: 0.7330\n",
            "Epoch 207/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4595 - accuracy: 0.8323 - auc: 0.9085 - prc: 0.7972\n",
            "Epoch: 207\n",
            "Train Loss: 0.45882534980773926\n",
            "Val Loss: 0.49211177229881287\n",
            "Train Accuracy: 0.8327777981758118\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4588 - accuracy: 0.8328 - auc: 0.9089 - prc: 0.7978 - val_loss: 0.4921 - val_accuracy: 0.8175 - val_auc: 0.8546 - val_prc: 0.7330\n",
            "Epoch 208/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.8297 - auc: 0.9091 - prc: 0.7938\n",
            "Epoch: 208\n",
            "Train Loss: 0.45882201194763184\n",
            "Val Loss: 0.48828911781311035\n",
            "Train Accuracy: 0.8294444680213928\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4588 - accuracy: 0.8294 - auc: 0.9085 - prc: 0.7929 - val_loss: 0.4883 - val_accuracy: 0.8175 - val_auc: 0.8549 - val_prc: 0.7328\n",
            "Epoch 209/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4586 - accuracy: 0.8363 - auc: 0.9092 - prc: 0.7890\n",
            "Epoch: 209\n",
            "Train Loss: 0.4582549035549164\n",
            "Val Loss: 0.5161651968955994\n",
            "Train Accuracy: 0.8366666436195374\n",
            "Val Accuracy: 0.7925000190734863\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4583 - accuracy: 0.8367 - auc: 0.9094 - prc: 0.7894 - val_loss: 0.5162 - val_accuracy: 0.7925 - val_auc: 0.8539 - val_prc: 0.7350\n",
            "Epoch 210/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4532 - accuracy: 0.8342 - auc: 0.9111 - prc: 0.7981\n",
            "Epoch: 210\n",
            "Train Loss: 0.45217791199684143\n",
            "Val Loss: 0.49918705224990845\n",
            "Train Accuracy: 0.8358333110809326\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4522 - accuracy: 0.8358 - auc: 0.9121 - prc: 0.8002 - val_loss: 0.4992 - val_accuracy: 0.8025 - val_auc: 0.8545 - val_prc: 0.7346\n",
            "Epoch 211/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.8299 - auc: 0.9107 - prc: 0.7987\n",
            "Epoch: 211\n",
            "Train Loss: 0.45431193709373474\n",
            "Val Loss: 0.4862237870693207\n",
            "Train Accuracy: 0.8299999833106995\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4543 - accuracy: 0.8300 - auc: 0.9104 - prc: 0.7983 - val_loss: 0.4862 - val_accuracy: 0.8200 - val_auc: 0.8555 - val_prc: 0.7346\n",
            "Epoch 212/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4540 - accuracy: 0.8299 - auc: 0.9100 - prc: 0.8032\n",
            "Epoch: 212\n",
            "Train Loss: 0.45457708835601807\n",
            "Val Loss: 0.4962230324745178\n",
            "Train Accuracy: 0.8294444680213928\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4546 - accuracy: 0.8294 - auc: 0.9095 - prc: 0.8005 - val_loss: 0.4962 - val_accuracy: 0.8075 - val_auc: 0.8554 - val_prc: 0.7352\n",
            "Epoch 213/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4487 - accuracy: 0.8323 - auc: 0.9131 - prc: 0.8014\n",
            "Epoch: 213\n",
            "Train Loss: 0.45017075538635254\n",
            "Val Loss: 0.48984116315841675\n",
            "Train Accuracy: 0.8322222232818604\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4502 - accuracy: 0.8322 - auc: 0.9123 - prc: 0.8011 - val_loss: 0.4898 - val_accuracy: 0.8150 - val_auc: 0.8554 - val_prc: 0.7345\n",
            "Epoch 214/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.8302 - auc: 0.9148 - prc: 0.8174\n",
            "Epoch: 214\n",
            "Train Loss: 0.4462951123714447\n",
            "Val Loss: 0.4911029040813446\n",
            "Train Accuracy: 0.8297222256660461\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4463 - accuracy: 0.8297 - auc: 0.9144 - prc: 0.8157 - val_loss: 0.4911 - val_accuracy: 0.8150 - val_auc: 0.8561 - val_prc: 0.7353\n",
            "Epoch 215/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.8371 - auc: 0.9143 - prc: 0.8085\n",
            "Epoch: 215\n",
            "Train Loss: 0.44665059447288513\n",
            "Val Loss: 0.4963395595550537\n",
            "Train Accuracy: 0.8369444608688354\n",
            "Val Accuracy: 0.8100000023841858\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4467 - accuracy: 0.8369 - auc: 0.9139 - prc: 0.8085 - val_loss: 0.4963 - val_accuracy: 0.8100 - val_auc: 0.8558 - val_prc: 0.7387\n",
            "Epoch 216/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4481 - accuracy: 0.8362 - auc: 0.9123 - prc: 0.8142\n",
            "Epoch: 216\n",
            "Train Loss: 0.44830602407455444\n",
            "Val Loss: 0.48945289850234985\n",
            "Train Accuracy: 0.8355555534362793\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4483 - accuracy: 0.8356 - auc: 0.9121 - prc: 0.8137 - val_loss: 0.4895 - val_accuracy: 0.8150 - val_auc: 0.8560 - val_prc: 0.7366\n",
            "Epoch 217/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4519 - accuracy: 0.8355 - auc: 0.9099 - prc: 0.8009\n",
            "Epoch: 217\n",
            "Train Loss: 0.45340871810913086\n",
            "Val Loss: 0.5007100105285645\n",
            "Train Accuracy: 0.835277795791626\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4534 - accuracy: 0.8353 - auc: 0.9093 - prc: 0.7984 - val_loss: 0.5007 - val_accuracy: 0.7975 - val_auc: 0.8561 - val_prc: 0.7389\n",
            "Epoch 218/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.8388 - auc: 0.9142 - prc: 0.8120\n",
            "Epoch: 218\n",
            "Train Loss: 0.44592294096946716\n",
            "Val Loss: 0.4904128909111023\n",
            "Train Accuracy: 0.8388888835906982\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4459 - accuracy: 0.8389 - auc: 0.9143 - prc: 0.8119 - val_loss: 0.4904 - val_accuracy: 0.8125 - val_auc: 0.8564 - val_prc: 0.7386\n",
            "Epoch 219/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4451 - accuracy: 0.8338 - auc: 0.9141 - prc: 0.8057\n",
            "Epoch: 219\n",
            "Train Loss: 0.4450153708457947\n",
            "Val Loss: 0.4894745945930481\n",
            "Train Accuracy: 0.8338888883590698\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4450 - accuracy: 0.8339 - auc: 0.9139 - prc: 0.8055 - val_loss: 0.4895 - val_accuracy: 0.8150 - val_auc: 0.8567 - val_prc: 0.7375\n",
            "Epoch 220/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4409 - accuracy: 0.8398 - auc: 0.9165 - prc: 0.8143\n",
            "Epoch: 220\n",
            "Train Loss: 0.44136741757392883\n",
            "Val Loss: 0.49165016412734985\n",
            "Train Accuracy: 0.8394444584846497\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4414 - accuracy: 0.8394 - auc: 0.9161 - prc: 0.8124 - val_loss: 0.4917 - val_accuracy: 0.8125 - val_auc: 0.8568 - val_prc: 0.7384\n",
            "Epoch 221/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.8426 - auc: 0.9163 - prc: 0.8141\n",
            "Epoch: 221\n",
            "Train Loss: 0.43944162130355835\n",
            "Val Loss: 0.49001896381378174\n",
            "Train Accuracy: 0.8430555462837219\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.4394 - accuracy: 0.8431 - auc: 0.9166 - prc: 0.8144 - val_loss: 0.4900 - val_accuracy: 0.8150 - val_auc: 0.8570 - val_prc: 0.7388\n",
            "Epoch 222/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4384 - accuracy: 0.8405 - auc: 0.9172 - prc: 0.8096\n",
            "Epoch: 222\n",
            "Train Loss: 0.4374004304409027\n",
            "Val Loss: 0.49854105710983276\n",
            "Train Accuracy: 0.8411111235618591\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.4374 - accuracy: 0.8411 - auc: 0.9177 - prc: 0.8101 - val_loss: 0.4985 - val_accuracy: 0.8025 - val_auc: 0.8570 - val_prc: 0.7410\n",
            "Epoch 223/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4444 - accuracy: 0.8379 - auc: 0.9127 - prc: 0.8077\n",
            "Epoch: 223\n",
            "Train Loss: 0.4433044493198395\n",
            "Val Loss: 0.4944613575935364\n",
            "Train Accuracy: 0.8383333086967468\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4433 - accuracy: 0.8383 - auc: 0.9133 - prc: 0.8091 - val_loss: 0.4945 - val_accuracy: 0.8075 - val_auc: 0.8569 - val_prc: 0.7412\n",
            "Epoch 224/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4379 - accuracy: 0.8401 - auc: 0.9164 - prc: 0.8180\n",
            "Epoch: 224\n",
            "Train Loss: 0.43828973174095154\n",
            "Val Loss: 0.470407098531723\n",
            "Train Accuracy: 0.8399999737739563\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4383 - accuracy: 0.8400 - auc: 0.9164 - prc: 0.8182 - val_loss: 0.4704 - val_accuracy: 0.8225 - val_auc: 0.8573 - val_prc: 0.7390\n",
            "Epoch 225/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.8406 - auc: 0.9169 - prc: 0.8098\n",
            "Epoch: 225\n",
            "Train Loss: 0.43917325139045715\n",
            "Val Loss: 0.5022687315940857\n",
            "Train Accuracy: 0.8405555486679077\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.4392 - accuracy: 0.8406 - auc: 0.9169 - prc: 0.8098 - val_loss: 0.5023 - val_accuracy: 0.7950 - val_auc: 0.8570 - val_prc: 0.7419\n",
            "Epoch 226/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4403 - accuracy: 0.8400 - auc: 0.9153 - prc: 0.8068\n",
            "Epoch: 226\n",
            "Train Loss: 0.43999552726745605\n",
            "Val Loss: 0.4977984130382538\n",
            "Train Accuracy: 0.839722216129303\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.4400 - accuracy: 0.8397 - auc: 0.9155 - prc: 0.8075 - val_loss: 0.4978 - val_accuracy: 0.8050 - val_auc: 0.8573 - val_prc: 0.7419\n",
            "Epoch 227/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4354 - accuracy: 0.8447 - auc: 0.9177 - prc: 0.8131\n",
            "Epoch: 227\n",
            "Train Loss: 0.435085266828537\n",
            "Val Loss: 0.4944385588169098\n",
            "Train Accuracy: 0.8447222113609314\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4351 - accuracy: 0.8447 - auc: 0.9179 - prc: 0.8131 - val_loss: 0.4944 - val_accuracy: 0.8075 - val_auc: 0.8576 - val_prc: 0.7414\n",
            "Epoch 228/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.8367 - auc: 0.9170 - prc: 0.8160\n",
            "Epoch: 228\n",
            "Train Loss: 0.436820387840271\n",
            "Val Loss: 0.4763000011444092\n",
            "Train Accuracy: 0.8366666436195374\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4368 - accuracy: 0.8367 - auc: 0.9170 - prc: 0.8160 - val_loss: 0.4763 - val_accuracy: 0.8175 - val_auc: 0.8576 - val_prc: 0.7401\n",
            "Epoch 229/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4303 - accuracy: 0.8463 - auc: 0.9202 - prc: 0.8081\n",
            "Epoch: 229\n",
            "Train Loss: 0.43150606751441956\n",
            "Val Loss: 0.48462164402008057\n",
            "Train Accuracy: 0.8455555438995361\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4315 - accuracy: 0.8456 - auc: 0.9200 - prc: 0.8094 - val_loss: 0.4846 - val_accuracy: 0.8125 - val_auc: 0.8580 - val_prc: 0.7422\n",
            "Epoch 230/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.8349 - auc: 0.9152 - prc: 0.8175\n",
            "Epoch: 230\n",
            "Train Loss: 0.43848535418510437\n",
            "Val Loss: 0.4793523848056793\n",
            "Train Accuracy: 0.8347222208976746\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4385 - accuracy: 0.8347 - auc: 0.9150 - prc: 0.8172 - val_loss: 0.4794 - val_accuracy: 0.8150 - val_auc: 0.8585 - val_prc: 0.7411\n",
            "Epoch 231/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.8459 - auc: 0.9209 - prc: 0.8181\n",
            "Epoch: 231\n",
            "Train Loss: 0.429880291223526\n",
            "Val Loss: 0.5006632804870605\n",
            "Train Accuracy: 0.8452777862548828\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4299 - accuracy: 0.8453 - auc: 0.9204 - prc: 0.8194 - val_loss: 0.5007 - val_accuracy: 0.7975 - val_auc: 0.8580 - val_prc: 0.7441\n",
            "Epoch 232/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.8331 - auc: 0.9148 - prc: 0.8103\n",
            "Epoch: 232\n",
            "Train Loss: 0.4375142753124237\n",
            "Val Loss: 0.48802420496940613\n",
            "Train Accuracy: 0.8330555558204651\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4375 - accuracy: 0.8331 - auc: 0.9151 - prc: 0.8111 - val_loss: 0.4880 - val_accuracy: 0.8075 - val_auc: 0.8586 - val_prc: 0.7452\n",
            "Epoch 233/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4266 - accuracy: 0.8400 - auc: 0.9218 - prc: 0.8263\n",
            "Epoch: 233\n",
            "Train Loss: 0.42751702666282654\n",
            "Val Loss: 0.4775570333003998\n",
            "Train Accuracy: 0.839722216129303\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4275 - accuracy: 0.8397 - auc: 0.9214 - prc: 0.8252 - val_loss: 0.4776 - val_accuracy: 0.8175 - val_auc: 0.8590 - val_prc: 0.7443\n",
            "Epoch 234/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.8386 - auc: 0.9202 - prc: 0.8196\n",
            "Epoch: 234\n",
            "Train Loss: 0.427921861410141\n",
            "Val Loss: 0.47046253085136414\n",
            "Train Accuracy: 0.8386111259460449\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4279 - accuracy: 0.8386 - auc: 0.9202 - prc: 0.8196 - val_loss: 0.4705 - val_accuracy: 0.8250 - val_auc: 0.8594 - val_prc: 0.7437\n",
            "Epoch 235/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.8406 - auc: 0.9185 - prc: 0.8127\n",
            "Epoch: 235\n",
            "Train Loss: 0.4312327206134796\n",
            "Val Loss: 0.4723341763019562\n",
            "Train Accuracy: 0.8405555486679077\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4312 - accuracy: 0.8406 - auc: 0.9185 - prc: 0.8127 - val_loss: 0.4723 - val_accuracy: 0.8200 - val_auc: 0.8595 - val_prc: 0.7441\n",
            "Epoch 236/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.8395 - auc: 0.9156 - prc: 0.8114\n",
            "Epoch: 236\n",
            "Train Loss: 0.4347723722457886\n",
            "Val Loss: 0.4888897240161896\n",
            "Train Accuracy: 0.840833306312561\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4348 - accuracy: 0.8408 - auc: 0.9165 - prc: 0.8136 - val_loss: 0.4889 - val_accuracy: 0.8075 - val_auc: 0.8594 - val_prc: 0.7473\n",
            "Epoch 237/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4326 - accuracy: 0.8463 - auc: 0.9175 - prc: 0.8203\n",
            "Epoch: 237\n",
            "Train Loss: 0.4324524998664856\n",
            "Val Loss: 0.47977501153945923\n",
            "Train Accuracy: 0.8452777862548828\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4325 - accuracy: 0.8453 - auc: 0.9173 - prc: 0.8195 - val_loss: 0.4798 - val_accuracy: 0.8125 - val_auc: 0.8597 - val_prc: 0.7464\n",
            "Epoch 238/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4283 - accuracy: 0.8451 - auc: 0.9196 - prc: 0.8205\n",
            "Epoch: 238\n",
            "Train Loss: 0.42723774909973145\n",
            "Val Loss: 0.4775802493095398\n",
            "Train Accuracy: 0.8452777862548828\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4272 - accuracy: 0.8453 - auc: 0.9201 - prc: 0.8216 - val_loss: 0.4776 - val_accuracy: 0.8150 - val_auc: 0.8597 - val_prc: 0.7468\n",
            "Epoch 239/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.8478 - auc: 0.9203 - prc: 0.8151\n",
            "Epoch: 239\n",
            "Train Loss: 0.42880979180336\n",
            "Val Loss: 0.49250516295433044\n",
            "Train Accuracy: 0.8472222089767456\n",
            "Val Accuracy: 0.800000011920929\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4288 - accuracy: 0.8472 - auc: 0.9203 - prc: 0.8161 - val_loss: 0.4925 - val_accuracy: 0.8000 - val_auc: 0.8594 - val_prc: 0.7472\n",
            "Epoch 240/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8474 - auc: 0.9225 - prc: 0.8252\n",
            "Epoch: 240\n",
            "Train Loss: 0.4225052297115326\n",
            "Val Loss: 0.4720696210861206\n",
            "Train Accuracy: 0.8469444513320923\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4225 - accuracy: 0.8469 - auc: 0.9220 - prc: 0.8246 - val_loss: 0.4721 - val_accuracy: 0.8175 - val_auc: 0.8602 - val_prc: 0.7466\n",
            "Epoch 241/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4279 - accuracy: 0.8430 - auc: 0.9194 - prc: 0.8216\n",
            "Epoch: 241\n",
            "Train Loss: 0.42841416597366333\n",
            "Val Loss: 0.49045857787132263\n",
            "Train Accuracy: 0.8427777886390686\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4284 - accuracy: 0.8428 - auc: 0.9189 - prc: 0.8197 - val_loss: 0.4905 - val_accuracy: 0.8025 - val_auc: 0.8601 - val_prc: 0.7484\n",
            "Epoch 242/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4222 - accuracy: 0.8458 - auc: 0.9222 - prc: 0.8280\n",
            "Epoch: 242\n",
            "Train Loss: 0.42265576124191284\n",
            "Val Loss: 0.48023247718811035\n",
            "Train Accuracy: 0.8455555438995361\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4227 - accuracy: 0.8456 - auc: 0.9220 - prc: 0.8271 - val_loss: 0.4802 - val_accuracy: 0.8125 - val_auc: 0.8595 - val_prc: 0.7476\n",
            "Epoch 243/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8381 - auc: 0.9226 - prc: 0.8264\n",
            "Epoch: 243\n",
            "Train Loss: 0.42156174778938293\n",
            "Val Loss: 0.47075173258781433\n",
            "Train Accuracy: 0.8380555510520935\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.4216 - accuracy: 0.8381 - auc: 0.9229 - prc: 0.8267 - val_loss: 0.4708 - val_accuracy: 0.8200 - val_auc: 0.8600 - val_prc: 0.7455\n",
            "Epoch 244/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4260 - accuracy: 0.8431 - auc: 0.9202 - prc: 0.8239\n",
            "Epoch: 244\n",
            "Train Loss: 0.4260037839412689\n",
            "Val Loss: 0.501213788986206\n",
            "Train Accuracy: 0.8430555462837219\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4260 - accuracy: 0.8431 - auc: 0.9202 - prc: 0.8239 - val_loss: 0.5012 - val_accuracy: 0.7950 - val_auc: 0.8597 - val_prc: 0.7500\n",
            "Epoch 245/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8454 - auc: 0.9225 - prc: 0.8279\n",
            "Epoch: 245\n",
            "Train Loss: 0.4219478666782379\n",
            "Val Loss: 0.4778011441230774\n",
            "Train Accuracy: 0.8452777862548828\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4219 - accuracy: 0.8453 - auc: 0.9226 - prc: 0.8278 - val_loss: 0.4778 - val_accuracy: 0.8125 - val_auc: 0.8604 - val_prc: 0.7480\n",
            "Epoch 246/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4167 - accuracy: 0.8508 - auc: 0.9245 - prc: 0.8301\n",
            "Epoch: 246\n",
            "Train Loss: 0.4198313355445862\n",
            "Val Loss: 0.46878647804260254\n",
            "Train Accuracy: 0.8497222065925598\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4198 - accuracy: 0.8497 - auc: 0.9230 - prc: 0.8225 - val_loss: 0.4688 - val_accuracy: 0.8200 - val_auc: 0.8604 - val_prc: 0.7476\n",
            "Epoch 247/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.8467 - auc: 0.9202 - prc: 0.8223\n",
            "Epoch: 247\n",
            "Train Loss: 0.42470264434814453\n",
            "Val Loss: 0.4901790916919708\n",
            "Train Accuracy: 0.846666693687439\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4247 - accuracy: 0.8467 - auc: 0.9202 - prc: 0.8223 - val_loss: 0.4902 - val_accuracy: 0.7975 - val_auc: 0.8603 - val_prc: 0.7512\n",
            "Epoch 248/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8507 - auc: 0.9238 - prc: 0.8254\n",
            "Epoch: 248\n",
            "Train Loss: 0.4186096787452698\n",
            "Val Loss: 0.4783017039299011\n",
            "Train Accuracy: 0.8508333563804626\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4186 - accuracy: 0.8508 - auc: 0.9234 - prc: 0.8258 - val_loss: 0.4783 - val_accuracy: 0.8125 - val_auc: 0.8606 - val_prc: 0.7487\n",
            "Epoch 249/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8479 - auc: 0.9244 - prc: 0.8297\n",
            "Epoch: 249\n",
            "Train Loss: 0.4167427122592926\n",
            "Val Loss: 0.4640345871448517\n",
            "Train Accuracy: 0.847777783870697\n",
            "Val Accuracy: 0.8299999833106995\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4167 - accuracy: 0.8478 - auc: 0.9239 - prc: 0.8307 - val_loss: 0.4640 - val_accuracy: 0.8300 - val_auc: 0.8614 - val_prc: 0.7480\n",
            "Epoch 250/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4158 - accuracy: 0.8548 - auc: 0.9243 - prc: 0.8356\n",
            "Epoch: 250\n",
            "Train Loss: 0.4162151515483856\n",
            "Val Loss: 0.47217926383018494\n",
            "Train Accuracy: 0.8550000190734863\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4162 - accuracy: 0.8550 - auc: 0.9242 - prc: 0.8362 - val_loss: 0.4722 - val_accuracy: 0.8150 - val_auc: 0.8608 - val_prc: 0.7495\n",
            "Epoch 251/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4177 - accuracy: 0.8518 - auc: 0.9235 - prc: 0.8357\n",
            "Epoch: 251\n",
            "Train Loss: 0.4173767864704132\n",
            "Val Loss: 0.4847862720489502\n",
            "Train Accuracy: 0.8516666889190674\n",
            "Val Accuracy: 0.800000011920929\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4174 - accuracy: 0.8517 - auc: 0.9237 - prc: 0.8357 - val_loss: 0.4848 - val_accuracy: 0.8000 - val_auc: 0.8605 - val_prc: 0.7505\n",
            "Epoch 252/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.8489 - auc: 0.9261 - prc: 0.8342\n",
            "Epoch: 252\n",
            "Train Loss: 0.4119343161582947\n",
            "Val Loss: 0.47437867522239685\n",
            "Train Accuracy: 0.8488888740539551\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4119 - accuracy: 0.8489 - auc: 0.9261 - prc: 0.8342 - val_loss: 0.4744 - val_accuracy: 0.8150 - val_auc: 0.8612 - val_prc: 0.7514\n",
            "Epoch 253/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4136 - accuracy: 0.8517 - auc: 0.9253 - prc: 0.8293\n",
            "Epoch: 253\n",
            "Train Loss: 0.4126165807247162\n",
            "Val Loss: 0.46048691868782043\n",
            "Train Accuracy: 0.852222204208374\n",
            "Val Accuracy: 0.8299999833106995\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4126 - accuracy: 0.8522 - auc: 0.9259 - prc: 0.8308 - val_loss: 0.4605 - val_accuracy: 0.8300 - val_auc: 0.8615 - val_prc: 0.7479\n",
            "Epoch 254/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4173 - accuracy: 0.8494 - auc: 0.9235 - prc: 0.8264\n",
            "Epoch: 254\n",
            "Train Loss: 0.41769590973854065\n",
            "Val Loss: 0.4802647829055786\n",
            "Train Accuracy: 0.8488888740539551\n",
            "Val Accuracy: 0.8100000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4177 - accuracy: 0.8489 - auc: 0.9234 - prc: 0.8263 - val_loss: 0.4803 - val_accuracy: 0.8100 - val_auc: 0.8613 - val_prc: 0.7523\n",
            "Epoch 255/300\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8483 - auc: 0.9238 - prc: 0.8289\n",
            "Epoch: 255\n",
            "Train Loss: 0.41488006711006165\n",
            "Val Loss: 0.4713578522205353\n",
            "Train Accuracy: 0.8494444489479065\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4149 - accuracy: 0.8494 - auc: 0.9245 - prc: 0.8298 - val_loss: 0.4714 - val_accuracy: 0.8150 - val_auc: 0.8618 - val_prc: 0.7526\n",
            "Epoch 256/300\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8455 - auc: 0.9272 - prc: 0.8352\n",
            "Epoch: 256\n",
            "Train Loss: 0.4088316559791565\n",
            "Val Loss: 0.461430162191391\n",
            "Train Accuracy: 0.8469444513320923\n",
            "Val Accuracy: 0.8274999856948853\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4088 - accuracy: 0.8469 - auc: 0.9279 - prc: 0.8363 - val_loss: 0.4614 - val_accuracy: 0.8275 - val_auc: 0.8620 - val_prc: 0.7507\n",
            "Epoch 257/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8535 - auc: 0.9266 - prc: 0.8260\n",
            "Epoch: 257\n",
            "Train Loss: 0.41079941391944885\n",
            "Val Loss: 0.4674855172634125\n",
            "Train Accuracy: 0.8527777791023254\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4108 - accuracy: 0.8528 - auc: 0.9263 - prc: 0.8251 - val_loss: 0.4675 - val_accuracy: 0.8250 - val_auc: 0.8620 - val_prc: 0.7513\n",
            "Epoch 258/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8467 - auc: 0.9235 - prc: 0.8302\n",
            "Epoch: 258\n",
            "Train Loss: 0.4148953855037689\n",
            "Val Loss: 0.45593830943107605\n",
            "Train Accuracy: 0.846666693687439\n",
            "Val Accuracy: 0.8349999785423279\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4149 - accuracy: 0.8467 - auc: 0.9235 - prc: 0.8302 - val_loss: 0.4559 - val_accuracy: 0.8350 - val_auc: 0.8628 - val_prc: 0.7506\n",
            "Epoch 259/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4091 - accuracy: 0.8494 - auc: 0.9272 - prc: 0.8361\n",
            "Epoch: 259\n",
            "Train Loss: 0.4089398980140686\n",
            "Val Loss: 0.4768829345703125\n",
            "Train Accuracy: 0.8488888740539551\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.4089 - accuracy: 0.8489 - auc: 0.9272 - prc: 0.8359 - val_loss: 0.4769 - val_accuracy: 0.8125 - val_auc: 0.8620 - val_prc: 0.7532\n",
            "Epoch 260/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8494 - auc: 0.9259 - prc: 0.8354\n",
            "Epoch: 260\n",
            "Train Loss: 0.4091631770133972\n",
            "Val Loss: 0.4672776758670807\n",
            "Train Accuracy: 0.8494444489479065\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4092 - accuracy: 0.8494 - auc: 0.9265 - prc: 0.8366 - val_loss: 0.4673 - val_accuracy: 0.8200 - val_auc: 0.8630 - val_prc: 0.7520\n",
            "Epoch 261/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4112 - accuracy: 0.8487 - auc: 0.9255 - prc: 0.8288\n",
            "Epoch: 261\n",
            "Train Loss: 0.4125974178314209\n",
            "Val Loss: 0.4619014859199524\n",
            "Train Accuracy: 0.8472222089767456\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4126 - accuracy: 0.8472 - auc: 0.9249 - prc: 0.8280 - val_loss: 0.4619 - val_accuracy: 0.8250 - val_auc: 0.8630 - val_prc: 0.7532\n",
            "Epoch 262/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4060 - accuracy: 0.8528 - auc: 0.9277 - prc: 0.8389\n",
            "Epoch: 262\n",
            "Train Loss: 0.4076954126358032\n",
            "Val Loss: 0.454337477684021\n",
            "Train Accuracy: 0.852222204208374\n",
            "Val Accuracy: 0.8349999785423279\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4077 - accuracy: 0.8522 - auc: 0.9270 - prc: 0.8377 - val_loss: 0.4543 - val_accuracy: 0.8350 - val_auc: 0.8631 - val_prc: 0.7518\n",
            "Epoch 263/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4062 - accuracy: 0.8600 - auc: 0.9282 - prc: 0.8361\n",
            "Epoch: 263\n",
            "Train Loss: 0.4061749279499054\n",
            "Val Loss: 0.47230178117752075\n",
            "Train Accuracy: 0.8600000143051147\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4062 - accuracy: 0.8600 - auc: 0.9282 - prc: 0.8361 - val_loss: 0.4723 - val_accuracy: 0.8175 - val_auc: 0.8619 - val_prc: 0.7539\n",
            "Epoch 264/300\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4108 - accuracy: 0.8467 - auc: 0.9252 - prc: 0.8335\n",
            "Epoch: 264\n",
            "Train Loss: 0.41155973076820374\n",
            "Val Loss: 0.46659719944000244\n",
            "Train Accuracy: 0.8461111187934875\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4116 - accuracy: 0.8461 - auc: 0.9251 - prc: 0.8336 - val_loss: 0.4666 - val_accuracy: 0.8200 - val_auc: 0.8624 - val_prc: 0.7528\n",
            "Epoch 265/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8493 - auc: 0.9270 - prc: 0.8399\n",
            "Epoch: 265\n",
            "Train Loss: 0.40811073780059814\n",
            "Val Loss: 0.47507867217063904\n",
            "Train Accuracy: 0.8497222065925598\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4081 - accuracy: 0.8497 - auc: 0.9266 - prc: 0.8394 - val_loss: 0.4751 - val_accuracy: 0.8075 - val_auc: 0.8626 - val_prc: 0.7540\n",
            "Epoch 266/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.8491 - auc: 0.9278 - prc: 0.8383\n",
            "Epoch: 266\n",
            "Train Loss: 0.4057618975639343\n",
            "Val Loss: 0.4640542268753052\n",
            "Train Accuracy: 0.8497222065925598\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4058 - accuracy: 0.8497 - auc: 0.9285 - prc: 0.8399 - val_loss: 0.4641 - val_accuracy: 0.8250 - val_auc: 0.8631 - val_prc: 0.7555\n",
            "Epoch 267/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4057 - accuracy: 0.8554 - auc: 0.9278 - prc: 0.8323\n",
            "Epoch: 267\n",
            "Train Loss: 0.4066373109817505\n",
            "Val Loss: 0.46196597814559937\n",
            "Train Accuracy: 0.8550000190734863\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4066 - accuracy: 0.8550 - auc: 0.9274 - prc: 0.8317 - val_loss: 0.4620 - val_accuracy: 0.8225 - val_auc: 0.8639 - val_prc: 0.7555\n",
            "Epoch 268/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4054 - accuracy: 0.8499 - auc: 0.9279 - prc: 0.8379\n",
            "Epoch: 268\n",
            "Train Loss: 0.405253529548645\n",
            "Val Loss: 0.4517499804496765\n",
            "Train Accuracy: 0.8497222065925598\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4053 - accuracy: 0.8497 - auc: 0.9281 - prc: 0.8386 - val_loss: 0.4517 - val_accuracy: 0.8375 - val_auc: 0.8638 - val_prc: 0.7519\n",
            "Epoch 269/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4073 - accuracy: 0.8513 - auc: 0.9271 - prc: 0.8307\n",
            "Epoch: 269\n",
            "Train Loss: 0.40711653232574463\n",
            "Val Loss: 0.46302881836891174\n",
            "Train Accuracy: 0.851111114025116\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4071 - accuracy: 0.8511 - auc: 0.9271 - prc: 0.8304 - val_loss: 0.4630 - val_accuracy: 0.8225 - val_auc: 0.8627 - val_prc: 0.7527\n",
            "Epoch 270/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3990 - accuracy: 0.8566 - auc: 0.9314 - prc: 0.8443\n",
            "Epoch: 270\n",
            "Train Loss: 0.3993532657623291\n",
            "Val Loss: 0.4616084396839142\n",
            "Train Accuracy: 0.8558333516120911\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3994 - accuracy: 0.8558 - auc: 0.9311 - prc: 0.8438 - val_loss: 0.4616 - val_accuracy: 0.8225 - val_auc: 0.8629 - val_prc: 0.7534\n",
            "Epoch 271/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4023 - accuracy: 0.8550 - auc: 0.9293 - prc: 0.8398\n",
            "Epoch: 271\n",
            "Train Loss: 0.4022621810436249\n",
            "Val Loss: 0.45153191685676575\n",
            "Train Accuracy: 0.8550000190734863\n",
            "Val Accuracy: 0.8324999809265137\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4023 - accuracy: 0.8550 - auc: 0.9292 - prc: 0.8391 - val_loss: 0.4515 - val_accuracy: 0.8325 - val_auc: 0.8641 - val_prc: 0.7530\n",
            "Epoch 272/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.4033 - accuracy: 0.8558 - auc: 0.9282 - prc: 0.8367\n",
            "Epoch: 272\n",
            "Train Loss: 0.40304845571517944\n",
            "Val Loss: 0.4623698890209198\n",
            "Train Accuracy: 0.8558333516120911\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4030 - accuracy: 0.8558 - auc: 0.9284 - prc: 0.8369 - val_loss: 0.4624 - val_accuracy: 0.8225 - val_auc: 0.8629 - val_prc: 0.7537\n",
            "Epoch 273/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4038 - accuracy: 0.8510 - auc: 0.9285 - prc: 0.8390\n",
            "Epoch: 273\n",
            "Train Loss: 0.40201371908187866\n",
            "Val Loss: 0.4512424170970917\n",
            "Train Accuracy: 0.852222204208374\n",
            "Val Accuracy: 0.8349999785423279\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4020 - accuracy: 0.8522 - auc: 0.9296 - prc: 0.8410 - val_loss: 0.4512 - val_accuracy: 0.8350 - val_auc: 0.8637 - val_prc: 0.7535\n",
            "Epoch 274/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.8589 - auc: 0.9336 - prc: 0.8437\n",
            "Epoch: 274\n",
            "Train Loss: 0.39401721954345703\n",
            "Val Loss: 0.45849543809890747\n",
            "Train Accuracy: 0.8588888645172119\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3940 - accuracy: 0.8589 - auc: 0.9336 - prc: 0.8437 - val_loss: 0.4585 - val_accuracy: 0.8250 - val_auc: 0.8635 - val_prc: 0.7540\n",
            "Epoch 275/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4011 - accuracy: 0.8583 - auc: 0.9290 - prc: 0.8367\n",
            "Epoch: 275\n",
            "Train Loss: 0.4010745882987976\n",
            "Val Loss: 0.4541478753089905\n",
            "Train Accuracy: 0.8583333492279053\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.4011 - accuracy: 0.8583 - auc: 0.9290 - prc: 0.8367 - val_loss: 0.4541 - val_accuracy: 0.8250 - val_auc: 0.8636 - val_prc: 0.7534\n",
            "Epoch 276/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4032 - accuracy: 0.8546 - auc: 0.9282 - prc: 0.8393\n",
            "Epoch: 276\n",
            "Train Loss: 0.40175965428352356\n",
            "Val Loss: 0.46005919575691223\n",
            "Train Accuracy: 0.8558333516120911\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4018 - accuracy: 0.8558 - auc: 0.9288 - prc: 0.8401 - val_loss: 0.4601 - val_accuracy: 0.8250 - val_auc: 0.8632 - val_prc: 0.7560\n",
            "Epoch 277/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3979 - accuracy: 0.8514 - auc: 0.9299 - prc: 0.8328\n",
            "Epoch: 277\n",
            "Train Loss: 0.3971649706363678\n",
            "Val Loss: 0.45194998383522034\n",
            "Train Accuracy: 0.852222204208374\n",
            "Val Accuracy: 0.8299999833106995\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3972 - accuracy: 0.8522 - auc: 0.9306 - prc: 0.8346 - val_loss: 0.4519 - val_accuracy: 0.8300 - val_auc: 0.8640 - val_prc: 0.7556\n",
            "Epoch 278/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3968 - accuracy: 0.8556 - auc: 0.9311 - prc: 0.8406\n",
            "Epoch: 278\n",
            "Train Loss: 0.3971494436264038\n",
            "Val Loss: 0.46580058336257935\n",
            "Train Accuracy: 0.8552777767181396\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3971 - accuracy: 0.8553 - auc: 0.9311 - prc: 0.8407 - val_loss: 0.4658 - val_accuracy: 0.8175 - val_auc: 0.8628 - val_prc: 0.7564\n",
            "Epoch 279/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3947 - accuracy: 0.8565 - auc: 0.9322 - prc: 0.8415\n",
            "Epoch: 279\n",
            "Train Loss: 0.397404283285141\n",
            "Val Loss: 0.47014573216438293\n",
            "Train Accuracy: 0.8558333516120911\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3974 - accuracy: 0.8558 - auc: 0.9306 - prc: 0.8409 - val_loss: 0.4701 - val_accuracy: 0.8125 - val_auc: 0.8627 - val_prc: 0.7564\n",
            "Epoch 280/300\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.3978 - accuracy: 0.8561 - auc: 0.9297 - prc: 0.8403\n",
            "Epoch: 280\n",
            "Train Loss: 0.3968939185142517\n",
            "Val Loss: 0.4623049795627594\n",
            "Train Accuracy: 0.8569444417953491\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3969 - accuracy: 0.8569 - auc: 0.9301 - prc: 0.8414 - val_loss: 0.4623 - val_accuracy: 0.8200 - val_auc: 0.8634 - val_prc: 0.7575\n",
            "Epoch 281/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3948 - accuracy: 0.8510 - auc: 0.9325 - prc: 0.8418\n",
            "Epoch: 281\n",
            "Train Loss: 0.39486464858055115\n",
            "Val Loss: 0.45669183135032654\n",
            "Train Accuracy: 0.8505555391311646\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3949 - accuracy: 0.8506 - auc: 0.9321 - prc: 0.8408 - val_loss: 0.4567 - val_accuracy: 0.8225 - val_auc: 0.8640 - val_prc: 0.7567\n",
            "Epoch 282/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3951 - accuracy: 0.8603 - auc: 0.9313 - prc: 0.8444\n",
            "Epoch: 282\n",
            "Train Loss: 0.3945845663547516\n",
            "Val Loss: 0.46911993622779846\n",
            "Train Accuracy: 0.8608333468437195\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3946 - accuracy: 0.8608 - auc: 0.9315 - prc: 0.8443 - val_loss: 0.4691 - val_accuracy: 0.8125 - val_auc: 0.8638 - val_prc: 0.7593\n",
            "Epoch 283/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3942 - accuracy: 0.8622 - auc: 0.9325 - prc: 0.8436\n",
            "Epoch: 283\n",
            "Train Loss: 0.3941620886325836\n",
            "Val Loss: 0.44951197504997253\n",
            "Train Accuracy: 0.8622221946716309\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3942 - accuracy: 0.8622 - auc: 0.9325 - prc: 0.8436 - val_loss: 0.4495 - val_accuracy: 0.8375 - val_auc: 0.8648 - val_prc: 0.7571\n",
            "Epoch 284/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8609 - auc: 0.9353 - prc: 0.8532\n",
            "Epoch: 284\n",
            "Train Loss: 0.38700947165489197\n",
            "Val Loss: 0.4652264416217804\n",
            "Train Accuracy: 0.8616666793823242\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3870 - accuracy: 0.8617 - auc: 0.9359 - prc: 0.8541 - val_loss: 0.4652 - val_accuracy: 0.8200 - val_auc: 0.8638 - val_prc: 0.7574\n",
            "Epoch 285/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8569 - auc: 0.9295 - prc: 0.8376\n",
            "Epoch: 285\n",
            "Train Loss: 0.39809417724609375\n",
            "Val Loss: 0.45309197902679443\n",
            "Train Accuracy: 0.8569444417953491\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3981 - accuracy: 0.8569 - auc: 0.9297 - prc: 0.8380 - val_loss: 0.4531 - val_accuracy: 0.8250 - val_auc: 0.8644 - val_prc: 0.7570\n",
            "Epoch 286/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.8581 - auc: 0.9323 - prc: 0.8430\n",
            "Epoch: 286\n",
            "Train Loss: 0.39394691586494446\n",
            "Val Loss: 0.4517815411090851\n",
            "Train Accuracy: 0.8580555319786072\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3939 - accuracy: 0.8581 - auc: 0.9323 - prc: 0.8430 - val_loss: 0.4518 - val_accuracy: 0.8250 - val_auc: 0.8643 - val_prc: 0.7564\n",
            "Epoch 287/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3906 - accuracy: 0.8640 - auc: 0.9334 - prc: 0.8469\n",
            "Epoch: 287\n",
            "Train Loss: 0.3906897306442261\n",
            "Val Loss: 0.4551768898963928\n",
            "Train Accuracy: 0.8638888597488403\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3907 - accuracy: 0.8639 - auc: 0.9334 - prc: 0.8470 - val_loss: 0.4552 - val_accuracy: 0.8225 - val_auc: 0.8638 - val_prc: 0.7562\n",
            "Epoch 288/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8619 - auc: 0.9355 - prc: 0.8531\n",
            "Epoch: 288\n",
            "Train Loss: 0.38760361075401306\n",
            "Val Loss: 0.4417881667613983\n",
            "Train Accuracy: 0.8616666793823242\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3876 - accuracy: 0.8617 - auc: 0.9351 - prc: 0.8525 - val_loss: 0.4418 - val_accuracy: 0.8425 - val_auc: 0.8646 - val_prc: 0.7564\n",
            "Epoch 289/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3886 - accuracy: 0.8616 - auc: 0.9340 - prc: 0.8474\n",
            "Epoch: 289\n",
            "Train Loss: 0.38832971453666687\n",
            "Val Loss: 0.4433601498603821\n",
            "Train Accuracy: 0.8616666793823242\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3883 - accuracy: 0.8617 - auc: 0.9341 - prc: 0.8471 - val_loss: 0.4434 - val_accuracy: 0.8400 - val_auc: 0.8651 - val_prc: 0.7570\n",
            "Epoch 290/300\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3931 - accuracy: 0.8616 - auc: 0.9316 - prc: 0.8443\n",
            "Epoch: 290\n",
            "Train Loss: 0.393346905708313\n",
            "Val Loss: 0.44432148337364197\n",
            "Train Accuracy: 0.8619444370269775\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3933 - accuracy: 0.8619 - auc: 0.9315 - prc: 0.8427 - val_loss: 0.4443 - val_accuracy: 0.8400 - val_auc: 0.8645 - val_prc: 0.7575\n",
            "Epoch 291/300\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3888 - accuracy: 0.8609 - auc: 0.9341 - prc: 0.8526\n",
            "Epoch: 291\n",
            "Train Loss: 0.38880980014801025\n",
            "Val Loss: 0.4541310966014862\n",
            "Train Accuracy: 0.8613888621330261\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3888 - accuracy: 0.8614 - auc: 0.9338 - prc: 0.8510 - val_loss: 0.4541 - val_accuracy: 0.8200 - val_auc: 0.8641 - val_prc: 0.7576\n",
            "Epoch 292/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8660 - auc: 0.9367 - prc: 0.8466\n",
            "Epoch: 292\n",
            "Train Loss: 0.3846493065357208\n",
            "Val Loss: 0.45251569151878357\n",
            "Train Accuracy: 0.8652777671813965\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3846 - accuracy: 0.8653 - auc: 0.9363 - prc: 0.8446 - val_loss: 0.4525 - val_accuracy: 0.8200 - val_auc: 0.8639 - val_prc: 0.7575\n",
            "Epoch 293/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.8658 - auc: 0.9364 - prc: 0.8511\n",
            "Epoch: 293\n",
            "Train Loss: 0.38277319073677063\n",
            "Val Loss: 0.45565375685691833\n",
            "Train Accuracy: 0.8658333420753479\n",
            "Val Accuracy: 0.8199999928474426\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3828 - accuracy: 0.8658 - auc: 0.9364 - prc: 0.8512 - val_loss: 0.4557 - val_accuracy: 0.8200 - val_auc: 0.8635 - val_prc: 0.7584\n",
            "Epoch 294/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3870 - accuracy: 0.8650 - auc: 0.9348 - prc: 0.8428\n",
            "Epoch: 294\n",
            "Train Loss: 0.38693612813949585\n",
            "Val Loss: 0.46124565601348877\n",
            "Train Accuracy: 0.8647222518920898\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3869 - accuracy: 0.8647 - auc: 0.9348 - prc: 0.8431 - val_loss: 0.4612 - val_accuracy: 0.8150 - val_auc: 0.8635 - val_prc: 0.7579\n",
            "Epoch 295/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8567 - auc: 0.9334 - prc: 0.8488\n",
            "Epoch: 295\n",
            "Train Loss: 0.38792508840560913\n",
            "Val Loss: 0.4544351100921631\n",
            "Train Accuracy: 0.8566666841506958\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3879 - accuracy: 0.8567 - auc: 0.9336 - prc: 0.8494 - val_loss: 0.4544 - val_accuracy: 0.8225 - val_auc: 0.8639 - val_prc: 0.7581\n",
            "Epoch 296/300\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8635 - auc: 0.9375 - prc: 0.8551\n",
            "Epoch: 296\n",
            "Train Loss: 0.381225049495697\n",
            "Val Loss: 0.44614529609680176\n",
            "Train Accuracy: 0.863611102104187\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3812 - accuracy: 0.8636 - auc: 0.9373 - prc: 0.8549 - val_loss: 0.4461 - val_accuracy: 0.8375 - val_auc: 0.8649 - val_prc: 0.7588\n",
            "Epoch 297/300\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8658 - auc: 0.9366 - prc: 0.8519\n",
            "Epoch: 297\n",
            "Train Loss: 0.3818853497505188\n",
            "Val Loss: 0.4513530433177948\n",
            "Train Accuracy: 0.8658333420753479\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3819 - accuracy: 0.8658 - auc: 0.9366 - prc: 0.8519 - val_loss: 0.4514 - val_accuracy: 0.8225 - val_auc: 0.8648 - val_prc: 0.7585\n",
            "Epoch 298/300\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8697 - auc: 0.9351 - prc: 0.8460\n",
            "Epoch: 298\n",
            "Train Loss: 0.3833019733428955\n",
            "Val Loss: 0.46575233340263367\n",
            "Train Accuracy: 0.8711110949516296\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3833 - accuracy: 0.8711 - auc: 0.9363 - prc: 0.8492 - val_loss: 0.4658 - val_accuracy: 0.8125 - val_auc: 0.8637 - val_prc: 0.7589\n",
            "Epoch 299/300\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3855 - accuracy: 0.8614 - auc: 0.9350 - prc: 0.8489\n",
            "Epoch: 299\n",
            "Train Loss: 0.385178804397583\n",
            "Val Loss: 0.45688292384147644\n",
            "Train Accuracy: 0.8616666793823242\n",
            "Val Accuracy: 0.8224999904632568\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3852 - accuracy: 0.8617 - auc: 0.9351 - prc: 0.8491 - val_loss: 0.4569 - val_accuracy: 0.8225 - val_auc: 0.8639 - val_prc: 0.7606\n",
            "Epoch 300/300\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8644 - auc: 0.9391 - prc: 0.8603\n",
            "Epoch: 300\n",
            "Train Loss: 0.37574502825737\n",
            "Val Loss: 0.44859176874160767\n",
            "Train Accuracy: 0.8647222518920898\n",
            "Val Accuracy: 0.8299999833106995\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3757 - accuracy: 0.8647 - auc: 0.9394 - prc: 0.8611 - val_loss: 0.4486 - val_accuracy: 0.8300 - val_auc: 0.8650 - val_prc: 0.7583\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.84      0.86       710\n",
            "           1       0.65      0.75      0.70       290\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.77      0.79      0.78      1000\n",
            "weighted avg       0.82      0.81      0.81      1000\n",
            "\n",
            "AUC Score: 0.8758766391452161\n"
          ]
        }
      ],
      "source": [
        "class PrintMetricsCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\"\\nEpoch:\", epoch+1)\n",
        "        print(\"Train Loss:\", logs['loss'])\n",
        "        print(\"Val Loss:\", logs['val_loss'])\n",
        "        print(\"Train Accuracy:\", logs['accuracy'])\n",
        "        print(\"Val Accuracy:\", logs['val_accuracy'])\n",
        "\n",
        "print_metrics_callback = PrintMetricsCallback()\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc',\n",
        "    verbose=1,\n",
        "    patience=20,\n",
        "    mode='max',\n",
        "    restore_best_weights=True\n",
        "    )\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=300, batch_size=8,\n",
        "    validation_split=0.1,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[print_metrics_callback])\n",
        "\n",
        "# Evaluate the model on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print performance metrics\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58V44MMeJrBg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='Train Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "OzAfmRO-eCOy",
        "outputId": "6c974859-0705-4488-ff41-dc2644dd687f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F84DHTvO1QHT"
      },
      "source": [
        "The model is still learning so, we continue the learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBw99ulD1Pbp",
        "outputId": "3802ceb5-61e6-4bb5-d42a-44c5350f9de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.8872 - auc: 0.9544 - prc: 0.8913\n",
            "Epoch: 1\n",
            "Train Loss: 0.32984334230422974\n",
            "Val Loss: 0.41229796409606934\n",
            "Train Accuracy: 0.8872222304344177\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 6s 14ms/step - loss: 0.3298 - accuracy: 0.8872 - auc: 0.9544 - prc: 0.8913 - val_loss: 0.4123 - val_accuracy: 0.8525 - val_auc: 0.8713 - val_prc: 0.7655\n",
            "Epoch 2/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3278 - accuracy: 0.8899 - auc: 0.9554 - prc: 0.8900\n",
            "Epoch: 2\n",
            "Train Loss: 0.3294997215270996\n",
            "Val Loss: 0.420179545879364\n",
            "Train Accuracy: 0.8897222280502319\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3295 - accuracy: 0.8897 - auc: 0.9548 - prc: 0.8897 - val_loss: 0.4202 - val_accuracy: 0.8525 - val_auc: 0.8708 - val_prc: 0.7670\n",
            "Epoch 3/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.8892 - auc: 0.9550 - prc: 0.8944\n",
            "Epoch: 3\n",
            "Train Loss: 0.32840731739997864\n",
            "Val Loss: 0.4419585168361664\n",
            "Train Accuracy: 0.8897222280502319\n",
            "Val Accuracy: 0.8349999785423279\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.3284 - accuracy: 0.8897 - auc: 0.9553 - prc: 0.8952 - val_loss: 0.4420 - val_accuracy: 0.8350 - val_auc: 0.8681 - val_prc: 0.7660\n",
            "Epoch 4/100\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.8872 - auc: 0.9539 - prc: 0.8867\n",
            "Epoch: 4\n",
            "Train Loss: 0.32980430126190186\n",
            "Val Loss: 0.4234481751918793\n",
            "Train Accuracy: 0.887499988079071\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.3298 - accuracy: 0.8875 - auc: 0.9540 - prc: 0.8870 - val_loss: 0.4234 - val_accuracy: 0.8450 - val_auc: 0.8709 - val_prc: 0.7686\n",
            "Epoch 5/100\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3252 - accuracy: 0.8953 - auc: 0.9562 - prc: 0.8923\n",
            "Epoch: 5\n",
            "Train Loss: 0.32522058486938477\n",
            "Val Loss: 0.4263252317905426\n",
            "Train Accuracy: 0.8952777981758118\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 5s 12ms/step - loss: 0.3252 - accuracy: 0.8953 - auc: 0.9562 - prc: 0.8923 - val_loss: 0.4263 - val_accuracy: 0.8450 - val_auc: 0.8699 - val_prc: 0.7666\n",
            "Epoch 6/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3306 - accuracy: 0.8913 - auc: 0.9538 - prc: 0.8909\n",
            "Epoch: 6\n",
            "Train Loss: 0.3307477831840515\n",
            "Val Loss: 0.44298824667930603\n",
            "Train Accuracy: 0.8913888931274414\n",
            "Val Accuracy: 0.8324999809265137\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.3307 - accuracy: 0.8914 - auc: 0.9537 - prc: 0.8906 - val_loss: 0.4430 - val_accuracy: 0.8325 - val_auc: 0.8691 - val_prc: 0.7672\n",
            "Epoch 7/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8929 - auc: 0.9571 - prc: 0.8908\n",
            "Epoch: 7\n",
            "Train Loss: 0.32123255729675293\n",
            "Val Loss: 0.4264312982559204\n",
            "Train Accuracy: 0.8930555582046509\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3212 - accuracy: 0.8931 - auc: 0.9573 - prc: 0.8910 - val_loss: 0.4264 - val_accuracy: 0.8425 - val_auc: 0.8710 - val_prc: 0.7692\n",
            "Epoch 8/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.8929 - auc: 0.9547 - prc: 0.8848\n",
            "Epoch: 8\n",
            "Train Loss: 0.3280864357948303\n",
            "Val Loss: 0.4223170578479767\n",
            "Train Accuracy: 0.8927778005599976\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3281 - accuracy: 0.8928 - auc: 0.9546 - prc: 0.8848 - val_loss: 0.4223 - val_accuracy: 0.8450 - val_auc: 0.8713 - val_prc: 0.7682\n",
            "Epoch 9/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8907 - auc: 0.9536 - prc: 0.8902\n",
            "Epoch: 9\n",
            "Train Loss: 0.3297928273677826\n",
            "Val Loss: 0.43642979860305786\n",
            "Train Accuracy: 0.8911111354827881\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3298 - accuracy: 0.8911 - auc: 0.9539 - prc: 0.8908 - val_loss: 0.4364 - val_accuracy: 0.8375 - val_auc: 0.8693 - val_prc: 0.7697\n",
            "Epoch 10/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8952 - auc: 0.9571 - prc: 0.8971\n",
            "Epoch: 10\n",
            "Train Loss: 0.31956538558006287\n",
            "Val Loss: 0.43139412999153137\n",
            "Train Accuracy: 0.8952777981758118\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3196 - accuracy: 0.8953 - auc: 0.9574 - prc: 0.8974 - val_loss: 0.4314 - val_accuracy: 0.8425 - val_auc: 0.8700 - val_prc: 0.7702\n",
            "Epoch 11/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8881 - auc: 0.9550 - prc: 0.8935\n",
            "Epoch: 11\n",
            "Train Loss: 0.3268159329891205\n",
            "Val Loss: 0.41892293095588684\n",
            "Train Accuracy: 0.8880555629730225\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3268 - accuracy: 0.8881 - auc: 0.9552 - prc: 0.8940 - val_loss: 0.4189 - val_accuracy: 0.8500 - val_auc: 0.8714 - val_prc: 0.7699\n",
            "Epoch 12/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8916 - auc: 0.9541 - prc: 0.8867\n",
            "Epoch: 12\n",
            "Train Loss: 0.32808876037597656\n",
            "Val Loss: 0.428126722574234\n",
            "Train Accuracy: 0.8919444680213928\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3281 - accuracy: 0.8919 - auc: 0.9545 - prc: 0.8876 - val_loss: 0.4281 - val_accuracy: 0.8425 - val_auc: 0.8701 - val_prc: 0.7682\n",
            "Epoch 13/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8961 - auc: 0.9573 - prc: 0.8980\n",
            "Epoch: 13\n",
            "Train Loss: 0.32199162244796753\n",
            "Val Loss: 0.41701140999794006\n",
            "Train Accuracy: 0.8963888883590698\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3220 - accuracy: 0.8964 - auc: 0.9574 - prc: 0.8981 - val_loss: 0.4170 - val_accuracy: 0.8500 - val_auc: 0.8715 - val_prc: 0.7694\n",
            "Epoch 14/100\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8928 - auc: 0.9574 - prc: 0.8959\n",
            "Epoch: 14\n",
            "Train Loss: 0.32087746262550354\n",
            "Val Loss: 0.41124966740608215\n",
            "Train Accuracy: 0.8927778005599976\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3209 - accuracy: 0.8928 - auc: 0.9574 - prc: 0.8954 - val_loss: 0.4112 - val_accuracy: 0.8525 - val_auc: 0.8715 - val_prc: 0.7677\n",
            "Epoch 15/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.8923 - auc: 0.9558 - prc: 0.8945\n",
            "Epoch: 15\n",
            "Train Loss: 0.3263407349586487\n",
            "Val Loss: 0.4213995337486267\n",
            "Train Accuracy: 0.8922222256660461\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3263 - accuracy: 0.8922 - auc: 0.9555 - prc: 0.8933 - val_loss: 0.4214 - val_accuracy: 0.8450 - val_auc: 0.8708 - val_prc: 0.7675\n",
            "Epoch 16/100\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8906 - auc: 0.9559 - prc: 0.8962\n",
            "Epoch: 16\n",
            "Train Loss: 0.32394352555274963\n",
            "Val Loss: 0.4200817942619324\n",
            "Train Accuracy: 0.89083331823349\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3239 - accuracy: 0.8908 - auc: 0.9560 - prc: 0.8963 - val_loss: 0.4201 - val_accuracy: 0.8475 - val_auc: 0.8708 - val_prc: 0.7685\n",
            "Epoch 17/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8913 - auc: 0.9562 - prc: 0.8956\n",
            "Epoch: 17\n",
            "Train Loss: 0.3214360773563385\n",
            "Val Loss: 0.4212890863418579\n",
            "Train Accuracy: 0.8922222256660461\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.3214 - accuracy: 0.8922 - auc: 0.9567 - prc: 0.8967 - val_loss: 0.4213 - val_accuracy: 0.8475 - val_auc: 0.8710 - val_prc: 0.7679\n",
            "Epoch 18/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.9023 - auc: 0.9585 - prc: 0.8996\n",
            "Epoch: 18\n",
            "Train Loss: 0.31679147481918335\n",
            "Val Loss: 0.4285649061203003\n",
            "Train Accuracy: 0.9027777910232544\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3168 - accuracy: 0.9028 - auc: 0.9588 - prc: 0.9007 - val_loss: 0.4286 - val_accuracy: 0.8425 - val_auc: 0.8710 - val_prc: 0.7699\n",
            "Epoch 19/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8937 - auc: 0.9555 - prc: 0.8961\n",
            "Epoch: 19\n",
            "Train Loss: 0.32416513562202454\n",
            "Val Loss: 0.4248308539390564\n",
            "Train Accuracy: 0.8933333158493042\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.3242 - accuracy: 0.8933 - auc: 0.9554 - prc: 0.8957 - val_loss: 0.4248 - val_accuracy: 0.8450 - val_auc: 0.8702 - val_prc: 0.7687\n",
            "Epoch 20/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.8932 - auc: 0.9557 - prc: 0.8902\n",
            "Epoch: 20\n",
            "Train Loss: 0.32317259907722473\n",
            "Val Loss: 0.4204384982585907\n",
            "Train Accuracy: 0.8927778005599976\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3232 - accuracy: 0.8928 - auc: 0.9560 - prc: 0.8919 - val_loss: 0.4204 - val_accuracy: 0.8475 - val_auc: 0.8713 - val_prc: 0.7683\n",
            "Epoch 21/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8924 - auc: 0.9555 - prc: 0.8914\n",
            "Epoch: 21\n",
            "Train Loss: 0.3256654441356659\n",
            "Val Loss: 0.42789822816848755\n",
            "Train Accuracy: 0.8911111354827881\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3257 - accuracy: 0.8911 - auc: 0.9550 - prc: 0.8906 - val_loss: 0.4279 - val_accuracy: 0.8425 - val_auc: 0.8714 - val_prc: 0.7697\n",
            "Epoch 22/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8981 - auc: 0.9580 - prc: 0.8995\n",
            "Epoch: 22\n",
            "Train Loss: 0.31861695647239685\n",
            "Val Loss: 0.4303478002548218\n",
            "Train Accuracy: 0.8972222208976746\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3186 - accuracy: 0.8972 - auc: 0.9580 - prc: 0.8997 - val_loss: 0.4303 - val_accuracy: 0.8425 - val_auc: 0.8706 - val_prc: 0.7712\n",
            "Epoch 23/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3229 - accuracy: 0.8930 - auc: 0.9560 - prc: 0.8973\n",
            "Epoch: 23\n",
            "Train Loss: 0.3225148320198059\n",
            "Val Loss: 0.4237689971923828\n",
            "Train Accuracy: 0.8936111330986023\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3225 - accuracy: 0.8936 - auc: 0.9561 - prc: 0.8974 - val_loss: 0.4238 - val_accuracy: 0.8475 - val_auc: 0.8712 - val_prc: 0.7697\n",
            "Epoch 24/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8979 - auc: 0.9563 - prc: 0.8975\n",
            "Epoch: 24\n",
            "Train Loss: 0.3219287693500519\n",
            "Val Loss: 0.42298203706741333\n",
            "Train Accuracy: 0.8974999785423279\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.3219 - accuracy: 0.8975 - auc: 0.9563 - prc: 0.8973 - val_loss: 0.4230 - val_accuracy: 0.8500 - val_auc: 0.8715 - val_prc: 0.7703\n",
            "Epoch 25/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.8955 - auc: 0.9587 - prc: 0.8974\n",
            "Epoch: 25\n",
            "Train Loss: 0.3173549175262451\n",
            "Val Loss: 0.41581857204437256\n",
            "Train Accuracy: 0.8952777981758118\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3174 - accuracy: 0.8953 - auc: 0.9588 - prc: 0.8975 - val_loss: 0.4158 - val_accuracy: 0.8525 - val_auc: 0.8722 - val_prc: 0.7692\n",
            "Epoch 26/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8907 - auc: 0.9555 - prc: 0.8910\n",
            "Epoch: 26\n",
            "Train Loss: 0.32417184114456177\n",
            "Val Loss: 0.42511501908302307\n",
            "Train Accuracy: 0.89083331823349\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3242 - accuracy: 0.8908 - auc: 0.9556 - prc: 0.8910 - val_loss: 0.4251 - val_accuracy: 0.8475 - val_auc: 0.8708 - val_prc: 0.7700\n",
            "Epoch 27/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.9006 - auc: 0.9610 - prc: 0.9025\n",
            "Epoch: 27\n",
            "Train Loss: 0.31132107973098755\n",
            "Val Loss: 0.4165829122066498\n",
            "Train Accuracy: 0.9002777934074402\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3113 - accuracy: 0.9003 - auc: 0.9605 - prc: 0.9014 - val_loss: 0.4166 - val_accuracy: 0.8550 - val_auc: 0.8718 - val_prc: 0.7700\n",
            "Epoch 28/100\n",
            "438/450 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.8973 - auc: 0.9594 - prc: 0.8979\n",
            "Epoch: 28\n",
            "Train Loss: 0.3185017704963684\n",
            "Val Loss: 0.4345547556877136\n",
            "Train Accuracy: 0.8941666483879089\n",
            "Val Accuracy: 0.8324999809265137\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.3185 - accuracy: 0.8942 - auc: 0.9579 - prc: 0.8937 - val_loss: 0.4346 - val_accuracy: 0.8325 - val_auc: 0.8708 - val_prc: 0.7702\n",
            "Epoch 29/100\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.8914 - auc: 0.9586 - prc: 0.9007\n",
            "Epoch: 29\n",
            "Train Loss: 0.316328763961792\n",
            "Val Loss: 0.42593979835510254\n",
            "Train Accuracy: 0.89083331823349\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3163 - accuracy: 0.8908 - auc: 0.9583 - prc: 0.9002 - val_loss: 0.4259 - val_accuracy: 0.8450 - val_auc: 0.8718 - val_prc: 0.7707\n",
            "Epoch 30/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8912 - auc: 0.9561 - prc: 0.9012\n",
            "Epoch: 30\n",
            "Train Loss: 0.3216496706008911\n",
            "Val Loss: 0.4229664206504822\n",
            "Train Accuracy: 0.8911111354827881\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3216 - accuracy: 0.8911 - auc: 0.9560 - prc: 0.9004 - val_loss: 0.4230 - val_accuracy: 0.8500 - val_auc: 0.8714 - val_prc: 0.7699\n",
            "Epoch 31/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8954 - auc: 0.9590 - prc: 0.8979\n",
            "Epoch: 31\n",
            "Train Loss: 0.31527093052864075\n",
            "Val Loss: 0.4193915128707886\n",
            "Train Accuracy: 0.8958333134651184\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3153 - accuracy: 0.8958 - auc: 0.9593 - prc: 0.8986 - val_loss: 0.4194 - val_accuracy: 0.8525 - val_auc: 0.8715 - val_prc: 0.7696\n",
            "Epoch 32/100\n",
            "439/450 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8978 - auc: 0.9596 - prc: 0.9026\n",
            "Epoch: 32\n",
            "Train Loss: 0.315871924161911\n",
            "Val Loss: 0.4265819787979126\n",
            "Train Accuracy: 0.8983333110809326\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3159 - accuracy: 0.8983 - auc: 0.9594 - prc: 0.9028 - val_loss: 0.4266 - val_accuracy: 0.8450 - val_auc: 0.8716 - val_prc: 0.7717\n",
            "Epoch 33/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.8937 - auc: 0.9579 - prc: 0.9058\n",
            "Epoch: 33\n",
            "Train Loss: 0.31740450859069824\n",
            "Val Loss: 0.42920470237731934\n",
            "Train Accuracy: 0.8933333158493042\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3174 - accuracy: 0.8933 - auc: 0.9580 - prc: 0.9061 - val_loss: 0.4292 - val_accuracy: 0.8425 - val_auc: 0.8705 - val_prc: 0.7700\n",
            "Epoch 34/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.9022 - auc: 0.9599 - prc: 0.9023\n",
            "Epoch: 34\n",
            "Train Loss: 0.31331488490104675\n",
            "Val Loss: 0.420867919921875\n",
            "Train Accuracy: 0.9019444584846497\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3133 - accuracy: 0.9019 - auc: 0.9593 - prc: 0.8999 - val_loss: 0.4209 - val_accuracy: 0.8525 - val_auc: 0.8709 - val_prc: 0.7704\n",
            "Epoch 35/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8976 - auc: 0.9592 - prc: 0.9003\n",
            "Epoch: 35\n",
            "Train Loss: 0.31641507148742676\n",
            "Val Loss: 0.4238849878311157\n",
            "Train Accuracy: 0.8963888883590698\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3164 - accuracy: 0.8964 - auc: 0.9586 - prc: 0.8985 - val_loss: 0.4239 - val_accuracy: 0.8500 - val_auc: 0.8716 - val_prc: 0.7715\n",
            "Epoch 36/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3241 - accuracy: 0.8944 - auc: 0.9553 - prc: 0.8902\n",
            "Epoch: 36\n",
            "Train Loss: 0.3228784203529358\n",
            "Val Loss: 0.42081475257873535\n",
            "Train Accuracy: 0.894444465637207\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3229 - accuracy: 0.8944 - auc: 0.9557 - prc: 0.8912 - val_loss: 0.4208 - val_accuracy: 0.8525 - val_auc: 0.8720 - val_prc: 0.7712\n",
            "Epoch 37/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.9016 - auc: 0.9606 - prc: 0.9018\n",
            "Epoch: 37\n",
            "Train Loss: 0.3097386360168457\n",
            "Val Loss: 0.4252244532108307\n",
            "Train Accuracy: 0.9011111259460449\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3097 - accuracy: 0.9011 - auc: 0.9606 - prc: 0.9020 - val_loss: 0.4252 - val_accuracy: 0.8500 - val_auc: 0.8712 - val_prc: 0.7713\n",
            "Epoch 38/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8965 - auc: 0.9580 - prc: 0.9010\n",
            "Epoch: 38\n",
            "Train Loss: 0.31619399785995483\n",
            "Val Loss: 0.41450852155685425\n",
            "Train Accuracy: 0.8963888883590698\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3162 - accuracy: 0.8964 - auc: 0.9581 - prc: 0.9013 - val_loss: 0.4145 - val_accuracy: 0.8525 - val_auc: 0.8732 - val_prc: 0.7715\n",
            "Epoch 39/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8962 - auc: 0.9596 - prc: 0.8991\n",
            "Epoch: 39\n",
            "Train Loss: 0.31344524025917053\n",
            "Val Loss: 0.4320412874221802\n",
            "Train Accuracy: 0.8966666460037231\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3134 - accuracy: 0.8967 - auc: 0.9594 - prc: 0.8982 - val_loss: 0.4320 - val_accuracy: 0.8375 - val_auc: 0.8713 - val_prc: 0.7700\n",
            "Epoch 40/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8978 - auc: 0.9584 - prc: 0.9017\n",
            "Epoch: 40\n",
            "Train Loss: 0.3171173334121704\n",
            "Val Loss: 0.42599567770957947\n",
            "Train Accuracy: 0.8972222208976746\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3171 - accuracy: 0.8972 - auc: 0.9579 - prc: 0.9019 - val_loss: 0.4260 - val_accuracy: 0.8500 - val_auc: 0.8716 - val_prc: 0.7715\n",
            "Epoch 41/100\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8959 - auc: 0.9598 - prc: 0.8950\n",
            "Epoch: 41\n",
            "Train Loss: 0.31476882100105286\n",
            "Val Loss: 0.4177974760532379\n",
            "Train Accuracy: 0.8955555558204651\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3148 - accuracy: 0.8956 - auc: 0.9589 - prc: 0.8942 - val_loss: 0.4178 - val_accuracy: 0.8525 - val_auc: 0.8730 - val_prc: 0.7707\n",
            "Epoch 42/100\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.9028 - auc: 0.9616 - prc: 0.9085\n",
            "Epoch: 42\n",
            "Train Loss: 0.30724307894706726\n",
            "Val Loss: 0.43347838521003723\n",
            "Train Accuracy: 0.9027777910232544\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3072 - accuracy: 0.9028 - auc: 0.9616 - prc: 0.9085 - val_loss: 0.4335 - val_accuracy: 0.8400 - val_auc: 0.8712 - val_prc: 0.7704\n",
            "Epoch 43/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8927 - auc: 0.9596 - prc: 0.8980\n",
            "Epoch: 43\n",
            "Train Loss: 0.3128925859928131\n",
            "Val Loss: 0.4255007803440094\n",
            "Train Accuracy: 0.8916666507720947\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3129 - accuracy: 0.8917 - auc: 0.9596 - prc: 0.8976 - val_loss: 0.4255 - val_accuracy: 0.8475 - val_auc: 0.8722 - val_prc: 0.7703\n",
            "Epoch 44/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8972 - auc: 0.9585 - prc: 0.8996\n",
            "Epoch: 44\n",
            "Train Loss: 0.3151971995830536\n",
            "Val Loss: 0.41717493534088135\n",
            "Train Accuracy: 0.8972222208976746\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3152 - accuracy: 0.8972 - auc: 0.9586 - prc: 0.8996 - val_loss: 0.4172 - val_accuracy: 0.8550 - val_auc: 0.8721 - val_prc: 0.7711\n",
            "Epoch 45/100\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.8976 - auc: 0.9592 - prc: 0.8956\n",
            "Epoch: 45\n",
            "Train Loss: 0.31567442417144775\n",
            "Val Loss: 0.425289511680603\n",
            "Train Accuracy: 0.8972222208976746\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3157 - accuracy: 0.8972 - auc: 0.9586 - prc: 0.8955 - val_loss: 0.4253 - val_accuracy: 0.8425 - val_auc: 0.8725 - val_prc: 0.7705\n",
            "Epoch 46/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3147 - accuracy: 0.8971 - auc: 0.9592 - prc: 0.9040\n",
            "Epoch: 46\n",
            "Train Loss: 0.3147023320198059\n",
            "Val Loss: 0.41034212708473206\n",
            "Train Accuracy: 0.8969444632530212\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3147 - accuracy: 0.8969 - auc: 0.9593 - prc: 0.9043 - val_loss: 0.4103 - val_accuracy: 0.8550 - val_auc: 0.8727 - val_prc: 0.7712\n",
            "Epoch 47/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.9007 - auc: 0.9603 - prc: 0.9019\n",
            "Epoch: 47\n",
            "Train Loss: 0.31178951263427734\n",
            "Val Loss: 0.43020060658454895\n",
            "Train Accuracy: 0.9008333086967468\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3118 - accuracy: 0.9008 - auc: 0.9603 - prc: 0.9019 - val_loss: 0.4302 - val_accuracy: 0.8425 - val_auc: 0.8706 - val_prc: 0.7702\n",
            "Epoch 48/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8993 - auc: 0.9591 - prc: 0.8969\n",
            "Epoch: 48\n",
            "Train Loss: 0.3123264014720917\n",
            "Val Loss: 0.4162083864212036\n",
            "Train Accuracy: 0.8994444608688354\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3123 - accuracy: 0.8994 - auc: 0.9592 - prc: 0.8974 - val_loss: 0.4162 - val_accuracy: 0.8525 - val_auc: 0.8721 - val_prc: 0.7707\n",
            "Epoch 49/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3116 - accuracy: 0.9003 - auc: 0.9594 - prc: 0.9033\n",
            "Epoch: 49\n",
            "Train Loss: 0.3123493194580078\n",
            "Val Loss: 0.41859889030456543\n",
            "Train Accuracy: 0.8994444608688354\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3123 - accuracy: 0.8994 - auc: 0.9591 - prc: 0.9019 - val_loss: 0.4186 - val_accuracy: 0.8525 - val_auc: 0.8720 - val_prc: 0.7708\n",
            "Epoch 50/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.8968 - auc: 0.9597 - prc: 0.8975\n",
            "Epoch: 50\n",
            "Train Loss: 0.3146401345729828\n",
            "Val Loss: 0.4199567139148712\n",
            "Train Accuracy: 0.8969444632530212\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3146 - accuracy: 0.8969 - auc: 0.9592 - prc: 0.8973 - val_loss: 0.4200 - val_accuracy: 0.8525 - val_auc: 0.8718 - val_prc: 0.7717\n",
            "Epoch 51/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8968 - auc: 0.9600 - prc: 0.9011\n",
            "Epoch: 51\n",
            "Train Loss: 0.31267470121383667\n",
            "Val Loss: 0.41348162293434143\n",
            "Train Accuracy: 0.8963888883590698\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3127 - accuracy: 0.8964 - auc: 0.9599 - prc: 0.9011 - val_loss: 0.4135 - val_accuracy: 0.8500 - val_auc: 0.8724 - val_prc: 0.7716\n",
            "Epoch 52/100\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.8964 - auc: 0.9590 - prc: 0.8984\n",
            "Epoch: 52\n",
            "Train Loss: 0.31467241048812866\n",
            "Val Loss: 0.4377302825450897\n",
            "Train Accuracy: 0.8966666460037231\n",
            "Val Accuracy: 0.8324999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3147 - accuracy: 0.8967 - auc: 0.9590 - prc: 0.8984 - val_loss: 0.4377 - val_accuracy: 0.8325 - val_auc: 0.8708 - val_prc: 0.7721\n",
            "Epoch 53/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.3112 - accuracy: 0.8985 - auc: 0.9590 - prc: 0.9003\n",
            "Epoch: 53\n",
            "Train Loss: 0.3113012909889221\n",
            "Val Loss: 0.42322012782096863\n",
            "Train Accuracy: 0.8983333110809326\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3113 - accuracy: 0.8983 - auc: 0.9591 - prc: 0.9001 - val_loss: 0.4232 - val_accuracy: 0.8500 - val_auc: 0.8720 - val_prc: 0.7696\n",
            "Epoch 54/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.9026 - auc: 0.9599 - prc: 0.9063\n",
            "Epoch: 54\n",
            "Train Loss: 0.3108412027359009\n",
            "Val Loss: 0.4275171756744385\n",
            "Train Accuracy: 0.9027777910232544\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3108 - accuracy: 0.9028 - auc: 0.9598 - prc: 0.9061 - val_loss: 0.4275 - val_accuracy: 0.8425 - val_auc: 0.8716 - val_prc: 0.7702\n",
            "Epoch 55/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.9021 - auc: 0.9609 - prc: 0.9079\n",
            "Epoch: 55\n",
            "Train Loss: 0.30829161405563354\n",
            "Val Loss: 0.41688528656959534\n",
            "Train Accuracy: 0.9016666412353516\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3083 - accuracy: 0.9017 - auc: 0.9607 - prc: 0.9076 - val_loss: 0.4169 - val_accuracy: 0.8500 - val_auc: 0.8714 - val_prc: 0.7705\n",
            "Epoch 56/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.9008 - auc: 0.9581 - prc: 0.8981\n",
            "Epoch: 56\n",
            "Train Loss: 0.31482937932014465\n",
            "Val Loss: 0.4314707815647125\n",
            "Train Accuracy: 0.9008333086967468\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3148 - accuracy: 0.9008 - auc: 0.9582 - prc: 0.8979 - val_loss: 0.4315 - val_accuracy: 0.8400 - val_auc: 0.8706 - val_prc: 0.7698\n",
            "Epoch 57/100\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.9033 - auc: 0.9621 - prc: 0.9103\n",
            "Epoch: 57\n",
            "Train Loss: 0.3049280345439911\n",
            "Val Loss: 0.40793484449386597\n",
            "Train Accuracy: 0.903333306312561\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3049 - accuracy: 0.9033 - auc: 0.9621 - prc: 0.9103 - val_loss: 0.4079 - val_accuracy: 0.8550 - val_auc: 0.8730 - val_prc: 0.7705\n",
            "Epoch 58/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.9048 - auc: 0.9613 - prc: 0.9084\n",
            "Epoch: 58\n",
            "Train Loss: 0.30459514260292053\n",
            "Val Loss: 0.4138264060020447\n",
            "Train Accuracy: 0.9052777886390686\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3046 - accuracy: 0.9053 - auc: 0.9620 - prc: 0.9095 - val_loss: 0.4138 - val_accuracy: 0.8500 - val_auc: 0.8718 - val_prc: 0.7707\n",
            "Epoch 59/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.8977 - auc: 0.9587 - prc: 0.8995\n",
            "Epoch: 59\n",
            "Train Loss: 0.3133350610733032\n",
            "Val Loss: 0.4140914976596832\n",
            "Train Accuracy: 0.8983333110809326\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3133 - accuracy: 0.8983 - auc: 0.9590 - prc: 0.8999 - val_loss: 0.4141 - val_accuracy: 0.8500 - val_auc: 0.8723 - val_prc: 0.7701\n",
            "Epoch 60/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.9008 - auc: 0.9609 - prc: 0.9059\n",
            "Epoch: 60\n",
            "Train Loss: 0.3080129027366638\n",
            "Val Loss: 0.41846340894699097\n",
            "Train Accuracy: 0.9011111259460449\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3080 - accuracy: 0.9011 - auc: 0.9613 - prc: 0.9067 - val_loss: 0.4185 - val_accuracy: 0.8475 - val_auc: 0.8726 - val_prc: 0.7708\n",
            "Epoch 61/100\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.9014 - auc: 0.9615 - prc: 0.9063\n",
            "Epoch: 61\n",
            "Train Loss: 0.3055662512779236\n",
            "Val Loss: 0.4228779971599579\n",
            "Train Accuracy: 0.9013888835906982\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3056 - accuracy: 0.9014 - auc: 0.9616 - prc: 0.9065 - val_loss: 0.4229 - val_accuracy: 0.8475 - val_auc: 0.8720 - val_prc: 0.7697\n",
            "Epoch 62/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8983 - auc: 0.9602 - prc: 0.9035\n",
            "Epoch: 62\n",
            "Train Loss: 0.31107017397880554\n",
            "Val Loss: 0.42293503880500793\n",
            "Train Accuracy: 0.8983333110809326\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3111 - accuracy: 0.8983 - auc: 0.9600 - prc: 0.9037 - val_loss: 0.4229 - val_accuracy: 0.8500 - val_auc: 0.8720 - val_prc: 0.7702\n",
            "Epoch 63/100\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.8994 - auc: 0.9610 - prc: 0.9021\n",
            "Epoch: 63\n",
            "Train Loss: 0.30640605092048645\n",
            "Val Loss: 0.4201497733592987\n",
            "Train Accuracy: 0.8994444608688354\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3064 - accuracy: 0.8994 - auc: 0.9610 - prc: 0.9021 - val_loss: 0.4201 - val_accuracy: 0.8500 - val_auc: 0.8728 - val_prc: 0.7707\n",
            "Epoch 64/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8971 - auc: 0.9600 - prc: 0.9060\n",
            "Epoch: 64\n",
            "Train Loss: 0.3080690801143646\n",
            "Val Loss: 0.41496729850769043\n",
            "Train Accuracy: 0.8974999785423279\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3081 - accuracy: 0.8975 - auc: 0.9604 - prc: 0.9071 - val_loss: 0.4150 - val_accuracy: 0.8550 - val_auc: 0.8728 - val_prc: 0.7710\n",
            "Epoch 65/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.9014 - auc: 0.9602 - prc: 0.9026\n",
            "Epoch: 65\n",
            "Train Loss: 0.3079620599746704\n",
            "Val Loss: 0.41228997707366943\n",
            "Train Accuracy: 0.9011111259460449\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3080 - accuracy: 0.9011 - auc: 0.9603 - prc: 0.9028 - val_loss: 0.4123 - val_accuracy: 0.8550 - val_auc: 0.8729 - val_prc: 0.7718\n",
            "Epoch 66/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.9017 - auc: 0.9612 - prc: 0.9053\n",
            "Epoch: 66\n",
            "Train Loss: 0.3035132586956024\n",
            "Val Loss: 0.40305328369140625\n",
            "Train Accuracy: 0.9027777910232544\n",
            "Val Accuracy: 0.8575000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3035 - accuracy: 0.9028 - auc: 0.9620 - prc: 0.9075 - val_loss: 0.4031 - val_accuracy: 0.8575 - val_auc: 0.8738 - val_prc: 0.7700\n",
            "Epoch 67/100\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.9031 - auc: 0.9611 - prc: 0.9048\n",
            "Epoch: 67\n",
            "Train Loss: 0.3062306344509125\n",
            "Val Loss: 0.4239802062511444\n",
            "Train Accuracy: 0.9036111235618591\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3062 - accuracy: 0.9036 - auc: 0.9614 - prc: 0.9048 - val_loss: 0.4240 - val_accuracy: 0.8475 - val_auc: 0.8712 - val_prc: 0.7690\n",
            "Epoch 68/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.9045 - auc: 0.9633 - prc: 0.9052\n",
            "Epoch: 68\n",
            "Train Loss: 0.300205260515213\n",
            "Val Loss: 0.4068719744682312\n",
            "Train Accuracy: 0.9052777886390686\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3002 - accuracy: 0.9053 - auc: 0.9636 - prc: 0.9063 - val_loss: 0.4069 - val_accuracy: 0.8550 - val_auc: 0.8726 - val_prc: 0.7704\n",
            "Epoch 69/100\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.9050 - auc: 0.9629 - prc: 0.9058\n",
            "Epoch: 69\n",
            "Train Loss: 0.30158400535583496\n",
            "Val Loss: 0.4134398400783539\n",
            "Train Accuracy: 0.9058333039283752\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3016 - accuracy: 0.9058 - auc: 0.9635 - prc: 0.9073 - val_loss: 0.4134 - val_accuracy: 0.8525 - val_auc: 0.8723 - val_prc: 0.7711\n",
            "Epoch 70/100\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3045 - accuracy: 0.9038 - auc: 0.9616 - prc: 0.9059\n",
            "Epoch: 70\n",
            "Train Loss: 0.3045823574066162\n",
            "Val Loss: 0.40327367186546326\n",
            "Train Accuracy: 0.9038888812065125\n",
            "Val Accuracy: 0.8575000166893005\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3046 - accuracy: 0.9039 - auc: 0.9616 - prc: 0.9059 - val_loss: 0.4033 - val_accuracy: 0.8575 - val_auc: 0.8739 - val_prc: 0.7702\n",
            "Epoch 71/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.9037 - auc: 0.9620 - prc: 0.9061\n",
            "Epoch: 71\n",
            "Train Loss: 0.3023427128791809\n",
            "Val Loss: 0.41581404209136963\n",
            "Train Accuracy: 0.9036111235618591\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3023 - accuracy: 0.9036 - auc: 0.9624 - prc: 0.9061 - val_loss: 0.4158 - val_accuracy: 0.8550 - val_auc: 0.8722 - val_prc: 0.7699\n",
            "Epoch 72/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.9025 - auc: 0.9625 - prc: 0.9088\n",
            "Epoch: 72\n",
            "Train Loss: 0.30382829904556274\n",
            "Val Loss: 0.41848254203796387\n",
            "Train Accuracy: 0.902222216129303\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3038 - accuracy: 0.9022 - auc: 0.9622 - prc: 0.9076 - val_loss: 0.4185 - val_accuracy: 0.8525 - val_auc: 0.8727 - val_prc: 0.7704\n",
            "Epoch 73/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.8976 - auc: 0.9607 - prc: 0.9063\n",
            "Epoch: 73\n",
            "Train Loss: 0.3077143728733063\n",
            "Val Loss: 0.42040127515792847\n",
            "Train Accuracy: 0.897777795791626\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3077 - accuracy: 0.8978 - auc: 0.9608 - prc: 0.9065 - val_loss: 0.4204 - val_accuracy: 0.8475 - val_auc: 0.8718 - val_prc: 0.7693\n",
            "Epoch 74/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.9003 - auc: 0.9622 - prc: 0.9038\n",
            "Epoch: 74\n",
            "Train Loss: 0.3050830662250519\n",
            "Val Loss: 0.4205549657344818\n",
            "Train Accuracy: 0.9002777934074402\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3051 - accuracy: 0.9003 - auc: 0.9618 - prc: 0.9038 - val_loss: 0.4206 - val_accuracy: 0.8500 - val_auc: 0.8724 - val_prc: 0.7708\n",
            "Epoch 75/100\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9063 - auc: 0.9643 - prc: 0.9115\n",
            "Epoch: 75\n",
            "Train Loss: 0.2965931296348572\n",
            "Val Loss: 0.41614219546318054\n",
            "Train Accuracy: 0.9069444537162781\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2966 - accuracy: 0.9069 - auc: 0.9644 - prc: 0.9120 - val_loss: 0.4161 - val_accuracy: 0.8550 - val_auc: 0.8722 - val_prc: 0.7704\n",
            "Epoch 76/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.9061 - auc: 0.9634 - prc: 0.9087\n",
            "Epoch: 76\n",
            "Train Loss: 0.2988709509372711\n",
            "Val Loss: 0.4252083897590637\n",
            "Train Accuracy: 0.9061111211776733\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.2989 - accuracy: 0.9061 - auc: 0.9635 - prc: 0.9087 - val_loss: 0.4252 - val_accuracy: 0.8500 - val_auc: 0.8717 - val_prc: 0.7713\n",
            "Epoch 77/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.9032 - auc: 0.9627 - prc: 0.9086\n",
            "Epoch: 77\n",
            "Train Loss: 0.3010682761669159\n",
            "Val Loss: 0.4214145243167877\n",
            "Train Accuracy: 0.9027777910232544\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3011 - accuracy: 0.9028 - auc: 0.9626 - prc: 0.9083 - val_loss: 0.4214 - val_accuracy: 0.8500 - val_auc: 0.8716 - val_prc: 0.7709\n",
            "Epoch 78/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.9068 - auc: 0.9635 - prc: 0.9080\n",
            "Epoch: 78\n",
            "Train Loss: 0.2994260787963867\n",
            "Val Loss: 0.41638875007629395\n",
            "Train Accuracy: 0.9063888788223267\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2994 - accuracy: 0.9064 - auc: 0.9635 - prc: 0.9081 - val_loss: 0.4164 - val_accuracy: 0.8525 - val_auc: 0.8723 - val_prc: 0.7699\n",
            "Epoch 79/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.9077 - auc: 0.9635 - prc: 0.9079\n",
            "Epoch: 79\n",
            "Train Loss: 0.29815036058425903\n",
            "Val Loss: 0.4116435945034027\n",
            "Train Accuracy: 0.9083333611488342\n",
            "Val Accuracy: 0.8575000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2982 - accuracy: 0.9083 - auc: 0.9638 - prc: 0.9096 - val_loss: 0.4116 - val_accuracy: 0.8575 - val_auc: 0.8736 - val_prc: 0.7703\n",
            "Epoch 80/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9054 - auc: 0.9619 - prc: 0.9059\n",
            "Epoch: 80\n",
            "Train Loss: 0.3043609857559204\n",
            "Val Loss: 0.43800535798072815\n",
            "Train Accuracy: 0.9044444561004639\n",
            "Val Accuracy: 0.8324999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3044 - accuracy: 0.9044 - auc: 0.9611 - prc: 0.9057 - val_loss: 0.4380 - val_accuracy: 0.8325 - val_auc: 0.8704 - val_prc: 0.7691\n",
            "Epoch 81/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.9043 - auc: 0.9612 - prc: 0.9054\n",
            "Epoch: 81\n",
            "Train Loss: 0.3033287823200226\n",
            "Val Loss: 0.4257065951824188\n",
            "Train Accuracy: 0.9038888812065125\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3033 - accuracy: 0.9039 - auc: 0.9612 - prc: 0.9057 - val_loss: 0.4257 - val_accuracy: 0.8450 - val_auc: 0.8724 - val_prc: 0.7704\n",
            "Epoch 82/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.9042 - auc: 0.9631 - prc: 0.9088\n",
            "Epoch: 82\n",
            "Train Loss: 0.3008793592453003\n",
            "Val Loss: 0.4280487298965454\n",
            "Train Accuracy: 0.9041666388511658\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3009 - accuracy: 0.9042 - auc: 0.9629 - prc: 0.9090 - val_loss: 0.4280 - val_accuracy: 0.8400 - val_auc: 0.8715 - val_prc: 0.7689\n",
            "Epoch 83/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.8990 - auc: 0.9608 - prc: 0.9019\n",
            "Epoch: 83\n",
            "Train Loss: 0.3057734966278076\n",
            "Val Loss: 0.42085182666778564\n",
            "Train Accuracy: 0.898888885974884\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3058 - accuracy: 0.8989 - auc: 0.9609 - prc: 0.9022 - val_loss: 0.4209 - val_accuracy: 0.8475 - val_auc: 0.8717 - val_prc: 0.7688\n",
            "Epoch 84/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9040 - auc: 0.9635 - prc: 0.9112\n",
            "Epoch: 84\n",
            "Train Loss: 0.3003430962562561\n",
            "Val Loss: 0.4160982072353363\n",
            "Train Accuracy: 0.9038888812065125\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3003 - accuracy: 0.9039 - auc: 0.9631 - prc: 0.9113 - val_loss: 0.4161 - val_accuracy: 0.8500 - val_auc: 0.8731 - val_prc: 0.7706\n",
            "Epoch 85/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9105 - auc: 0.9672 - prc: 0.9195\n",
            "Epoch: 85\n",
            "Train Loss: 0.29216620326042175\n",
            "Val Loss: 0.42952728271484375\n",
            "Train Accuracy: 0.9100000262260437\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2922 - accuracy: 0.9100 - auc: 0.9668 - prc: 0.9169 - val_loss: 0.4295 - val_accuracy: 0.8400 - val_auc: 0.8715 - val_prc: 0.7693\n",
            "Epoch 86/100\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.9034 - auc: 0.9622 - prc: 0.9104\n",
            "Epoch: 86\n",
            "Train Loss: 0.30066800117492676\n",
            "Val Loss: 0.41555747389793396\n",
            "Train Accuracy: 0.903333306312561\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3007 - accuracy: 0.9033 - auc: 0.9622 - prc: 0.9105 - val_loss: 0.4156 - val_accuracy: 0.8525 - val_auc: 0.8730 - val_prc: 0.7702\n",
            "Epoch 87/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.9060 - auc: 0.9648 - prc: 0.9092\n",
            "Epoch: 87\n",
            "Train Loss: 0.2972078025341034\n",
            "Val Loss: 0.4263668358325958\n",
            "Train Accuracy: 0.9047222137451172\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.2972 - accuracy: 0.9047 - auc: 0.9644 - prc: 0.9085 - val_loss: 0.4264 - val_accuracy: 0.8425 - val_auc: 0.8718 - val_prc: 0.7703\n",
            "Epoch 88/100\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.9017 - auc: 0.9647 - prc: 0.9056\n",
            "Epoch: 88\n",
            "Train Loss: 0.296270489692688\n",
            "Val Loss: 0.40761062502861023\n",
            "Train Accuracy: 0.9011111259460449\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.2963 - accuracy: 0.9011 - auc: 0.9647 - prc: 0.9060 - val_loss: 0.4076 - val_accuracy: 0.8550 - val_auc: 0.8745 - val_prc: 0.7723\n",
            "Epoch 89/100\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.9031 - auc: 0.9632 - prc: 0.9135\n",
            "Epoch: 89\n",
            "Train Loss: 0.2987781763076782\n",
            "Val Loss: 0.41842103004455566\n",
            "Train Accuracy: 0.903333306312561\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2988 - accuracy: 0.9033 - auc: 0.9632 - prc: 0.9138 - val_loss: 0.4184 - val_accuracy: 0.8525 - val_auc: 0.8728 - val_prc: 0.7713\n",
            "Epoch 90/100\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.9087 - auc: 0.9652 - prc: 0.9149\n",
            "Epoch: 90\n",
            "Train Loss: 0.29332637786865234\n",
            "Val Loss: 0.4315265715122223\n",
            "Train Accuracy: 0.9086111187934875\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.2933 - accuracy: 0.9086 - auc: 0.9653 - prc: 0.9151 - val_loss: 0.4315 - val_accuracy: 0.8400 - val_auc: 0.8709 - val_prc: 0.7692\n",
            "Epoch 91/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.9078 - auc: 0.9638 - prc: 0.9126\n",
            "Epoch: 91\n",
            "Train Loss: 0.2976766526699066\n",
            "Val Loss: 0.4257412850856781\n",
            "Train Accuracy: 0.9075000286102295\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2977 - accuracy: 0.9075 - auc: 0.9635 - prc: 0.9114 - val_loss: 0.4257 - val_accuracy: 0.8425 - val_auc: 0.8714 - val_prc: 0.7700\n",
            "Epoch 92/100\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9100 - auc: 0.9651 - prc: 0.9133\n",
            "Epoch: 92\n",
            "Train Loss: 0.29412081837654114\n",
            "Val Loss: 0.42787644267082214\n",
            "Train Accuracy: 0.9100000262260437\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.2941 - accuracy: 0.9100 - auc: 0.9651 - prc: 0.9133 - val_loss: 0.4279 - val_accuracy: 0.8400 - val_auc: 0.8712 - val_prc: 0.7693\n",
            "Epoch 93/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.9044 - auc: 0.9627 - prc: 0.9115\n",
            "Epoch: 93\n",
            "Train Loss: 0.2999882996082306\n",
            "Val Loss: 0.4163272976875305\n",
            "Train Accuracy: 0.9052777886390686\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3000 - accuracy: 0.9053 - auc: 0.9630 - prc: 0.9118 - val_loss: 0.4163 - val_accuracy: 0.8475 - val_auc: 0.8726 - val_prc: 0.7710\n",
            "Epoch 94/100\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9048 - auc: 0.9653 - prc: 0.9162\n",
            "Epoch: 94\n",
            "Train Loss: 0.29347464442253113\n",
            "Val Loss: 0.4225251078605652\n",
            "Train Accuracy: 0.9047222137451172\n",
            "Val Accuracy: 0.8450000286102295\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.2935 - accuracy: 0.9047 - auc: 0.9654 - prc: 0.9160 - val_loss: 0.4225 - val_accuracy: 0.8450 - val_auc: 0.8722 - val_prc: 0.7696\n",
            "Epoch 95/100\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9099 - auc: 0.9666 - prc: 0.9185\n",
            "Epoch: 95\n",
            "Train Loss: 0.2896310091018677\n",
            "Val Loss: 0.43476802110671997\n",
            "Train Accuracy: 0.9100000262260437\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2896 - accuracy: 0.9100 - auc: 0.9669 - prc: 0.9190 - val_loss: 0.4348 - val_accuracy: 0.8375 - val_auc: 0.8714 - val_prc: 0.7696\n",
            "Epoch 96/100\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.9036 - auc: 0.9645 - prc: 0.9131\n",
            "Epoch: 96\n",
            "Train Loss: 0.29802417755126953\n",
            "Val Loss: 0.43171244859695435\n",
            "Train Accuracy: 0.9036111235618591\n",
            "Val Accuracy: 0.8399999737739563\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2980 - accuracy: 0.9036 - auc: 0.9641 - prc: 0.9124 - val_loss: 0.4317 - val_accuracy: 0.8400 - val_auc: 0.8712 - val_prc: 0.7700\n",
            "Epoch 97/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.9058 - auc: 0.9656 - prc: 0.9135\n",
            "Epoch: 97\n",
            "Train Loss: 0.2918018698692322\n",
            "Val Loss: 0.4119757413864136\n",
            "Train Accuracy: 0.9061111211776733\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.2918 - accuracy: 0.9061 - auc: 0.9657 - prc: 0.9137 - val_loss: 0.4120 - val_accuracy: 0.8550 - val_auc: 0.8737 - val_prc: 0.7704\n",
            "Epoch 98/100\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9123 - auc: 0.9672 - prc: 0.9213\n",
            "Epoch: 98\n",
            "Train Loss: 0.28722965717315674\n",
            "Val Loss: 0.4136275053024292\n",
            "Train Accuracy: 0.9127777814865112\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.2872 - accuracy: 0.9128 - auc: 0.9673 - prc: 0.9214 - val_loss: 0.4136 - val_accuracy: 0.8550 - val_auc: 0.8737 - val_prc: 0.7706\n",
            "Epoch 99/100\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9133 - auc: 0.9673 - prc: 0.9206\n",
            "Epoch: 99\n",
            "Train Loss: 0.28724199533462524\n",
            "Val Loss: 0.41449809074401855\n",
            "Train Accuracy: 0.9133333563804626\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 6ms/step - loss: 0.2872 - accuracy: 0.9133 - auc: 0.9673 - prc: 0.9195 - val_loss: 0.4145 - val_accuracy: 0.8500 - val_auc: 0.8732 - val_prc: 0.7699\n",
            "Epoch 100/100\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.9082 - auc: 0.9641 - prc: 0.9098\n",
            "Epoch: 100\n",
            "Train Loss: 0.2952621579170227\n",
            "Val Loss: 0.4185486137866974\n",
            "Train Accuracy: 0.9077777862548828\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2953 - accuracy: 0.9078 - auc: 0.9641 - prc: 0.9096 - val_loss: 0.4185 - val_accuracy: 0.8475 - val_auc: 0.8715 - val_prc: 0.7687\n"
          ]
        }
      ],
      "source": [
        "history2 = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200, batch_size=8,\n",
        "    validation_split=0.1,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[print_metrics_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "0DAakEGe1p3d",
        "outputId": "ead97a9b-94c9-4382-d3c9-963365e44923"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "history.history['loss'].extend(history2.history['loss'])\n",
        "history.history['val_loss'].extend(history2.history['val_loss'])\n",
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bazF_mop3Ch5",
        "outputId": "84479807-744a-4da9-ad73-66dbde08374c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.87       710\n",
            "           1       0.68      0.73      0.71       290\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.79      0.80      0.79      1000\n",
            "weighted avg       0.83      0.82      0.83      1000\n",
            "\n",
            "AUC Score: 0.8744924720738222\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print performance metrics\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaCoYKqI42Rq"
      },
      "source": [
        "It has converged at epoch ~400, So we should stop training at epoch 400 for this configuration of MLP model. Note that the validation loss is not going up yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fyQGcnF05iOe",
        "outputId": "91b4dad1-da2e-49a8-eba9-c05f050cfb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.6412 - accuracy: 0.6646 - auc: 0.7126 - prc: 0.4870\n",
            "Epoch: 1\n",
            "Train Loss: 0.6400520205497742\n",
            "Val Loss: 0.5669969320297241\n",
            "Train Accuracy: 0.6652500033378601\n",
            "Val Accuracy: 0.75\n",
            "450/450 [==============================] - 6s 10ms/step - loss: 0.6401 - accuracy: 0.6653 - auc: 0.7129 - prc: 0.4864 - val_loss: 0.5670 - val_accuracy: 0.7500 - val_auc: 0.7942 - val_prc: 0.5294\n",
            "Epoch 2/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5423 - accuracy: 0.7425 - auc: 0.8073 - prc: 0.6155\n",
            "Epoch: 2\n",
            "Train Loss: 0.5423250794410706\n",
            "Val Loss: 0.6275832653045654\n",
            "Train Accuracy: 0.7425000071525574\n",
            "Val Accuracy: 0.6600000262260437\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.5423 - accuracy: 0.7425 - auc: 0.8073 - prc: 0.6155 - val_loss: 0.6276 - val_accuracy: 0.6600 - val_auc: 0.8199 - val_prc: 0.6593\n",
            "Epoch 3/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.5107 - accuracy: 0.7595 - auc: 0.8336 - prc: 0.6548\n",
            "Epoch: 3\n",
            "Train Loss: 0.5106511116027832\n",
            "Val Loss: 0.7521575093269348\n",
            "Train Accuracy: 0.7594444155693054\n",
            "Val Accuracy: 0.5874999761581421\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.5107 - accuracy: 0.7594 - auc: 0.8335 - prc: 0.6544 - val_loss: 0.7522 - val_accuracy: 0.5875 - val_auc: 0.8260 - val_prc: 0.6850\n",
            "Epoch 4/50\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4715 - accuracy: 0.7868 - auc: 0.8605 - prc: 0.7190\n",
            "Epoch: 4\n",
            "Train Loss: 0.47258439660072327\n",
            "Val Loss: 0.4442867338657379\n",
            "Train Accuracy: 0.7872222065925598\n",
            "Val Accuracy: 0.800000011920929\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4726 - accuracy: 0.7872 - auc: 0.8601 - prc: 0.7202 - val_loss: 0.4443 - val_accuracy: 0.8000 - val_auc: 0.8443 - val_prc: 0.7158\n",
            "Epoch 5/50\n",
            "439/450 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.7993 - auc: 0.8720 - prc: 0.7452\n",
            "Epoch: 5\n",
            "Train Loss: 0.45218929648399353\n",
            "Val Loss: 0.41236981749534607\n",
            "Train Accuracy: 0.8008333444595337\n",
            "Val Accuracy: 0.8424999713897705\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4522 - accuracy: 0.8008 - auc: 0.8731 - prc: 0.7451 - val_loss: 0.4124 - val_accuracy: 0.8425 - val_auc: 0.8530 - val_prc: 0.7250\n",
            "Epoch 6/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.8110 - auc: 0.8771 - prc: 0.7491\n",
            "Epoch: 6\n",
            "Train Loss: 0.4447917938232422\n",
            "Val Loss: 0.36858895421028137\n",
            "Train Accuracy: 0.8113889098167419\n",
            "Val Accuracy: 0.862500011920929\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4448 - accuracy: 0.8114 - auc: 0.8777 - prc: 0.7507 - val_loss: 0.3686 - val_accuracy: 0.8625 - val_auc: 0.8604 - val_prc: 0.7400\n",
            "Epoch 7/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.4294 - accuracy: 0.8111 - auc: 0.8859 - prc: 0.7676\n",
            "Epoch: 7\n",
            "Train Loss: 0.4286472499370575\n",
            "Val Loss: 0.4202657639980316\n",
            "Train Accuracy: 0.8111110925674438\n",
            "Val Accuracy: 0.8274999856948853\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4286 - accuracy: 0.8111 - auc: 0.8861 - prc: 0.7678 - val_loss: 0.4203 - val_accuracy: 0.8275 - val_auc: 0.8653 - val_prc: 0.7551\n",
            "Epoch 8/50\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.4123 - accuracy: 0.8237 - auc: 0.8950 - prc: 0.7865\n",
            "Epoch: 8\n",
            "Train Loss: 0.41384652256965637\n",
            "Val Loss: 0.4234578013420105\n",
            "Train Accuracy: 0.8247222304344177\n",
            "Val Accuracy: 0.8349999785423279\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4138 - accuracy: 0.8247 - auc: 0.8945 - prc: 0.7879 - val_loss: 0.4235 - val_accuracy: 0.8350 - val_auc: 0.8598 - val_prc: 0.7504\n",
            "Epoch 9/50\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.3982 - accuracy: 0.8311 - auc: 0.9034 - prc: 0.8005\n",
            "Epoch: 9\n",
            "Train Loss: 0.3977276384830475\n",
            "Val Loss: 0.37347304821014404\n",
            "Train Accuracy: 0.8316666483879089\n",
            "Val Accuracy: 0.8525000214576721\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3977 - accuracy: 0.8317 - auc: 0.9040 - prc: 0.8019 - val_loss: 0.3735 - val_accuracy: 0.8525 - val_auc: 0.8686 - val_prc: 0.7607\n",
            "Epoch 10/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3901 - accuracy: 0.8364 - auc: 0.9072 - prc: 0.8081\n",
            "Epoch: 10\n",
            "Train Loss: 0.39236778020858765\n",
            "Val Loss: 0.3704787790775299\n",
            "Train Accuracy: 0.8358333110809326\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3924 - accuracy: 0.8358 - auc: 0.9060 - prc: 0.8070 - val_loss: 0.3705 - val_accuracy: 0.8550 - val_auc: 0.8680 - val_prc: 0.7613\n",
            "Epoch 11/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8482 - auc: 0.9171 - prc: 0.8257\n",
            "Epoch: 11\n",
            "Train Loss: 0.37281301617622375\n",
            "Val Loss: 0.3972324728965759\n",
            "Train Accuracy: 0.8480555415153503\n",
            "Val Accuracy: 0.8349999785423279\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3728 - accuracy: 0.8481 - auc: 0.9173 - prc: 0.8263 - val_loss: 0.3972 - val_accuracy: 0.8350 - val_auc: 0.8710 - val_prc: 0.7681\n",
            "Epoch 12/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3746 - accuracy: 0.8410 - auc: 0.9152 - prc: 0.8202\n",
            "Epoch: 12\n",
            "Train Loss: 0.3742792010307312\n",
            "Val Loss: 0.5278147459030151\n",
            "Train Accuracy: 0.8405555486679077\n",
            "Val Accuracy: 0.7325000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3743 - accuracy: 0.8406 - auc: 0.9155 - prc: 0.8215 - val_loss: 0.5278 - val_accuracy: 0.7325 - val_auc: 0.8649 - val_prc: 0.7547\n",
            "Epoch 13/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.3664 - accuracy: 0.8486 - auc: 0.9207 - prc: 0.8355\n",
            "Epoch: 13\n",
            "Train Loss: 0.36638373136520386\n",
            "Val Loss: 0.3575047254562378\n",
            "Train Accuracy: 0.8486111164093018\n",
            "Val Accuracy: 0.8600000143051147\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3664 - accuracy: 0.8486 - auc: 0.9207 - prc: 0.8355 - val_loss: 0.3575 - val_accuracy: 0.8600 - val_auc: 0.8759 - val_prc: 0.7739\n",
            "Epoch 14/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.8535 - auc: 0.9270 - prc: 0.8443\n",
            "Epoch: 14\n",
            "Train Loss: 0.34946975111961365\n",
            "Val Loss: 0.4146363139152527\n",
            "Train Accuracy: 0.8538888692855835\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3495 - accuracy: 0.8539 - auc: 0.9271 - prc: 0.8435 - val_loss: 0.4146 - val_accuracy: 0.8050 - val_auc: 0.8729 - val_prc: 0.7759\n",
            "Epoch 15/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8580 - auc: 0.9329 - prc: 0.8560\n",
            "Epoch: 15\n",
            "Train Loss: 0.33615240454673767\n",
            "Val Loss: 0.42562398314476013\n",
            "Train Accuracy: 0.8583333492279053\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3362 - accuracy: 0.8583 - auc: 0.9333 - prc: 0.8569 - val_loss: 0.4256 - val_accuracy: 0.8250 - val_auc: 0.8687 - val_prc: 0.7478\n",
            "Epoch 16/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.3416 - accuracy: 0.8636 - auc: 0.9316 - prc: 0.8496\n",
            "Epoch: 16\n",
            "Train Loss: 0.34266841411590576\n",
            "Val Loss: 0.4181276559829712\n",
            "Train Accuracy: 0.8633333444595337\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3427 - accuracy: 0.8633 - auc: 0.9311 - prc: 0.8489 - val_loss: 0.4181 - val_accuracy: 0.8150 - val_auc: 0.8736 - val_prc: 0.7790\n",
            "Epoch 17/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.8647 - auc: 0.9372 - prc: 0.8667\n",
            "Epoch: 17\n",
            "Train Loss: 0.32432207465171814\n",
            "Val Loss: 0.3871528208255768\n",
            "Train Accuracy: 0.8647222518920898\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.3243 - accuracy: 0.8647 - auc: 0.9371 - prc: 0.8668 - val_loss: 0.3872 - val_accuracy: 0.8475 - val_auc: 0.8793 - val_prc: 0.7777\n",
            "Epoch 18/50\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8713 - auc: 0.9401 - prc: 0.8720\n",
            "Epoch: 18\n",
            "Train Loss: 0.318134605884552\n",
            "Val Loss: 0.32476672530174255\n",
            "Train Accuracy: 0.8722222447395325\n",
            "Val Accuracy: 0.8774999976158142\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.3181 - accuracy: 0.8722 - auc: 0.9406 - prc: 0.8720 - val_loss: 0.3248 - val_accuracy: 0.8775 - val_auc: 0.8738 - val_prc: 0.7746\n",
            "Epoch 19/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8688 - auc: 0.9395 - prc: 0.8661\n",
            "Epoch: 19\n",
            "Train Loss: 0.31962165236473083\n",
            "Val Loss: 0.4309316873550415\n",
            "Train Accuracy: 0.8691666722297668\n",
            "Val Accuracy: 0.8125\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3196 - accuracy: 0.8692 - auc: 0.9398 - prc: 0.8668 - val_loss: 0.4309 - val_accuracy: 0.8125 - val_auc: 0.8665 - val_prc: 0.7709\n",
            "Epoch 20/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.2997 - accuracy: 0.8784 - auc: 0.9466 - prc: 0.8865\n",
            "Epoch: 20\n",
            "Train Loss: 0.2988241910934448\n",
            "Val Loss: 0.35168808698654175\n",
            "Train Accuracy: 0.8788889050483704\n",
            "Val Accuracy: 0.8824999928474426\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.2988 - accuracy: 0.8789 - auc: 0.9469 - prc: 0.8867 - val_loss: 0.3517 - val_accuracy: 0.8825 - val_auc: 0.8722 - val_prc: 0.7500\n",
            "Epoch 21/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8825 - auc: 0.9471 - prc: 0.8838\n",
            "Epoch: 21\n",
            "Train Loss: 0.29798275232315063\n",
            "Val Loss: 0.38309136033058167\n",
            "Train Accuracy: 0.8822222352027893\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 5s 12ms/step - loss: 0.2980 - accuracy: 0.8822 - auc: 0.9470 - prc: 0.8837 - val_loss: 0.3831 - val_accuracy: 0.8500 - val_auc: 0.8752 - val_prc: 0.7688\n",
            "Epoch 22/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8806 - auc: 0.9472 - prc: 0.8879\n",
            "Epoch: 22\n",
            "Train Loss: 0.29762667417526245\n",
            "Val Loss: 0.3465448021888733\n",
            "Train Accuracy: 0.8805555701255798\n",
            "Val Accuracy: 0.875\n",
            "450/450 [==============================] - 5s 10ms/step - loss: 0.2976 - accuracy: 0.8806 - auc: 0.9471 - prc: 0.8880 - val_loss: 0.3465 - val_accuracy: 0.8750 - val_auc: 0.8772 - val_prc: 0.7593\n",
            "Epoch 23/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.2954 - accuracy: 0.8811 - auc: 0.9484 - prc: 0.8878\n",
            "Epoch: 23\n",
            "Train Loss: 0.2954026758670807\n",
            "Val Loss: 0.37149786949157715\n",
            "Train Accuracy: 0.8811110854148865\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.2954 - accuracy: 0.8811 - auc: 0.9484 - prc: 0.8878 - val_loss: 0.3715 - val_accuracy: 0.8500 - val_auc: 0.8748 - val_prc: 0.7717\n",
            "Epoch 24/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8826 - auc: 0.9505 - prc: 0.8886\n",
            "Epoch: 24\n",
            "Train Loss: 0.2880646288394928\n",
            "Val Loss: 0.4625431299209595\n",
            "Train Accuracy: 0.8836110830307007\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.2881 - accuracy: 0.8836 - auc: 0.9511 - prc: 0.8895 - val_loss: 0.4625 - val_accuracy: 0.8025 - val_auc: 0.8718 - val_prc: 0.7668\n",
            "Epoch 25/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.8950 - auc: 0.9577 - prc: 0.9007\n",
            "Epoch: 25\n",
            "Train Loss: 0.2701658606529236\n",
            "Val Loss: 0.44094032049179077\n",
            "Train Accuracy: 0.8949999809265137\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 6s 14ms/step - loss: 0.2702 - accuracy: 0.8950 - auc: 0.9577 - prc: 0.9007 - val_loss: 0.4409 - val_accuracy: 0.8050 - val_auc: 0.8644 - val_prc: 0.7689\n",
            "Epoch 26/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.8922 - auc: 0.9579 - prc: 0.9092\n",
            "Epoch: 26\n",
            "Train Loss: 0.26746585965156555\n",
            "Val Loss: 0.3785836696624756\n",
            "Train Accuracy: 0.8922222256660461\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.2675 - accuracy: 0.8922 - auc: 0.9579 - prc: 0.9092 - val_loss: 0.3786 - val_accuracy: 0.8550 - val_auc: 0.8710 - val_prc: 0.7426\n",
            "Epoch 27/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8973 - auc: 0.9575 - prc: 0.9052\n",
            "Epoch: 27\n",
            "Train Loss: 0.2694104313850403\n",
            "Val Loss: 0.38329440355300903\n",
            "Train Accuracy: 0.8974999785423279\n",
            "Val Accuracy: 0.8475000262260437\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.2694 - accuracy: 0.8975 - auc: 0.9576 - prc: 0.9054 - val_loss: 0.3833 - val_accuracy: 0.8475 - val_auc: 0.8680 - val_prc: 0.7633\n",
            "Epoch 28/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.2744 - accuracy: 0.8895 - auc: 0.9553 - prc: 0.9036\n",
            "Epoch: 28\n",
            "Train Loss: 0.273357093334198\n",
            "Val Loss: 0.6216943860054016\n",
            "Train Accuracy: 0.8899999856948853\n",
            "Val Accuracy: 0.7325000166893005\n",
            "450/450 [==============================] - 6s 12ms/step - loss: 0.2734 - accuracy: 0.8900 - auc: 0.9557 - prc: 0.9047 - val_loss: 0.6217 - val_accuracy: 0.7325 - val_auc: 0.8543 - val_prc: 0.7502\n",
            "Epoch 29/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.9013 - auc: 0.9598 - prc: 0.9080\n",
            "Epoch: 29\n",
            "Train Loss: 0.2616451382637024\n",
            "Val Loss: 0.40689533948898315\n",
            "Train Accuracy: 0.9013888835906982\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 6s 12ms/step - loss: 0.2616 - accuracy: 0.9014 - auc: 0.9600 - prc: 0.9083 - val_loss: 0.4069 - val_accuracy: 0.8550 - val_auc: 0.8713 - val_prc: 0.7448\n",
            "Epoch 30/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2558 - accuracy: 0.8948 - auc: 0.9614 - prc: 0.9142\n",
            "Epoch: 30\n",
            "Train Loss: 0.25523072481155396\n",
            "Val Loss: 0.4271472990512848\n",
            "Train Accuracy: 0.8949999809265137\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.2552 - accuracy: 0.8950 - auc: 0.9616 - prc: 0.9146 - val_loss: 0.4271 - val_accuracy: 0.8375 - val_auc: 0.8658 - val_prc: 0.7407\n",
            "Epoch 31/50\n",
            "439/450 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9026 - auc: 0.9643 - prc: 0.9241\n",
            "Epoch: 31\n",
            "Train Loss: 0.25054293870925903\n",
            "Val Loss: 0.45251351594924927\n",
            "Train Accuracy: 0.9008333086967468\n",
            "Val Accuracy: 0.8100000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2505 - accuracy: 0.9008 - auc: 0.9633 - prc: 0.9223 - val_loss: 0.4525 - val_accuracy: 0.8100 - val_auc: 0.8672 - val_prc: 0.7593\n",
            "Epoch 32/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9073 - auc: 0.9675 - prc: 0.9331\n",
            "Epoch: 32\n",
            "Train Loss: 0.23476430773735046\n",
            "Val Loss: 0.4386226236820221\n",
            "Train Accuracy: 0.9080555438995361\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.2348 - accuracy: 0.9081 - auc: 0.9680 - prc: 0.9341 - val_loss: 0.4386 - val_accuracy: 0.8250 - val_auc: 0.8648 - val_prc: 0.7653\n",
            "Epoch 33/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9065 - auc: 0.9665 - prc: 0.9223\n",
            "Epoch: 33\n",
            "Train Loss: 0.23982031643390656\n",
            "Val Loss: 0.41291797161102295\n",
            "Train Accuracy: 0.9066666960716248\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.2398 - accuracy: 0.9067 - auc: 0.9665 - prc: 0.9220 - val_loss: 0.4129 - val_accuracy: 0.8500 - val_auc: 0.8665 - val_prc: 0.7647\n",
            "Epoch 34/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2418 - accuracy: 0.9021 - auc: 0.9661 - prc: 0.9222\n",
            "Epoch: 34\n",
            "Train Loss: 0.24166961014270782\n",
            "Val Loss: 0.4358689785003662\n",
            "Train Accuracy: 0.902222216129303\n",
            "Val Accuracy: 0.8374999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2417 - accuracy: 0.9022 - auc: 0.9662 - prc: 0.9226 - val_loss: 0.4359 - val_accuracy: 0.8375 - val_auc: 0.8588 - val_prc: 0.7492\n",
            "Epoch 35/50\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9099 - auc: 0.9691 - prc: 0.9338\n",
            "Epoch: 35\n",
            "Train Loss: 0.2315533459186554\n",
            "Val Loss: 0.42016345262527466\n",
            "Train Accuracy: 0.9097222089767456\n",
            "Val Accuracy: 0.8299999833106995\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.2316 - accuracy: 0.9097 - auc: 0.9685 - prc: 0.9310 - val_loss: 0.4202 - val_accuracy: 0.8300 - val_auc: 0.8600 - val_prc: 0.7671\n",
            "Epoch 36/50\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9080 - auc: 0.9679 - prc: 0.9262\n",
            "Epoch: 36\n",
            "Train Loss: 0.23529644310474396\n",
            "Val Loss: 0.4799114167690277\n",
            "Train Accuracy: 0.9080555438995361\n",
            "Val Accuracy: 0.8100000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2353 - accuracy: 0.9081 - auc: 0.9674 - prc: 0.9255 - val_loss: 0.4799 - val_accuracy: 0.8100 - val_auc: 0.8668 - val_prc: 0.7455\n",
            "Epoch 37/50\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.2369 - accuracy: 0.9068 - auc: 0.9668 - prc: 0.9237\n",
            "Epoch: 37\n",
            "Train Loss: 0.2372943013906479\n",
            "Val Loss: 0.40997612476348877\n",
            "Train Accuracy: 0.9063888788223267\n",
            "Val Accuracy: 0.8650000095367432\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.2373 - accuracy: 0.9064 - auc: 0.9667 - prc: 0.9224 - val_loss: 0.4100 - val_accuracy: 0.8650 - val_auc: 0.8636 - val_prc: 0.7449\n",
            "Epoch 38/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9159 - auc: 0.9701 - prc: 0.9281\n",
            "Epoch: 38\n",
            "Train Loss: 0.22433310747146606\n",
            "Val Loss: 0.3881624937057495\n",
            "Train Accuracy: 0.9163888692855835\n",
            "Val Accuracy: 0.8799999952316284\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.2243 - accuracy: 0.9164 - auc: 0.9704 - prc: 0.9285 - val_loss: 0.3882 - val_accuracy: 0.8800 - val_auc: 0.8652 - val_prc: 0.7623\n",
            "Epoch 39/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9049 - auc: 0.9676 - prc: 0.9249\n",
            "Epoch: 39\n",
            "Train Loss: 0.23383161425590515\n",
            "Val Loss: 0.5291975736618042\n",
            "Train Accuracy: 0.9049999713897705\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.2338 - accuracy: 0.9050 - auc: 0.9677 - prc: 0.9251 - val_loss: 0.5292 - val_accuracy: 0.7750 - val_auc: 0.8470 - val_prc: 0.7206\n",
            "Epoch 40/50\n",
            "439/450 [============================>.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9200 - auc: 0.9740 - prc: 0.9420\n",
            "Epoch: 40\n",
            "Train Loss: 0.21071062982082367\n",
            "Val Loss: 0.405979186296463\n",
            "Train Accuracy: 0.9194444417953491\n",
            "Val Accuracy: 0.875\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.2107 - accuracy: 0.9194 - auc: 0.9738 - prc: 0.9398 - val_loss: 0.4060 - val_accuracy: 0.8750 - val_auc: 0.8578 - val_prc: 0.7546\n",
            "Epoch 41/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.2234 - accuracy: 0.9073 - auc: 0.9709 - prc: 0.9343\n",
            "Epoch: 41\n",
            "Train Loss: 0.2238709032535553\n",
            "Val Loss: 0.4405249059200287\n",
            "Train Accuracy: 0.9072222113609314\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.2239 - accuracy: 0.9072 - auc: 0.9708 - prc: 0.9340 - val_loss: 0.4405 - val_accuracy: 0.8500 - val_auc: 0.8541 - val_prc: 0.7366\n",
            "Epoch 42/50\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.9203 - auc: 0.9729 - prc: 0.9392\n",
            "Epoch: 42\n",
            "Train Loss: 0.2148004025220871\n",
            "Val Loss: 0.37134674191474915\n",
            "Train Accuracy: 0.9205555319786072\n",
            "Val Accuracy: 0.8899999856948853\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.2148 - accuracy: 0.9206 - auc: 0.9721 - prc: 0.9385 - val_loss: 0.3713 - val_accuracy: 0.8900 - val_auc: 0.8671 - val_prc: 0.7596\n",
            "Epoch 43/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9176 - auc: 0.9717 - prc: 0.9364\n",
            "Epoch: 43\n",
            "Train Loss: 0.2175929993391037\n",
            "Val Loss: 0.42963799834251404\n",
            "Train Accuracy: 0.9177777767181396\n",
            "Val Accuracy: 0.8600000143051147\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2176 - accuracy: 0.9178 - auc: 0.9718 - prc: 0.9365 - val_loss: 0.4296 - val_accuracy: 0.8600 - val_auc: 0.8579 - val_prc: 0.7268\n",
            "Epoch 44/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9208 - auc: 0.9748 - prc: 0.9393\n",
            "Epoch: 44\n",
            "Train Loss: 0.2059103548526764\n",
            "Val Loss: 0.4577081799507141\n",
            "Train Accuracy: 0.9208333492279053\n",
            "Val Accuracy: 0.8600000143051147\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2059 - accuracy: 0.9208 - auc: 0.9748 - prc: 0.9393 - val_loss: 0.4577 - val_accuracy: 0.8600 - val_auc: 0.8610 - val_prc: 0.7375\n",
            "Epoch 45/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.2107 - accuracy: 0.9219 - auc: 0.9738 - prc: 0.9421\n",
            "Epoch: 45\n",
            "Train Loss: 0.21088680624961853\n",
            "Val Loss: 0.5980349183082581\n",
            "Train Accuracy: 0.9219444394111633\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.2109 - accuracy: 0.9219 - auc: 0.9737 - prc: 0.9418 - val_loss: 0.5980 - val_accuracy: 0.7800 - val_auc: 0.8496 - val_prc: 0.7339\n",
            "Epoch 46/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.2130 - accuracy: 0.9187 - auc: 0.9730 - prc: 0.9392\n",
            "Epoch: 46\n",
            "Train Loss: 0.21275442838668823\n",
            "Val Loss: 0.434658408164978\n",
            "Train Accuracy: 0.9186111092567444\n",
            "Val Accuracy: 0.8575000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2128 - accuracy: 0.9186 - auc: 0.9731 - prc: 0.9394 - val_loss: 0.4347 - val_accuracy: 0.8575 - val_auc: 0.8558 - val_prc: 0.7339\n",
            "Epoch 47/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9225 - auc: 0.9743 - prc: 0.9376\n",
            "Epoch: 47\n",
            "Train Loss: 0.20647388696670532\n",
            "Val Loss: 0.4977288544178009\n",
            "Train Accuracy: 0.9225000143051147\n",
            "Val Accuracy: 0.8299999833106995\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2065 - accuracy: 0.9225 - auc: 0.9743 - prc: 0.9376 - val_loss: 0.4977 - val_accuracy: 0.8300 - val_auc: 0.8531 - val_prc: 0.7321\n",
            "Epoch 48/50\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9215 - auc: 0.9751 - prc: 0.9419\n",
            "Epoch: 48\n",
            "Train Loss: 0.20466488599777222\n",
            "Val Loss: 0.5863462090492249\n",
            "Train Accuracy: 0.92166668176651\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2047 - accuracy: 0.9217 - auc: 0.9752 - prc: 0.9427 - val_loss: 0.5863 - val_accuracy: 0.7700 - val_auc: 0.8607 - val_prc: 0.7145\n",
            "Epoch 49/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9295 - auc: 0.9769 - prc: 0.9438\n",
            "Epoch: 49\n",
            "Train Loss: 0.1958332508802414\n",
            "Val Loss: 0.4342595934867859\n",
            "Train Accuracy: 0.9300000071525574\n",
            "Val Accuracy: 0.8725000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.1958 - accuracy: 0.9300 - auc: 0.9772 - prc: 0.9443 - val_loss: 0.4343 - val_accuracy: 0.8725 - val_auc: 0.8611 - val_prc: 0.7232\n",
            "Epoch 50/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9228 - auc: 0.9756 - prc: 0.9446\n",
            "Epoch: 50\n",
            "Train Loss: 0.20288246870040894\n",
            "Val Loss: 0.42459067702293396\n",
            "Train Accuracy: 0.9227777719497681\n",
            "Val Accuracy: 0.8675000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.2029 - accuracy: 0.9228 - auc: 0.9756 - prc: 0.9446 - val_loss: 0.4246 - val_accuracy: 0.8675 - val_auc: 0.8579 - val_prc: 0.7199\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_matrix.shape[1],)),\n",
        "        Dropout(0.5),\n",
        "        #Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)),\n",
        "        #Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid',bias_initializer=keras.initializers.Constant(initial_bias))\n",
        "    ])\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50, batch_size=8,\n",
        "    validation_split=0.1,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[print_metrics_callback])\n",
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOe03UAl8OvH"
      },
      "source": [
        "For this configuration we need to stop training at epoch 10, where the validation loss is going up and them model is getting overfitted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-muSdOT5iLv",
        "outputId": "f0bad25b-2006-46b6-a8dd-1884b0f587df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       710\n",
            "           1       0.72      0.63      0.67       290\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.79      0.77      0.78      1000\n",
            "weighted avg       0.82      0.82      0.82      1000\n",
            "\n",
            "AUC Score: 0.8627367654201068\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print performance metrics\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgsT7My35iIq",
        "outputId": "76a88b2f-b3c9-4470-80f6-c1f3080a0ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.1942 - accuracy: 0.9248 - auc: 0.9674 - prc: 0.9299\n",
            "Epoch: 1\n",
            "Train Loss: 0.19472378492355347\n",
            "Val Loss: 0.5157363414764404\n",
            "Train Accuracy: 0.9242500066757202\n",
            "Val Accuracy: 0.8050000071525574\n",
            "450/450 [==============================] - 3s 5ms/step - loss: 0.1947 - accuracy: 0.9243 - auc: 0.9673 - prc: 0.9293 - val_loss: 0.5157 - val_accuracy: 0.8050 - val_auc: 0.8488 - val_prc: 0.7039\n",
            "Epoch 2/10\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.1860 - accuracy: 0.9330 - auc: 0.9798 - prc: 0.9516\n",
            "Epoch: 2\n",
            "Train Loss: 0.18497933447360992\n",
            "Val Loss: 0.5372897982597351\n",
            "Train Accuracy: 0.933055579662323\n",
            "Val Accuracy: 0.8174999952316284\n",
            "450/450 [==============================] - 2s 4ms/step - loss: 0.1850 - accuracy: 0.9331 - auc: 0.9800 - prc: 0.9522 - val_loss: 0.5373 - val_accuracy: 0.8175 - val_auc: 0.8568 - val_prc: 0.7141\n",
            "Epoch 3/10\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9255 - auc: 0.9780 - prc: 0.9494\n",
            "Epoch: 3\n",
            "Train Loss: 0.1920969933271408\n",
            "Val Loss: 0.4347895681858063\n",
            "Train Accuracy: 0.9258333444595337\n",
            "Val Accuracy: 0.8725000023841858\n",
            "450/450 [==============================] - 5s 10ms/step - loss: 0.1921 - accuracy: 0.9258 - auc: 0.9781 - prc: 0.9496 - val_loss: 0.4348 - val_accuracy: 0.8725 - val_auc: 0.8612 - val_prc: 0.7011\n",
            "Epoch 4/10\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9265 - auc: 0.9782 - prc: 0.9465\n",
            "Epoch: 4\n",
            "Train Loss: 0.19162197411060333\n",
            "Val Loss: 0.5731469988822937\n",
            "Train Accuracy: 0.9263888597488403\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.1916 - accuracy: 0.9264 - auc: 0.9781 - prc: 0.9463 - val_loss: 0.5731 - val_accuracy: 0.8075 - val_auc: 0.8521 - val_prc: 0.7282\n",
            "Epoch 5/10\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.1869 - accuracy: 0.9299 - auc: 0.9794 - prc: 0.9499\n",
            "Epoch: 5\n",
            "Train Loss: 0.18610058724880219\n",
            "Val Loss: 0.4549344778060913\n",
            "Train Accuracy: 0.9305555820465088\n",
            "Val Accuracy: 0.8500000238418579\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.1861 - accuracy: 0.9306 - auc: 0.9796 - prc: 0.9501 - val_loss: 0.4549 - val_accuracy: 0.8500 - val_auc: 0.8625 - val_prc: 0.7297\n",
            "Epoch 6/10\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9386 - auc: 0.9821 - prc: 0.9559\n",
            "Epoch: 6\n",
            "Train Loss: 0.1703149378299713\n",
            "Val Loss: 0.585783064365387\n",
            "Train Accuracy: 0.9386110901832581\n",
            "Val Accuracy: 0.8100000023841858\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.1703 - accuracy: 0.9386 - auc: 0.9821 - prc: 0.9559 - val_loss: 0.5858 - val_accuracy: 0.8100 - val_auc: 0.8645 - val_prc: 0.7367\n",
            "Epoch 7/10\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9317 - auc: 0.9817 - prc: 0.9561\n",
            "Epoch: 7\n",
            "Train Loss: 0.17528079450130463\n",
            "Val Loss: 0.4747115671634674\n",
            "Train Accuracy: 0.9319444298744202\n",
            "Val Accuracy: 0.8550000190734863\n",
            "450/450 [==============================] - 5s 10ms/step - loss: 0.1753 - accuracy: 0.9319 - auc: 0.9818 - prc: 0.9559 - val_loss: 0.4747 - val_accuracy: 0.8550 - val_auc: 0.8520 - val_prc: 0.6988\n",
            "Epoch 8/10\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9351 - auc: 0.9832 - prc: 0.9582\n",
            "Epoch: 8\n",
            "Train Loss: 0.16909167170524597\n",
            "Val Loss: 0.5250214338302612\n",
            "Train Accuracy: 0.9355555772781372\n",
            "Val Accuracy: 0.8274999856948853\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.1691 - accuracy: 0.9356 - auc: 0.9834 - prc: 0.9587 - val_loss: 0.5250 - val_accuracy: 0.8275 - val_auc: 0.8546 - val_prc: 0.7060\n",
            "Epoch 9/10\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.1818 - accuracy: 0.9293 - auc: 0.9801 - prc: 0.9551\n",
            "Epoch: 9\n",
            "Train Loss: 0.1810799092054367\n",
            "Val Loss: 0.438953161239624\n",
            "Train Accuracy: 0.9300000071525574\n",
            "Val Accuracy: 0.8650000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.1811 - accuracy: 0.9300 - auc: 0.9803 - prc: 0.9555 - val_loss: 0.4390 - val_accuracy: 0.8650 - val_auc: 0.8543 - val_prc: 0.7176\n",
            "Epoch 10/10\n",
            "437/450 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.9311 - auc: 0.9818 - prc: 0.9593\n",
            "Epoch: 10\n",
            "Train Loss: 0.17679420113563538\n",
            "Val Loss: 0.4866805970668793\n",
            "Train Accuracy: 0.9313889145851135\n",
            "Val Accuracy: 0.8600000143051147\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.1768 - accuracy: 0.9314 - auc: 0.9819 - prc: 0.9594 - val_loss: 0.4867 - val_accuracy: 0.8600 - val_auc: 0.8644 - val_prc: 0.7129\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10, batch_size=8,\n",
        "    validation_data=(X_test, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[print_metrics_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDZbrjcj5iBZ",
        "outputId": "c5bfca57-c4ec-4503-ab8b-a7ae8ddaea13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 5ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       710\n",
            "           1       0.69      0.68      0.69       290\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.78      0.78      0.78      1000\n",
            "weighted avg       0.82      0.82      0.82      1000\n",
            "\n",
            "AUC Score: 0.8612190383681398\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print performance metrics\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IY9rqj_1zoV"
      },
      "source": [
        "## Section4.5 Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZTwR5XxQlFU"
      },
      "source": [
        "### Keras Tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FbfCr33IQj1A",
        "outputId": "df875d0b-76bf-4993-8590-494d2c3a39ad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmodel = Sequential([\\n        Dense(128, activation='relu', input_shape=(input_matrix.shape[1],)),\\n        Dropout(0.5),\\n        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)),\\n        Dropout(0.3),\\n        Dense(1, activation='sigmoid',bias_initializer=keras.initializers.Constant(initial_bias))\\n    ])\\n\\n\\nprint_metrics_callback = PrintMetricsCallback()\\n\""
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def tuner_make_model(hp,  metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential()\n",
        "  model.add(Dense(\n",
        "      hp.Choice('units', [64, 128,256,512]),\n",
        "      activation='relu')\n",
        "  )\n",
        "  model.add(keras.layers.Dropout(hp.Choice('dropout_rate', values=[0.3, 0.5])))\n",
        "  '''\n",
        "  model.add(Dense(\n",
        "      hp.Choice('units', [64,32]),\n",
        "      activation='relu')\n",
        "  )\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "  '''\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias))\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 4e-4,5e-5,6e-6])),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LHW-KTFY6nQ",
        "outputId": "28d0db88-6286-46eb-bab0-229a3b51ab50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 20 Complete [00h 01m 23s]\n",
            "val_auc: 0.7838360071182251\n",
            "\n",
            "Best val_auc So Far: 0.8807364702224731\n",
            "Total elapsed time: 01h 07m 35s\n"
          ]
        }
      ],
      "source": [
        "hp = keras_tuner.HyperParameters()\n",
        "class MyTuner(keras_tuner.tuners.RandomSearch):\n",
        "  def run_trial(self, trial, *args, **kwargs):\n",
        "\n",
        "    kwargs['batch_size'] = trial.hyperparameters.Choice('batch_size',\n",
        "                                         values=[4,8,16,32],\n",
        "                                         default=16)\n",
        "    return super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
        "\n",
        "\n",
        "tuner = MyTuner(\n",
        "    tuner_make_model,\n",
        "    max_trials=20,\n",
        "    # Do not resume the previous search in the same directory.\n",
        "    overwrite=True,\n",
        "    # Pick the best model based on validation accuracy\n",
        "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
        "    # Set a directory to store the intermediate results.\n",
        "    directory=\"/tuner\",\n",
        "    )\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=50, validation_split=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gc8obMIMea4",
        "outputId": "577083e0-8a74-42c9-c0d8-b54257176f0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters at 0x7e54c7f211e0>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrn-gAQUSsBV",
        "outputId": "e1c0f67d-0e38-4e5a-ea73-9924d72a3ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 1s 6ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       710\n",
            "           1       0.72      0.62      0.67       290\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.79      0.76      0.77      1000\n",
            "weighted avg       0.81      0.82      0.81      1000\n",
            "\n",
            "AUC Score: 0.8709422049538611\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "\n",
        "best_model = tuner.get_best_models()[0]\n",
        "# Evaluate the model on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print performance metrics\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDBdF7QkLGag"
      },
      "source": [
        "# BERT - using CLS embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "a4228b7fe78b485fba61f2cf750e697e",
            "691043100e434396aa0b189a4519e8e3",
            "613cca74045246bface5bc65234e8058",
            "f94a5130c0034e3b9c26a18fcf3adb95",
            "edc224522f444cb9a6600730aa6ee887",
            "ff3f542935c0412e915f488e4b3b15bf",
            "3e584691f5d74b928230ed45dedd9dd1",
            "5224feb03e664f0ab4e5390a498a656c",
            "5d556af4e189459bacb36b1ad6728a13",
            "974fbb54b0ca4b999cdc355b4411cae5",
            "cd24e3b40af1484cb40b03312c57da31",
            "bc5c3b9e152142649f95ed595c57ff33",
            "5ae2cb39327d409e9005e7a0d6ea617f",
            "e004276eaaaf4943bdd411ecaf43d529",
            "732e415e0cee44b68904ca4854673021",
            "93b422e7db1c4acdb1b1ea3afc4b61ed",
            "09980d352bcf4d21a308033c61fe5c2d",
            "b89a3b5281bd412386345ac062790f48",
            "a72a385a1827478ab1a70933598c1601",
            "b431d9b05f174c2f92a7fdfd4149c601",
            "3f044c8f32dd47b181920dce9135e13a",
            "5840dc03b27c4fe2b19223e09a8a0546",
            "ad233a1e243640a9b79646dfc7ac1211",
            "4398d6f0d4864739849a0b3d95033ec9",
            "0796c2c212d54ccc9e848e9ff3571a5c",
            "51a9d988670d4651ab2a946aeb05b2a4",
            "e4b69d89c80c4a2181bfad42ebb97d59",
            "380956e8f1f148be9e97a0cc5d75482e",
            "fe08731cbc564fcca8ed73ae091586ac",
            "738bba3ebe404537b385362f78d7e406",
            "f3b4b6337ae44db0b164ea60d7ed2411",
            "9281b17453d649e2bacd290923615d5e",
            "a35a7079c82845b48b06ccd0d3b7dd94",
            "1f133239855042639d728bcafc3bccea",
            "82e5ceafc0114e7cbdc7e0f7fd2ca336",
            "0ed9c46b6c1f4786ab5e69a5d103fd31",
            "3ccc00adb4be4743852b7a0c183e0060",
            "b0014a4658d94c489ab46039292804b8",
            "aa429cd87dd442b0aac024c33af2129c",
            "0c8a7f4101a54c49addc7cd962d85e82",
            "7294539b9bb147df8a6dfb0aa04d46f5",
            "6634b0b49fe04c339fbe68c2c8befd5b",
            "d1a15c832ef942f3826c3505d033d5db",
            "50ab56fb09104b18abc3b70cbbe5921e",
            "382505f254cb4b64bbcd3bf7d280aab8",
            "0d6b7adc761e4453aab722bb0c572ea0",
            "e6ffbb32c00f4ebb9efc30027b180af9",
            "600cdae8f12b4a158f1b13c812c246b7",
            "630647fc03bf49eaa8f17899f78d8af1",
            "21dfd979d7bd487ea0b166ee368b5b53",
            "854b4be29d2349fab809fd3275454a7e",
            "7ed9fbe5a76747e9ae84d0b6bd098df1",
            "bf9ed3c0ca594fd3ae3c2e982950445d",
            "2262221614f540348ab294a9684e832e",
            "0b0936e874f94f919497aa828f218537"
          ]
        },
        "id": "uF0vGnv-Zhof",
        "outputId": "23fbc76f-a9d5-45e8-afe8-bc709649a76b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4228b7fe78b485fba61f2cf750e697e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc5c3b9e152142649f95ed595c57ff33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad233a1e243640a9b79646dfc7ac1211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f133239855042639d728bcafc3bccea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "382505f254cb4b64bbcd3bf7d280aab8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [46:06<00:00,  2.77s/it]\n",
            "100%|██████████| 1000/1000 [44:26<00:00,  2.67s/it]\n",
            "100%|██████████| 1000/1000 [45:37<00:00,  2.74s/it]\n",
            "100%|██████████| 1000/1000 [44:59<00:00,  2.70s/it]\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer,BertModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def embed_text_bert(df, max_length=512):\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    encoded_texts = []\n",
        "    for text in tqdm(df['cleaned_comment']):\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt',\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length'\n",
        "            )\n",
        "        with torch.no_grad():\n",
        "          outputs = model(**inputs)\n",
        "        sentence_embedding = outputs[0][:,0,:].numpy()\n",
        "        encoded_texts.append(sentence_embedding)\n",
        "        del inputs, outputs\n",
        "        gc.collect()\n",
        "    return encoded_texts\n",
        "#bert_sentence_embeddings = np.array(embed_text_bert(grouped_comments_labeled[:1000]))\n",
        "bert_sentence_embeddings_all = []\n",
        "for i in range(0, 5000, 1000):\n",
        "  bert_sentence_embeddings = np.array(embed_text_bert(grouped_comments_labeled[i:i+1000]))\n",
        "  np.savez_compressed(f'lemma_bert_sentence_embeddings_{i}.npz',bert_sentence_embeddings)\n",
        "  bert_sentence_embeddings_all.append(bert_sentence_embeddings)\n",
        "#NOTE: Due to RAM limitation we've splitted the data, embedded each split and then concatenated them all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM_E0HfAk6Zd"
      },
      "outputs": [],
      "source": [
        "#bert_sentence_embeddings = np.load('bert_sentence_embeddings_0.npz')['arr_0']\n",
        "bert_embeddings_all = np.concatenate((bert_sentence_embeddings_all))\n",
        "bert_embeddings_all = bert_embeddings_all.squeeze()\n",
        "np.savez_compressed('lemma_bert_embeddings_all.npz',bert_embeddings_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tUxqU0A-fo0"
      },
      "outputs": [],
      "source": [
        "bert_embeddings_all=np.load('lemma_bert_embeddings_all.npz')['arr_0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b60NrBaTZhmM"
      },
      "outputs": [],
      "source": [
        "input_matrix = np.concatenate((author_subreddit_matrix_reduced,bert_embeddings_all),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2_GUGCcZhjh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "combined_data = list(zip(grouped_comments_labeled.values, input_matrix))\n",
        "\n",
        "# Split the combined data into training and testing sets\n",
        "train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Unzip the training and testing sets into separate DataFrames and TF-IDF matrices\n",
        "train_df, X_train = zip(*train_data)\n",
        "train_df = pd.DataFrame(train_df, columns=grouped_comments_labeled.columns)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_df['gender'])\n",
        "\n",
        "test_df, X_test = zip(*test_data)\n",
        "test_df = pd.DataFrame(test_df, columns=grouped_comments_labeled.columns)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(test_df['gender'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX2-AyQMZhgg"
      },
      "outputs": [],
      "source": [
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kat1-fUdZhdU",
        "outputId": "82892a57-eacf-4ed7-a442-0bc76701c6ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 128)               201600    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 209921 (820.00 KB)\n",
            "Trainable params: 209921 (820.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras_tuner\n",
        "\n",
        "from keras import regularizers\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam,SGD,RMSprop\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class PrintMetricsCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\"\\nEpoch:\", epoch+1)\n",
        "        print(\"Train Loss:\", logs['loss'])\n",
        "        print(\"Val Loss:\", logs['val_loss'])\n",
        "        print(\"Train Accuracy:\", logs['accuracy'])\n",
        "        print(\"Val Accuracy:\", logs['val_accuracy'])\n",
        "\n",
        "METRICS = [\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "# Balancing class weights\n",
        "initial_bias = np.log([sum(y_train)/(len(y_train)-sum(y_train))])\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_matrix.shape[1],)),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid',bias_initializer=keras.initializers.Constant(initial_bias))\n",
        "    ])\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(5e-5), loss='binary_crossentropy', metrics=METRICS)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei-2jEl8ZhU0",
        "outputId": "d1e17e9d-b0aa-4780-9ea7-2080949a682d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "444/450 [============================>.] - ETA: 0s - loss: 1.4986 - accuracy: 0.6116 - auc: 0.5754 - prc: 0.3520\n",
            "Epoch: 1\n",
            "Train Loss: 1.496911883354187\n",
            "Val Loss: 1.3369563817977905\n",
            "Train Accuracy: 0.6115000247955322\n",
            "Val Accuracy: 0.7599999904632568\n",
            "450/450 [==============================] - 4s 6ms/step - loss: 1.4969 - accuracy: 0.6115 - auc: 0.5761 - prc: 0.3532 - val_loss: 1.3370 - val_accuracy: 0.7600 - val_auc: 0.7237 - val_prc: 0.4733\n",
            "Epoch 2/50\n",
            "440/450 [============================>.] - ETA: 0s - loss: 1.3145 - accuracy: 0.6011 - auc: 0.6016 - prc: 0.3485\n",
            "Epoch: 2\n",
            "Train Loss: 1.3124293088912964\n",
            "Val Loss: 1.2188278436660767\n",
            "Train Accuracy: 0.602222204208374\n",
            "Val Accuracy: 0.7049999833106995\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 1.3124 - accuracy: 0.6022 - auc: 0.6065 - prc: 0.3565 - val_loss: 1.2188 - val_accuracy: 0.7050 - val_auc: 0.7672 - val_prc: 0.5210\n",
            "Epoch 3/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.1811 - accuracy: 0.6433 - auc: 0.6603 - prc: 0.4362\n",
            "Epoch: 3\n",
            "Train Loss: 1.181054949760437\n",
            "Val Loss: 1.0974340438842773\n",
            "Train Accuracy: 0.6433333158493042\n",
            "Val Accuracy: 0.7225000262260437\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 1.1811 - accuracy: 0.6433 - auc: 0.6603 - prc: 0.4362 - val_loss: 1.0974 - val_accuracy: 0.7225 - val_auc: 0.7840 - val_prc: 0.5559\n",
            "Epoch 4/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 1.0719 - accuracy: 0.6687 - auc: 0.7152 - prc: 0.4749\n",
            "Epoch: 4\n",
            "Train Loss: 1.0702344179153442\n",
            "Val Loss: 1.0095853805541992\n",
            "Train Accuracy: 0.6694444417953491\n",
            "Val Accuracy: 0.7024999856948853\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 1.0702 - accuracy: 0.6694 - auc: 0.7161 - prc: 0.4738 - val_loss: 1.0096 - val_accuracy: 0.7025 - val_auc: 0.7926 - val_prc: 0.5674\n",
            "Epoch 5/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.9845 - accuracy: 0.6995 - auc: 0.7497 - prc: 0.5135\n",
            "Epoch: 5\n",
            "Train Loss: 0.9830226898193359\n",
            "Val Loss: 0.9264652729034424\n",
            "Train Accuracy: 0.6997222304344177\n",
            "Val Accuracy: 0.7250000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.9830 - accuracy: 0.6997 - auc: 0.7505 - prc: 0.5178 - val_loss: 0.9265 - val_accuracy: 0.7250 - val_auc: 0.7917 - val_prc: 0.5641\n",
            "Epoch 6/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.9151 - accuracy: 0.7096 - auc: 0.7704 - prc: 0.5662\n",
            "Epoch: 6\n",
            "Train Loss: 0.9126791954040527\n",
            "Val Loss: 0.8485157489776611\n",
            "Train Accuracy: 0.7111111283302307\n",
            "Val Accuracy: 0.7574999928474426\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.9127 - accuracy: 0.7111 - auc: 0.7722 - prc: 0.5673 - val_loss: 0.8485 - val_accuracy: 0.7575 - val_auc: 0.7969 - val_prc: 0.5719\n",
            "Epoch 7/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.8562 - accuracy: 0.7355 - auc: 0.7915 - prc: 0.5746\n",
            "Epoch: 7\n",
            "Train Loss: 0.8554145693778992\n",
            "Val Loss: 0.8150140643119812\n",
            "Train Accuracy: 0.7361111044883728\n",
            "Val Accuracy: 0.7400000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.8554 - accuracy: 0.7361 - auc: 0.7921 - prc: 0.5750 - val_loss: 0.8150 - val_accuracy: 0.7400 - val_auc: 0.7993 - val_prc: 0.5782\n",
            "Epoch 8/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.8181 - accuracy: 0.7317 - auc: 0.7943 - prc: 0.5978\n",
            "Epoch: 8\n",
            "Train Loss: 0.8179327249526978\n",
            "Val Loss: 0.8209558725357056\n",
            "Train Accuracy: 0.7319444417953491\n",
            "Val Accuracy: 0.6899999976158142\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.8179 - accuracy: 0.7319 - auc: 0.7950 - prc: 0.5995 - val_loss: 0.8210 - val_accuracy: 0.6900 - val_auc: 0.8029 - val_prc: 0.5872\n",
            "Epoch 9/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.7734 - accuracy: 0.7458 - auc: 0.8134 - prc: 0.6220\n",
            "Epoch: 9\n",
            "Train Loss: 0.7734455466270447\n",
            "Val Loss: 0.7519436478614807\n",
            "Train Accuracy: 0.7458333373069763\n",
            "Val Accuracy: 0.7325000166893005\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.7734 - accuracy: 0.7458 - auc: 0.8134 - prc: 0.6220 - val_loss: 0.7519 - val_accuracy: 0.7325 - val_auc: 0.8074 - val_prc: 0.5913\n",
            "Epoch 10/50\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.7412 - accuracy: 0.7528 - auc: 0.8244 - prc: 0.6283\n",
            "Epoch: 10\n",
            "Train Loss: 0.7424336075782776\n",
            "Val Loss: 0.7157560586929321\n",
            "Train Accuracy: 0.7508333325386047\n",
            "Val Accuracy: 0.7524999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.7424 - accuracy: 0.7508 - auc: 0.8235 - prc: 0.6279 - val_loss: 0.7158 - val_accuracy: 0.7525 - val_auc: 0.8102 - val_prc: 0.5989\n",
            "Epoch 11/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.7199 - accuracy: 0.7520 - auc: 0.8254 - prc: 0.6330\n",
            "Epoch: 11\n",
            "Train Loss: 0.7195585370063782\n",
            "Val Loss: 0.7179960608482361\n",
            "Train Accuracy: 0.7513889074325562\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.7196 - accuracy: 0.7514 - auc: 0.8251 - prc: 0.6324 - val_loss: 0.7180 - val_accuracy: 0.7275 - val_auc: 0.8132 - val_prc: 0.6084\n",
            "Epoch 12/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.6994 - accuracy: 0.7612 - auc: 0.8289 - prc: 0.6442\n",
            "Epoch: 12\n",
            "Train Loss: 0.6991351842880249\n",
            "Val Loss: 0.6996890306472778\n",
            "Train Accuracy: 0.761388897895813\n",
            "Val Accuracy: 0.7425000071525574\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6991 - accuracy: 0.7614 - auc: 0.8292 - prc: 0.6454 - val_loss: 0.6997 - val_accuracy: 0.7425 - val_auc: 0.8146 - val_prc: 0.6144\n",
            "Epoch 13/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6716 - accuracy: 0.7583 - auc: 0.8416 - prc: 0.6584\n",
            "Epoch: 13\n",
            "Train Loss: 0.6715859770774841\n",
            "Val Loss: 0.6670165061950684\n",
            "Train Accuracy: 0.7583333253860474\n",
            "Val Accuracy: 0.7524999976158142\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.6716 - accuracy: 0.7583 - auc: 0.8416 - prc: 0.6584 - val_loss: 0.6670 - val_accuracy: 0.7525 - val_auc: 0.8162 - val_prc: 0.6204\n",
            "Epoch 14/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.7806 - auc: 0.8462 - prc: 0.6609\n",
            "Epoch: 14\n",
            "Train Loss: 0.6530677080154419\n",
            "Val Loss: 0.6926056146621704\n",
            "Train Accuracy: 0.7805555462837219\n",
            "Val Accuracy: 0.7200000286102295\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.6531 - accuracy: 0.7806 - auc: 0.8462 - prc: 0.6609 - val_loss: 0.6926 - val_accuracy: 0.7200 - val_auc: 0.8205 - val_prc: 0.6311\n",
            "Epoch 15/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.6345 - accuracy: 0.7764 - auc: 0.8499 - prc: 0.6802\n",
            "Epoch: 15\n",
            "Train Loss: 0.6345125436782837\n",
            "Val Loss: 0.6188483238220215\n",
            "Train Accuracy: 0.7763888835906982\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.6345 - accuracy: 0.7764 - auc: 0.8499 - prc: 0.6802 - val_loss: 0.6188 - val_accuracy: 0.7725 - val_auc: 0.8222 - val_prc: 0.6347\n",
            "Epoch 16/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.6186 - accuracy: 0.7760 - auc: 0.8535 - prc: 0.6924\n",
            "Epoch: 16\n",
            "Train Loss: 0.6182916760444641\n",
            "Val Loss: 0.6683648824691772\n",
            "Train Accuracy: 0.7755555510520935\n",
            "Val Accuracy: 0.7250000238418579\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.6183 - accuracy: 0.7756 - auc: 0.8536 - prc: 0.6933 - val_loss: 0.6684 - val_accuracy: 0.7250 - val_auc: 0.8231 - val_prc: 0.6415\n",
            "Epoch 17/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5995 - accuracy: 0.7860 - auc: 0.8614 - prc: 0.7112\n",
            "Epoch: 17\n",
            "Train Loss: 0.599556565284729\n",
            "Val Loss: 0.6426601409912109\n",
            "Train Accuracy: 0.7844444513320923\n",
            "Val Accuracy: 0.7350000143051147\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5996 - accuracy: 0.7844 - auc: 0.8611 - prc: 0.7092 - val_loss: 0.6427 - val_accuracy: 0.7350 - val_auc: 0.8237 - val_prc: 0.6471\n",
            "Epoch 18/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.5917 - accuracy: 0.7855 - auc: 0.8594 - prc: 0.6959\n",
            "Epoch: 18\n",
            "Train Loss: 0.592880368232727\n",
            "Val Loss: 0.5742119550704956\n",
            "Train Accuracy: 0.784166693687439\n",
            "Val Accuracy: 0.7925000190734863\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.5929 - accuracy: 0.7842 - auc: 0.8583 - prc: 0.6951 - val_loss: 0.5742 - val_accuracy: 0.7925 - val_auc: 0.8255 - val_prc: 0.6496\n",
            "Epoch 19/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7894 - auc: 0.8692 - prc: 0.7210\n",
            "Epoch: 19\n",
            "Train Loss: 0.5718688368797302\n",
            "Val Loss: 0.5733170509338379\n",
            "Train Accuracy: 0.7894444465637207\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.5719 - accuracy: 0.7894 - auc: 0.8692 - prc: 0.7210 - val_loss: 0.5733 - val_accuracy: 0.7800 - val_auc: 0.8278 - val_prc: 0.6560\n",
            "Epoch 20/50\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7959 - auc: 0.8677 - prc: 0.7191\n",
            "Epoch: 20\n",
            "Train Loss: 0.5663121938705444\n",
            "Val Loss: 0.5662069916725159\n",
            "Train Accuracy: 0.7950000166893005\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5663 - accuracy: 0.7950 - auc: 0.8674 - prc: 0.7181 - val_loss: 0.5662 - val_accuracy: 0.7750 - val_auc: 0.8306 - val_prc: 0.6709\n",
            "Epoch 21/50\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.5531 - accuracy: 0.7952 - auc: 0.8717 - prc: 0.7336\n",
            "Epoch: 21\n",
            "Train Loss: 0.5526862144470215\n",
            "Val Loss: 0.6257542371749878\n",
            "Train Accuracy: 0.7950000166893005\n",
            "Val Accuracy: 0.7275000214576721\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5527 - accuracy: 0.7950 - auc: 0.8719 - prc: 0.7319 - val_loss: 0.6258 - val_accuracy: 0.7275 - val_auc: 0.8314 - val_prc: 0.6710\n",
            "Epoch 22/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.5400 - accuracy: 0.7992 - auc: 0.8769 - prc: 0.7395\n",
            "Epoch: 22\n",
            "Train Loss: 0.5415244698524475\n",
            "Val Loss: 0.582230806350708\n",
            "Train Accuracy: 0.7986111044883728\n",
            "Val Accuracy: 0.75\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.5415 - accuracy: 0.7986 - auc: 0.8762 - prc: 0.7381 - val_loss: 0.5822 - val_accuracy: 0.7500 - val_auc: 0.8322 - val_prc: 0.6762\n",
            "Epoch 23/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.5276 - accuracy: 0.7985 - auc: 0.8814 - prc: 0.7449\n",
            "Epoch: 23\n",
            "Train Loss: 0.5281130075454712\n",
            "Val Loss: 0.5623450875282288\n",
            "Train Accuracy: 0.7986111044883728\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.5281 - accuracy: 0.7986 - auc: 0.8810 - prc: 0.7445 - val_loss: 0.5623 - val_accuracy: 0.7750 - val_auc: 0.8311 - val_prc: 0.6739\n",
            "Epoch 24/50\n",
            "441/450 [============================>.] - ETA: 0s - loss: 0.5167 - accuracy: 0.8112 - auc: 0.8847 - prc: 0.7543\n",
            "Epoch: 24\n",
            "Train Loss: 0.5178064107894897\n",
            "Val Loss: 0.5259524583816528\n",
            "Train Accuracy: 0.8097222447395325\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.5178 - accuracy: 0.8097 - auc: 0.8839 - prc: 0.7524 - val_loss: 0.5260 - val_accuracy: 0.7850 - val_auc: 0.8331 - val_prc: 0.6795\n",
            "Epoch 25/50\n",
            "440/450 [============================>.] - ETA: 0s - loss: 0.5117 - accuracy: 0.8034 - auc: 0.8848 - prc: 0.7490\n",
            "Epoch: 25\n",
            "Train Loss: 0.5127312541007996\n",
            "Val Loss: 0.5571532249450684\n",
            "Train Accuracy: 0.8022222518920898\n",
            "Val Accuracy: 0.75\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.5127 - accuracy: 0.8022 - auc: 0.8848 - prc: 0.7480 - val_loss: 0.5572 - val_accuracy: 0.7500 - val_auc: 0.8324 - val_prc: 0.6794\n",
            "Epoch 26/50\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4989 - accuracy: 0.8122 - auc: 0.8903 - prc: 0.7651\n",
            "Epoch: 26\n",
            "Train Loss: 0.4981675148010254\n",
            "Val Loss: 0.5207799077033997\n",
            "Train Accuracy: 0.8127777576446533\n",
            "Val Accuracy: 0.7674999833106995\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4982 - accuracy: 0.8128 - auc: 0.8905 - prc: 0.7647 - val_loss: 0.5208 - val_accuracy: 0.7675 - val_auc: 0.8310 - val_prc: 0.6827\n",
            "Epoch 27/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4866 - accuracy: 0.8145 - auc: 0.8959 - prc: 0.7702\n",
            "Epoch: 27\n",
            "Train Loss: 0.4886454939842224\n",
            "Val Loss: 0.533815860748291\n",
            "Train Accuracy: 0.8130555748939514\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4886 - accuracy: 0.8131 - auc: 0.8944 - prc: 0.7678 - val_loss: 0.5338 - val_accuracy: 0.7800 - val_auc: 0.8346 - val_prc: 0.6870\n",
            "Epoch 28/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.8154 - auc: 0.8950 - prc: 0.7751\n",
            "Epoch: 28\n",
            "Train Loss: 0.48412930965423584\n",
            "Val Loss: 0.5432058572769165\n",
            "Train Accuracy: 0.8152777552604675\n",
            "Val Accuracy: 0.7549999952316284\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4841 - accuracy: 0.8153 - auc: 0.8949 - prc: 0.7748 - val_loss: 0.5432 - val_accuracy: 0.7550 - val_auc: 0.8362 - val_prc: 0.6899\n",
            "Epoch 29/50\n",
            "449/450 [============================>.] - ETA: 0s - loss: 0.4739 - accuracy: 0.8252 - auc: 0.8992 - prc: 0.7738\n",
            "Epoch: 29\n",
            "Train Loss: 0.47415435314178467\n",
            "Val Loss: 0.5268149375915527\n",
            "Train Accuracy: 0.824999988079071\n",
            "Val Accuracy: 0.7574999928474426\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.4742 - accuracy: 0.8250 - auc: 0.8993 - prc: 0.7741 - val_loss: 0.5268 - val_accuracy: 0.7575 - val_auc: 0.8363 - val_prc: 0.6939\n",
            "Epoch 30/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4690 - accuracy: 0.8185 - auc: 0.9002 - prc: 0.7777\n",
            "Epoch: 30\n",
            "Train Loss: 0.4719485342502594\n",
            "Val Loss: 0.51102614402771\n",
            "Train Accuracy: 0.8174999952316284\n",
            "Val Accuracy: 0.7774999737739563\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4719 - accuracy: 0.8175 - auc: 0.8988 - prc: 0.7737 - val_loss: 0.5110 - val_accuracy: 0.7775 - val_auc: 0.8369 - val_prc: 0.6943\n",
            "Epoch 31/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.4627 - accuracy: 0.8183 - auc: 0.9028 - prc: 0.7857\n",
            "Epoch: 31\n",
            "Train Loss: 0.46194806694984436\n",
            "Val Loss: 0.49231505393981934\n",
            "Train Accuracy: 0.8188889026641846\n",
            "Val Accuracy: 0.7850000262260437\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4619 - accuracy: 0.8189 - auc: 0.9028 - prc: 0.7847 - val_loss: 0.4923 - val_accuracy: 0.7850 - val_auc: 0.8383 - val_prc: 0.6995\n",
            "Epoch 32/50\n",
            "444/450 [============================>.] - ETA: 0s - loss: 0.4477 - accuracy: 0.8308 - auc: 0.9090 - prc: 0.7884\n",
            "Epoch: 32\n",
            "Train Loss: 0.44996586441993713\n",
            "Val Loss: 0.5574610233306885\n",
            "Train Accuracy: 0.8291666507720947\n",
            "Val Accuracy: 0.7475000023841858\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4500 - accuracy: 0.8292 - auc: 0.9077 - prc: 0.7861 - val_loss: 0.5575 - val_accuracy: 0.7475 - val_auc: 0.8372 - val_prc: 0.6985\n",
            "Epoch 33/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4382 - accuracy: 0.8369 - auc: 0.9123 - prc: 0.8058\n",
            "Epoch: 33\n",
            "Train Loss: 0.4392778277397156\n",
            "Val Loss: 0.47379070520401\n",
            "Train Accuracy: 0.836388885974884\n",
            "Val Accuracy: 0.7950000166893005\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4393 - accuracy: 0.8364 - auc: 0.9120 - prc: 0.8066 - val_loss: 0.4738 - val_accuracy: 0.7950 - val_auc: 0.8367 - val_prc: 0.6983\n",
            "Epoch 34/50\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.4396 - accuracy: 0.8361 - auc: 0.9113 - prc: 0.7999\n",
            "Epoch: 34\n",
            "Train Loss: 0.4396076798439026\n",
            "Val Loss: 0.47883152961730957\n",
            "Train Accuracy: 0.8361111283302307\n",
            "Val Accuracy: 0.7975000143051147\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.4396 - accuracy: 0.8361 - auc: 0.9113 - prc: 0.7999 - val_loss: 0.4788 - val_accuracy: 0.7975 - val_auc: 0.8381 - val_prc: 0.6985\n",
            "Epoch 35/50\n",
            "442/450 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.8354 - auc: 0.9157 - prc: 0.8181\n",
            "Epoch: 35\n",
            "Train Loss: 0.42821258306503296\n",
            "Val Loss: 0.5096930861473083\n",
            "Train Accuracy: 0.8361111283302307\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.4282 - accuracy: 0.8361 - auc: 0.9163 - prc: 0.8207 - val_loss: 0.5097 - val_accuracy: 0.7750 - val_auc: 0.8390 - val_prc: 0.7050\n",
            "Epoch 36/50\n",
            "446/450 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8501 - auc: 0.9190 - prc: 0.8096\n",
            "Epoch: 36\n",
            "Train Loss: 0.4233211278915405\n",
            "Val Loss: 0.47268253564834595\n",
            "Train Accuracy: 0.8502777814865112\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.4233 - accuracy: 0.8503 - auc: 0.9180 - prc: 0.8083 - val_loss: 0.4727 - val_accuracy: 0.8075 - val_auc: 0.8388 - val_prc: 0.7074\n",
            "Epoch 37/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8381 - auc: 0.9175 - prc: 0.8045\n",
            "Epoch: 37\n",
            "Train Loss: 0.42161065340042114\n",
            "Val Loss: 0.515656054019928\n",
            "Train Accuracy: 0.8374999761581421\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4216 - accuracy: 0.8375 - auc: 0.9177 - prc: 0.8059 - val_loss: 0.5157 - val_accuracy: 0.7750 - val_auc: 0.8374 - val_prc: 0.7038\n",
            "Epoch 38/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8431 - auc: 0.9230 - prc: 0.8315\n",
            "Epoch: 38\n",
            "Train Loss: 0.4087640047073364\n",
            "Val Loss: 0.4601845145225525\n",
            "Train Accuracy: 0.8430555462837219\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.4088 - accuracy: 0.8431 - auc: 0.9230 - prc: 0.8312 - val_loss: 0.4602 - val_accuracy: 0.8025 - val_auc: 0.8385 - val_prc: 0.7049\n",
            "Epoch 39/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8514 - auc: 0.9276 - prc: 0.8254\n",
            "Epoch: 39\n",
            "Train Loss: 0.4008866250514984\n",
            "Val Loss: 0.522490918636322\n",
            "Train Accuracy: 0.8516666889190674\n",
            "Val Accuracy: 0.7699999809265137\n",
            "450/450 [==============================] - 3s 8ms/step - loss: 0.4009 - accuracy: 0.8517 - auc: 0.9277 - prc: 0.8261 - val_loss: 0.5225 - val_accuracy: 0.7700 - val_auc: 0.8385 - val_prc: 0.7103\n",
            "Epoch 40/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3958 - accuracy: 0.8560 - auc: 0.9286 - prc: 0.8319\n",
            "Epoch: 40\n",
            "Train Loss: 0.39596906304359436\n",
            "Val Loss: 0.5139073133468628\n",
            "Train Accuracy: 0.8552777767181396\n",
            "Val Accuracy: 0.7749999761581421\n",
            "450/450 [==============================] - 4s 8ms/step - loss: 0.3960 - accuracy: 0.8553 - auc: 0.9285 - prc: 0.8315 - val_loss: 0.5139 - val_accuracy: 0.7750 - val_auc: 0.8414 - val_prc: 0.7162\n",
            "Epoch 41/50\n",
            "447/450 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8588 - auc: 0.9303 - prc: 0.8356\n",
            "Epoch: 41\n",
            "Train Loss: 0.3925936222076416\n",
            "Val Loss: 0.4372369349002838\n",
            "Train Accuracy: 0.8594444394111633\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3926 - accuracy: 0.8594 - auc: 0.9299 - prc: 0.8354 - val_loss: 0.4372 - val_accuracy: 0.8150 - val_auc: 0.8413 - val_prc: 0.7095\n",
            "Epoch 42/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8571 - auc: 0.9302 - prc: 0.8418\n",
            "Epoch: 42\n",
            "Train Loss: 0.3909907042980194\n",
            "Val Loss: 0.5010738968849182\n",
            "Train Accuracy: 0.8566666841506958\n",
            "Val Accuracy: 0.7825000286102295\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3910 - accuracy: 0.8567 - auc: 0.9295 - prc: 0.8383 - val_loss: 0.5011 - val_accuracy: 0.7825 - val_auc: 0.8426 - val_prc: 0.7180\n",
            "Epoch 43/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8612 - auc: 0.9345 - prc: 0.8466\n",
            "Epoch: 43\n",
            "Train Loss: 0.38220351934432983\n",
            "Val Loss: 0.505147397518158\n",
            "Train Accuracy: 0.8611111044883728\n",
            "Val Accuracy: 0.7724999785423279\n",
            "450/450 [==============================] - 2s 5ms/step - loss: 0.3822 - accuracy: 0.8611 - auc: 0.9340 - prc: 0.8439 - val_loss: 0.5051 - val_accuracy: 0.7725 - val_auc: 0.8438 - val_prc: 0.7208\n",
            "Epoch 44/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8613 - auc: 0.9335 - prc: 0.8461\n",
            "Epoch: 44\n",
            "Train Loss: 0.3811349868774414\n",
            "Val Loss: 0.5029280185699463\n",
            "Train Accuracy: 0.8613888621330261\n",
            "Val Accuracy: 0.7875000238418579\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3811 - accuracy: 0.8614 - auc: 0.9336 - prc: 0.8465 - val_loss: 0.5029 - val_accuracy: 0.7875 - val_auc: 0.8430 - val_prc: 0.7241\n",
            "Epoch 45/50\n",
            "439/450 [============================>.] - ETA: 0s - loss: 0.3782 - accuracy: 0.8647 - auc: 0.9355 - prc: 0.8501\n",
            "Epoch: 45\n",
            "Train Loss: 0.37719354033470154\n",
            "Val Loss: 0.4765181243419647\n",
            "Train Accuracy: 0.8650000095367432\n",
            "Val Accuracy: 0.8075000047683716\n",
            "450/450 [==============================] - 4s 9ms/step - loss: 0.3772 - accuracy: 0.8650 - auc: 0.9357 - prc: 0.8490 - val_loss: 0.4765 - val_accuracy: 0.8075 - val_auc: 0.8397 - val_prc: 0.7135\n",
            "Epoch 46/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8654 - auc: 0.9365 - prc: 0.8476\n",
            "Epoch: 46\n",
            "Train Loss: 0.3730860650539398\n",
            "Val Loss: 0.4704655110836029\n",
            "Train Accuracy: 0.8661110997200012\n",
            "Val Accuracy: 0.8025000095367432\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3731 - accuracy: 0.8661 - auc: 0.9373 - prc: 0.8494 - val_loss: 0.4705 - val_accuracy: 0.8025 - val_auc: 0.8438 - val_prc: 0.7227\n",
            "Epoch 47/50\n",
            "445/450 [============================>.] - ETA: 0s - loss: 0.3601 - accuracy: 0.8702 - auc: 0.9421 - prc: 0.8651\n",
            "Epoch: 47\n",
            "Train Loss: 0.36046838760375977\n",
            "Val Loss: 0.4634529948234558\n",
            "Train Accuracy: 0.8697222471237183\n",
            "Val Accuracy: 0.800000011920929\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3605 - accuracy: 0.8697 - auc: 0.9417 - prc: 0.8633 - val_loss: 0.4635 - val_accuracy: 0.8000 - val_auc: 0.8439 - val_prc: 0.7282\n",
            "Epoch 48/50\n",
            "443/450 [============================>.] - ETA: 0s - loss: 0.3561 - accuracy: 0.8705 - auc: 0.9443 - prc: 0.8642\n",
            "Epoch: 48\n",
            "Train Loss: 0.35522937774658203\n",
            "Val Loss: 0.43453511595726013\n",
            "Train Accuracy: 0.8702777624130249\n",
            "Val Accuracy: 0.8149999976158142\n",
            "450/450 [==============================] - 3s 6ms/step - loss: 0.3552 - accuracy: 0.8703 - auc: 0.9446 - prc: 0.8649 - val_loss: 0.4345 - val_accuracy: 0.8150 - val_auc: 0.8442 - val_prc: 0.7265\n",
            "Epoch 49/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.8761 - auc: 0.9438 - prc: 0.8617\n",
            "Epoch: 49\n",
            "Train Loss: 0.35506802797317505\n",
            "Val Loss: 0.4969359338283539\n",
            "Train Accuracy: 0.8761110901832581\n",
            "Val Accuracy: 0.7799999713897705\n",
            "450/450 [==============================] - 3s 7ms/step - loss: 0.3551 - accuracy: 0.8761 - auc: 0.9440 - prc: 0.8628 - val_loss: 0.4969 - val_accuracy: 0.7800 - val_auc: 0.8442 - val_prc: 0.7331\n",
            "Epoch 50/50\n",
            "448/450 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8789 - auc: 0.9482 - prc: 0.8768\n",
            "Epoch: 50\n",
            "Train Loss: 0.34470295906066895\n",
            "Val Loss: 0.4360555410385132\n",
            "Train Accuracy: 0.878333330154419\n",
            "Val Accuracy: 0.824999988079071\n",
            "450/450 [==============================] - 4s 10ms/step - loss: 0.3447 - accuracy: 0.8783 - auc: 0.9479 - prc: 0.8761 - val_loss: 0.4361 - val_accuracy: 0.8250 - val_auc: 0.8407 - val_prc: 0.7293\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86       710\n",
            "           1       0.66      0.70      0.68       290\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.77      0.78      0.77      1000\n",
            "weighted avg       0.81      0.81      0.81      1000\n",
            "\n",
            "AUC Score: 0.861864983001457\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print_metrics_callback = PrintMetricsCallback()\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc',\n",
        "    verbose=1,\n",
        "    patience=20,\n",
        "    mode='max',\n",
        "    restore_best_weights=True\n",
        "    )\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50, batch_size=8,\n",
        "    validation_split=0.1,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[print_metrics_callback])\n",
        "\n",
        "# Evaluate the model on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print performance metrics\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "xxG3hzAaE1wQ",
        "outputId": "aecaf8d0-27da-4529-89e0-af24665a7d76"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p83WOLn0kQeI",
        "outputId": "86d7ff8a-3060-4512-c4db-b2a3e17dcbbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.8842 - auc: 0.9502 - prc: 0.8748\n",
            "Epoch: 1\n",
            "Train Loss: 0.34012481570243835\n",
            "Val Loss: 0.44680255651474\n",
            "Train Accuracy: 0.8841666579246521\n",
            "Val Accuracy: 0.8125\n",
            "225/225 [==============================] - 3s 9ms/step - loss: 0.3401 - accuracy: 0.8842 - auc: 0.9502 - prc: 0.8745 - val_loss: 0.4468 - val_accuracy: 0.8125 - val_auc: 0.8442 - val_prc: 0.7335\n",
            "Epoch 2/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8874 - auc: 0.9518 - prc: 0.8812\n",
            "Epoch: 2\n",
            "Train Loss: 0.3362373113632202\n",
            "Val Loss: 0.4661586284637451\n",
            "Train Accuracy: 0.8872222304344177\n",
            "Val Accuracy: 0.8100000023841858\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.3362 - accuracy: 0.8872 - auc: 0.9519 - prc: 0.8830 - val_loss: 0.4662 - val_accuracy: 0.8100 - val_auc: 0.8443 - val_prc: 0.7348\n",
            "Epoch 3/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3321 - accuracy: 0.8895 - auc: 0.9526 - prc: 0.8832\n",
            "Epoch: 3\n",
            "Train Loss: 0.33219030499458313\n",
            "Val Loss: 0.425979346036911\n",
            "Train Accuracy: 0.8897222280502319\n",
            "Val Accuracy: 0.8399999737739563\n",
            "225/225 [==============================] - 2s 9ms/step - loss: 0.3322 - accuracy: 0.8897 - auc: 0.9525 - prc: 0.8835 - val_loss: 0.4260 - val_accuracy: 0.8400 - val_auc: 0.8437 - val_prc: 0.7304\n",
            "Epoch 4/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3343 - accuracy: 0.8831 - auc: 0.9523 - prc: 0.8813\n",
            "Epoch: 4\n",
            "Train Loss: 0.3338237702846527\n",
            "Val Loss: 0.4194589853286743\n",
            "Train Accuracy: 0.8833333253860474\n",
            "Val Accuracy: 0.8424999713897705\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3338 - accuracy: 0.8833 - auc: 0.9524 - prc: 0.8809 - val_loss: 0.4195 - val_accuracy: 0.8425 - val_auc: 0.8425 - val_prc: 0.7292\n",
            "Epoch 5/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8887 - auc: 0.9532 - prc: 0.8846\n",
            "Epoch: 5\n",
            "Train Loss: 0.33123114705085754\n",
            "Val Loss: 0.44445422291755676\n",
            "Train Accuracy: 0.8886111378669739\n",
            "Val Accuracy: 0.824999988079071\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3312 - accuracy: 0.8886 - auc: 0.9532 - prc: 0.8847 - val_loss: 0.4445 - val_accuracy: 0.8250 - val_auc: 0.8445 - val_prc: 0.7354\n",
            "Epoch 6/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8961 - auc: 0.9527 - prc: 0.8838\n",
            "Epoch: 6\n",
            "Train Loss: 0.3306390941143036\n",
            "Val Loss: 0.4311204254627228\n",
            "Train Accuracy: 0.8961111307144165\n",
            "Val Accuracy: 0.824999988079071\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3306 - accuracy: 0.8961 - auc: 0.9527 - prc: 0.8838 - val_loss: 0.4311 - val_accuracy: 0.8250 - val_auc: 0.8454 - val_prc: 0.7309\n",
            "Epoch 7/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8898 - auc: 0.9567 - prc: 0.8953\n",
            "Epoch: 7\n",
            "Train Loss: 0.32274287939071655\n",
            "Val Loss: 0.42963799834251404\n",
            "Train Accuracy: 0.8894444704055786\n",
            "Val Accuracy: 0.824999988079071\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3227 - accuracy: 0.8894 - auc: 0.9565 - prc: 0.8949 - val_loss: 0.4296 - val_accuracy: 0.8250 - val_auc: 0.8467 - val_prc: 0.7318\n",
            "Epoch 8/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8951 - auc: 0.9565 - prc: 0.8906\n",
            "Epoch: 8\n",
            "Train Loss: 0.31964409351348877\n",
            "Val Loss: 0.46787557005882263\n",
            "Train Accuracy: 0.8955555558204651\n",
            "Val Accuracy: 0.8075000047683716\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3196 - accuracy: 0.8956 - auc: 0.9568 - prc: 0.8914 - val_loss: 0.4679 - val_accuracy: 0.8075 - val_auc: 0.8453 - val_prc: 0.7332\n",
            "Epoch 9/50\n",
            "218/225 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.8865 - auc: 0.9535 - prc: 0.8906\n",
            "Epoch: 9\n",
            "Train Loss: 0.327802836894989\n",
            "Val Loss: 0.497965008020401\n",
            "Train Accuracy: 0.8869444727897644\n",
            "Val Accuracy: 0.7774999737739563\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3278 - accuracy: 0.8869 - auc: 0.9539 - prc: 0.8920 - val_loss: 0.4980 - val_accuracy: 0.7775 - val_auc: 0.8468 - val_prc: 0.7408\n",
            "Epoch 10/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8909 - auc: 0.9578 - prc: 0.8966\n",
            "Epoch: 10\n",
            "Train Loss: 0.3187175393104553\n",
            "Val Loss: 0.44107916951179504\n",
            "Train Accuracy: 0.89083331823349\n",
            "Val Accuracy: 0.8299999833106995\n",
            "225/225 [==============================] - 2s 9ms/step - loss: 0.3187 - accuracy: 0.8908 - auc: 0.9577 - prc: 0.8950 - val_loss: 0.4411 - val_accuracy: 0.8300 - val_auc: 0.8432 - val_prc: 0.7359\n",
            "Epoch 11/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.8962 - auc: 0.9600 - prc: 0.8996\n",
            "Epoch: 11\n",
            "Train Loss: 0.3127063810825348\n",
            "Val Loss: 0.45124804973602295\n",
            "Train Accuracy: 0.8961111307144165\n",
            "Val Accuracy: 0.8224999904632568\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.3127 - accuracy: 0.8961 - auc: 0.9599 - prc: 0.8980 - val_loss: 0.4512 - val_accuracy: 0.8225 - val_auc: 0.8467 - val_prc: 0.7375\n",
            "Epoch 12/50\n",
            "219/225 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8978 - auc: 0.9579 - prc: 0.8922\n",
            "Epoch: 12\n",
            "Train Loss: 0.3164348304271698\n",
            "Val Loss: 0.45581185817718506\n",
            "Train Accuracy: 0.8980555534362793\n",
            "Val Accuracy: 0.8149999976158142\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 0.3164 - accuracy: 0.8981 - auc: 0.9577 - prc: 0.8939 - val_loss: 0.4558 - val_accuracy: 0.8150 - val_auc: 0.8452 - val_prc: 0.7387\n",
            "Epoch 13/50\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 0.3096 - accuracy: 0.8958 - auc: 0.9602 - prc: 0.9013\n",
            "Epoch: 13\n",
            "Train Loss: 0.30866676568984985\n",
            "Val Loss: 0.4385056793689728\n",
            "Train Accuracy: 0.8963888883590698\n",
            "Val Accuracy: 0.8274999856948853\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3087 - accuracy: 0.8964 - auc: 0.9604 - prc: 0.9006 - val_loss: 0.4385 - val_accuracy: 0.8275 - val_auc: 0.8443 - val_prc: 0.7368\n",
            "Epoch 14/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.9035 - auc: 0.9619 - prc: 0.9040\n",
            "Epoch: 14\n",
            "Train Loss: 0.30544304847717285\n",
            "Val Loss: 0.45111218094825745\n",
            "Train Accuracy: 0.9036111235618591\n",
            "Val Accuracy: 0.8224999904632568\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3054 - accuracy: 0.9036 - auc: 0.9619 - prc: 0.9039 - val_loss: 0.4511 - val_accuracy: 0.8225 - val_auc: 0.8447 - val_prc: 0.7326\n",
            "Epoch 15/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9096 - auc: 0.9655 - prc: 0.9113\n",
            "Epoch: 15\n",
            "Train Loss: 0.2931809723377228\n",
            "Val Loss: 0.45889005064964294\n",
            "Train Accuracy: 0.9097222089767456\n",
            "Val Accuracy: 0.8199999928474426\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2932 - accuracy: 0.9097 - auc: 0.9655 - prc: 0.9116 - val_loss: 0.4589 - val_accuracy: 0.8200 - val_auc: 0.8454 - val_prc: 0.7433\n",
            "Epoch 16/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.9018 - auc: 0.9638 - prc: 0.9077\n",
            "Epoch: 16\n",
            "Train Loss: 0.2983352541923523\n",
            "Val Loss: 0.4085095524787903\n",
            "Train Accuracy: 0.9019444584846497\n",
            "Val Accuracy: 0.8550000190734863\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2983 - accuracy: 0.9019 - auc: 0.9639 - prc: 0.9079 - val_loss: 0.4085 - val_accuracy: 0.8550 - val_auc: 0.8432 - val_prc: 0.7316\n",
            "Epoch 17/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8962 - auc: 0.9609 - prc: 0.8994\n",
            "Epoch: 17\n",
            "Train Loss: 0.30631259083747864\n",
            "Val Loss: 0.4439558684825897\n",
            "Train Accuracy: 0.8958333134651184\n",
            "Val Accuracy: 0.8224999904632568\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.3063 - accuracy: 0.8958 - auc: 0.9608 - prc: 0.8995 - val_loss: 0.4440 - val_accuracy: 0.8225 - val_auc: 0.8478 - val_prc: 0.7409\n",
            "Epoch 18/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.9076 - auc: 0.9657 - prc: 0.9093\n",
            "Epoch: 18\n",
            "Train Loss: 0.29410842061042786\n",
            "Val Loss: 0.43025219440460205\n",
            "Train Accuracy: 0.9080555438995361\n",
            "Val Accuracy: 0.8374999761581421\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2941 - accuracy: 0.9081 - auc: 0.9659 - prc: 0.9095 - val_loss: 0.4303 - val_accuracy: 0.8375 - val_auc: 0.8474 - val_prc: 0.7399\n",
            "Epoch 19/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.9011 - auc: 0.9637 - prc: 0.9067\n",
            "Epoch: 19\n",
            "Train Loss: 0.2959443926811218\n",
            "Val Loss: 0.4227292537689209\n",
            "Train Accuracy: 0.9019444584846497\n",
            "Val Accuracy: 0.8399999737739563\n",
            "225/225 [==============================] - 2s 9ms/step - loss: 0.2959 - accuracy: 0.9019 - auc: 0.9639 - prc: 0.9073 - val_loss: 0.4227 - val_accuracy: 0.8400 - val_auc: 0.8479 - val_prc: 0.7391\n",
            "Epoch 20/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9089 - auc: 0.9655 - prc: 0.9118\n",
            "Epoch: 20\n",
            "Train Loss: 0.2922857701778412\n",
            "Val Loss: 0.45054614543914795\n",
            "Train Accuracy: 0.9088888764381409\n",
            "Val Accuracy: 0.8125\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.2923 - accuracy: 0.9089 - auc: 0.9655 - prc: 0.9118 - val_loss: 0.4505 - val_accuracy: 0.8125 - val_auc: 0.8488 - val_prc: 0.7411\n",
            "Epoch 21/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9106 - auc: 0.9682 - prc: 0.9182\n",
            "Epoch: 21\n",
            "Train Loss: 0.28474459052085876\n",
            "Val Loss: 0.4224977493286133\n",
            "Train Accuracy: 0.9113888740539551\n",
            "Val Accuracy: 0.8525000214576721\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 0.2847 - accuracy: 0.9114 - auc: 0.9684 - prc: 0.9185 - val_loss: 0.4225 - val_accuracy: 0.8525 - val_auc: 0.8452 - val_prc: 0.7385\n",
            "Epoch 22/50\n",
            "219/225 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9135 - auc: 0.9666 - prc: 0.9113\n",
            "Epoch: 22\n",
            "Train Loss: 0.2870762050151825\n",
            "Val Loss: 0.4171603322029114\n",
            "Train Accuracy: 0.9133333563804626\n",
            "Val Accuracy: 0.8500000238418579\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2871 - accuracy: 0.9133 - auc: 0.9668 - prc: 0.9117 - val_loss: 0.4172 - val_accuracy: 0.8500 - val_auc: 0.8482 - val_prc: 0.7385\n",
            "Epoch 23/50\n",
            "219/225 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.9087 - auc: 0.9664 - prc: 0.9133\n",
            "Epoch: 23\n",
            "Train Loss: 0.28884461522102356\n",
            "Val Loss: 0.4275660216808319\n",
            "Train Accuracy: 0.9086111187934875\n",
            "Val Accuracy: 0.8450000286102295\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2888 - accuracy: 0.9086 - auc: 0.9667 - prc: 0.9140 - val_loss: 0.4276 - val_accuracy: 0.8450 - val_auc: 0.8472 - val_prc: 0.7460\n",
            "Epoch 24/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.9142 - auc: 0.9724 - prc: 0.9219\n",
            "Epoch: 24\n",
            "Train Loss: 0.27091342210769653\n",
            "Val Loss: 0.4368022680282593\n",
            "Train Accuracy: 0.9144444465637207\n",
            "Val Accuracy: 0.8374999761581421\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2709 - accuracy: 0.9144 - auc: 0.9719 - prc: 0.9222 - val_loss: 0.4368 - val_accuracy: 0.8375 - val_auc: 0.8477 - val_prc: 0.7438\n",
            "Epoch 25/50\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 0.2735 - accuracy: 0.9093 - auc: 0.9712 - prc: 0.9256\n",
            "Epoch: 25\n",
            "Train Loss: 0.2721841037273407\n",
            "Val Loss: 0.42654311656951904\n",
            "Train Accuracy: 0.9111111164093018\n",
            "Val Accuracy: 0.8424999713897705\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2722 - accuracy: 0.9111 - auc: 0.9718 - prc: 0.9271 - val_loss: 0.4265 - val_accuracy: 0.8425 - val_auc: 0.8468 - val_prc: 0.7394\n",
            "Epoch 26/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.9150 - auc: 0.9714 - prc: 0.9248\n",
            "Epoch: 26\n",
            "Train Loss: 0.27376440167427063\n",
            "Val Loss: 0.42706725001335144\n",
            "Train Accuracy: 0.9150000214576721\n",
            "Val Accuracy: 0.8450000286102295\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2738 - accuracy: 0.9150 - auc: 0.9714 - prc: 0.9247 - val_loss: 0.4271 - val_accuracy: 0.8450 - val_auc: 0.8466 - val_prc: 0.7426\n",
            "Epoch 27/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.9187 - auc: 0.9720 - prc: 0.9242\n",
            "Epoch: 27\n",
            "Train Loss: 0.2692379653453827\n",
            "Val Loss: 0.4342200756072998\n",
            "Train Accuracy: 0.9188888669013977\n",
            "Val Accuracy: 0.8424999713897705\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2692 - accuracy: 0.9189 - auc: 0.9720 - prc: 0.9248 - val_loss: 0.4342 - val_accuracy: 0.8425 - val_auc: 0.8466 - val_prc: 0.7476\n",
            "Epoch 28/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9238 - auc: 0.9747 - prc: 0.9364\n",
            "Epoch: 28\n",
            "Train Loss: 0.25980639457702637\n",
            "Val Loss: 0.4162704050540924\n",
            "Train Accuracy: 0.9233333468437195\n",
            "Val Accuracy: 0.8550000190734863\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 0.2598 - accuracy: 0.9233 - auc: 0.9748 - prc: 0.9365 - val_loss: 0.4163 - val_accuracy: 0.8550 - val_auc: 0.8466 - val_prc: 0.7451\n",
            "Epoch 29/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2727 - accuracy: 0.9154 - auc: 0.9711 - prc: 0.9260\n",
            "Epoch: 29\n",
            "Train Loss: 0.2718238830566406\n",
            "Val Loss: 0.4288102686405182\n",
            "Train Accuracy: 0.9158333539962769\n",
            "Val Accuracy: 0.8399999737739563\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.2718 - accuracy: 0.9158 - auc: 0.9713 - prc: 0.9259 - val_loss: 0.4288 - val_accuracy: 0.8400 - val_auc: 0.8485 - val_prc: 0.7461\n",
            "Epoch 30/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.9245 - auc: 0.9733 - prc: 0.9330\n",
            "Epoch: 30\n",
            "Train Loss: 0.2657105326652527\n",
            "Val Loss: 0.41803139448165894\n",
            "Train Accuracy: 0.9244444370269775\n",
            "Val Accuracy: 0.8525000214576721\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 0.2657 - accuracy: 0.9244 - auc: 0.9731 - prc: 0.9322 - val_loss: 0.4180 - val_accuracy: 0.8525 - val_auc: 0.8473 - val_prc: 0.7484\n",
            "Epoch 31/50\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 0.2600 - accuracy: 0.9271 - auc: 0.9747 - prc: 0.9304\n",
            "Epoch: 31\n",
            "Train Loss: 0.26022911071777344\n",
            "Val Loss: 0.4576123356819153\n",
            "Train Accuracy: 0.9275000095367432\n",
            "Val Accuracy: 0.8174999952316284\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2602 - accuracy: 0.9275 - auc: 0.9748 - prc: 0.9295 - val_loss: 0.4576 - val_accuracy: 0.8175 - val_auc: 0.8476 - val_prc: 0.7486\n",
            "Epoch 32/50\n",
            "219/225 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.9224 - auc: 0.9771 - prc: 0.9336\n",
            "Epoch: 32\n",
            "Train Loss: 0.2555353343486786\n",
            "Val Loss: 0.46323296427726746\n",
            "Train Accuracy: 0.92166668176651\n",
            "Val Accuracy: 0.8224999904632568\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 0.2555 - accuracy: 0.9217 - auc: 0.9764 - prc: 0.9336 - val_loss: 0.4632 - val_accuracy: 0.8225 - val_auc: 0.8457 - val_prc: 0.7521\n",
            "Epoch 33/50\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 0.2594 - accuracy: 0.9225 - auc: 0.9749 - prc: 0.9327\n",
            "Epoch: 33\n",
            "Train Loss: 0.2576327621936798\n",
            "Val Loss: 0.4393765330314636\n",
            "Train Accuracy: 0.9222221970558167\n",
            "Val Accuracy: 0.8349999785423279\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2576 - accuracy: 0.9222 - auc: 0.9753 - prc: 0.9341 - val_loss: 0.4394 - val_accuracy: 0.8350 - val_auc: 0.8476 - val_prc: 0.7468\n",
            "Epoch 34/50\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 0.2492 - accuracy: 0.9227 - auc: 0.9772 - prc: 0.9365\n",
            "Epoch: 34\n",
            "Train Loss: 0.25037798285484314\n",
            "Val Loss: 0.4419829249382019\n",
            "Train Accuracy: 0.92166668176651\n",
            "Val Accuracy: 0.8299999833106995\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2504 - accuracy: 0.9217 - auc: 0.9773 - prc: 0.9378 - val_loss: 0.4420 - val_accuracy: 0.8300 - val_auc: 0.8466 - val_prc: 0.7499\n",
            "Epoch 35/50\n",
            "218/225 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9309 - auc: 0.9797 - prc: 0.9452\n",
            "Epoch: 35\n",
            "Train Loss: 0.2436356246471405\n",
            "Val Loss: 0.42174285650253296\n",
            "Train Accuracy: 0.9305555820465088\n",
            "Val Accuracy: 0.8525000214576721\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 0.2436 - accuracy: 0.9306 - auc: 0.9796 - prc: 0.9441 - val_loss: 0.4217 - val_accuracy: 0.8525 - val_auc: 0.8471 - val_prc: 0.7439\n",
            "Epoch 36/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9253 - auc: 0.9771 - prc: 0.9390\n",
            "Epoch: 36\n",
            "Train Loss: 0.25224989652633667\n",
            "Val Loss: 0.46402108669281006\n",
            "Train Accuracy: 0.925000011920929\n",
            "Val Accuracy: 0.8199999928474426\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2522 - accuracy: 0.9250 - auc: 0.9770 - prc: 0.9369 - val_loss: 0.4640 - val_accuracy: 0.8200 - val_auc: 0.8456 - val_prc: 0.7495\n",
            "Epoch 37/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9322 - auc: 0.9798 - prc: 0.9445\n",
            "Epoch: 37\n",
            "Train Loss: 0.242418572306633\n",
            "Val Loss: 0.43279215693473816\n",
            "Train Accuracy: 0.9316666722297668\n",
            "Val Accuracy: 0.8575000166893005\n",
            "225/225 [==============================] - 2s 9ms/step - loss: 0.2424 - accuracy: 0.9317 - auc: 0.9795 - prc: 0.9441 - val_loss: 0.4328 - val_accuracy: 0.8575 - val_auc: 0.8480 - val_prc: 0.7452\n",
            "Epoch 38/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.9249 - auc: 0.9796 - prc: 0.9414\n",
            "Epoch: 38\n",
            "Train Loss: 0.24395190179347992\n",
            "Val Loss: 0.442253977060318\n",
            "Train Accuracy: 0.9247221946716309\n",
            "Val Accuracy: 0.8399999737739563\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.2440 - accuracy: 0.9247 - auc: 0.9797 - prc: 0.9418 - val_loss: 0.4423 - val_accuracy: 0.8400 - val_auc: 0.8458 - val_prc: 0.7426\n",
            "Epoch 39/50\n",
            "219/225 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.9358 - auc: 0.9799 - prc: 0.9434\n",
            "Epoch: 39\n",
            "Train Loss: 0.23879139125347137\n",
            "Val Loss: 0.43319979310035706\n",
            "Train Accuracy: 0.9350000023841858\n",
            "Val Accuracy: 0.8500000238418579\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 0.2388 - accuracy: 0.9350 - auc: 0.9801 - prc: 0.9442 - val_loss: 0.4332 - val_accuracy: 0.8500 - val_auc: 0.8440 - val_prc: 0.7409\n",
            "Epoch 40/50\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 0.2491 - accuracy: 0.9311 - auc: 0.9767 - prc: 0.9389\n",
            "Epoch: 40\n",
            "Train Loss: 0.2469305545091629\n",
            "Val Loss: 0.49096739292144775\n",
            "Train Accuracy: 0.9322222471237183\n",
            "Val Accuracy: 0.8075000047683716\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2469 - accuracy: 0.9322 - auc: 0.9774 - prc: 0.9413 - val_loss: 0.4910 - val_accuracy: 0.8075 - val_auc: 0.8455 - val_prc: 0.7531\n",
            "Epoch 41/50\n",
            "219/225 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9346 - auc: 0.9797 - prc: 0.9401\n",
            "Epoch: 41\n",
            "Train Loss: 0.23592591285705566\n",
            "Val Loss: 0.4174598455429077\n",
            "Train Accuracy: 0.9350000023841858\n",
            "Val Accuracy: 0.8550000190734863\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2359 - accuracy: 0.9350 - auc: 0.9804 - prc: 0.9422 - val_loss: 0.4175 - val_accuracy: 0.8550 - val_auc: 0.8449 - val_prc: 0.7429\n",
            "Epoch 42/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9327 - auc: 0.9804 - prc: 0.9439\n",
            "Epoch: 42\n",
            "Train Loss: 0.23651717603206635\n",
            "Val Loss: 0.4580957889556885\n",
            "Train Accuracy: 0.9327777624130249\n",
            "Val Accuracy: 0.8199999928474426\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2365 - accuracy: 0.9328 - auc: 0.9806 - prc: 0.9445 - val_loss: 0.4581 - val_accuracy: 0.8200 - val_auc: 0.8443 - val_prc: 0.7481\n",
            "Epoch 43/50\n",
            "219/225 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9349 - auc: 0.9812 - prc: 0.9419\n",
            "Epoch: 43\n",
            "Train Loss: 0.23250311613082886\n",
            "Val Loss: 0.4443610906600952\n",
            "Train Accuracy: 0.9355555772781372\n",
            "Val Accuracy: 0.8374999761581421\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2325 - accuracy: 0.9356 - auc: 0.9813 - prc: 0.9424 - val_loss: 0.4444 - val_accuracy: 0.8375 - val_auc: 0.8456 - val_prc: 0.7458\n",
            "Epoch 44/50\n",
            "218/225 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9341 - auc: 0.9792 - prc: 0.9454\n",
            "Epoch: 44\n",
            "Train Loss: 0.23605239391326904\n",
            "Val Loss: 0.4260408878326416\n",
            "Train Accuracy: 0.9352777600288391\n",
            "Val Accuracy: 0.8550000190734863\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2361 - accuracy: 0.9353 - auc: 0.9797 - prc: 0.9469 - val_loss: 0.4260 - val_accuracy: 0.8550 - val_auc: 0.8457 - val_prc: 0.7483\n",
            "Epoch 45/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9367 - auc: 0.9823 - prc: 0.9513\n",
            "Epoch: 45\n",
            "Train Loss: 0.229121133685112\n",
            "Val Loss: 0.4160822331905365\n",
            "Train Accuracy: 0.9366666674613953\n",
            "Val Accuracy: 0.8650000095367432\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 0.2291 - accuracy: 0.9367 - auc: 0.9822 - prc: 0.9515 - val_loss: 0.4161 - val_accuracy: 0.8650 - val_auc: 0.8426 - val_prc: 0.7456\n",
            "Epoch 46/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9349 - auc: 0.9826 - prc: 0.9486\n",
            "Epoch: 46\n",
            "Train Loss: 0.22621852159500122\n",
            "Val Loss: 0.42560261487960815\n",
            "Train Accuracy: 0.9350000023841858\n",
            "Val Accuracy: 0.8575000166893005\n",
            "225/225 [==============================] - 2s 9ms/step - loss: 0.2262 - accuracy: 0.9350 - auc: 0.9824 - prc: 0.9481 - val_loss: 0.4256 - val_accuracy: 0.8575 - val_auc: 0.8478 - val_prc: 0.7446\n",
            "Epoch 47/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9378 - auc: 0.9824 - prc: 0.9504\n",
            "Epoch: 47\n",
            "Train Loss: 0.2260126918554306\n",
            "Val Loss: 0.4679192006587982\n",
            "Train Accuracy: 0.9377777576446533\n",
            "Val Accuracy: 0.8299999833106995\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.2260 - accuracy: 0.9378 - auc: 0.9824 - prc: 0.9504 - val_loss: 0.4679 - val_accuracy: 0.8300 - val_auc: 0.8449 - val_prc: 0.7536\n",
            "Epoch 48/50\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 0.2232 - accuracy: 0.9392 - auc: 0.9836 - prc: 0.9491\n",
            "Epoch: 48\n",
            "Train Loss: 0.22530503571033478\n",
            "Val Loss: 0.473324179649353\n",
            "Train Accuracy: 0.9386110901832581\n",
            "Val Accuracy: 0.824999988079071\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 0.2253 - accuracy: 0.9386 - auc: 0.9830 - prc: 0.9466 - val_loss: 0.4733 - val_accuracy: 0.8250 - val_auc: 0.8445 - val_prc: 0.7503\n",
            "Epoch 49/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2205 - accuracy: 0.9409 - auc: 0.9838 - prc: 0.9523\n",
            "Epoch: 49\n",
            "Train Loss: 0.22062020003795624\n",
            "Val Loss: 0.43702802062034607\n",
            "Train Accuracy: 0.9405555725097656\n",
            "Val Accuracy: 0.8500000238418579\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2206 - accuracy: 0.9406 - auc: 0.9839 - prc: 0.9527 - val_loss: 0.4370 - val_accuracy: 0.8500 - val_auc: 0.8450 - val_prc: 0.7472\n",
            "Epoch 50/50\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 0.2210 - accuracy: 0.9384 - auc: 0.9832 - prc: 0.9529\n",
            "Epoch: 50\n",
            "Train Loss: 0.2221013605594635\n",
            "Val Loss: 0.44667160511016846\n",
            "Train Accuracy: 0.9372222423553467\n",
            "Val Accuracy: 0.8374999761581421\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 0.2221 - accuracy: 0.9372 - auc: 0.9832 - prc: 0.9535 - val_loss: 0.4467 - val_accuracy: 0.8375 - val_auc: 0.8448 - val_prc: 0.7501\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.87       710\n",
            "           1       0.66      0.73      0.69       290\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.77      0.79      0.78      1000\n",
            "weighted avg       0.82      0.81      0.82      1000\n",
            "\n",
            "AUC Score: 0.8634337056823702\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50, batch_size=16,\n",
        "    validation_split=0.1,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[print_metrics_callback])\n",
        "\n",
        "# Evaluate the model on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print performance metrics\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spTpcsn-NJMp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tuner_make_model(hp,  metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential()\n",
        "  model.add(Dense(\n",
        "      hp.Choice('units', [128,256,512]),\n",
        "      activation='relu')\n",
        "  )\n",
        "  model.add(keras.layers.Dropout( 0.5))\n",
        "\n",
        "  model.add(Dense(\n",
        "      hp.Choice('units', [64,32]),\n",
        "      activation='relu')\n",
        "  )\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias))\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 4e-4,5e-5,6e-6])),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5kGJXuYwD8X",
        "outputId": "9997517d-5dd6-4ec4-fc2c-b0ed38af59ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 20 Complete [00h 04m 22s]\n",
            "val_auc: 0.8571532964706421\n",
            "\n",
            "Best val_auc So Far: 0.8624708652496338\n",
            "Total elapsed time: 03h 53m 18s\n"
          ]
        }
      ],
      "source": [
        "hp = keras_tuner.HyperParameters()\n",
        "class MyTuner(keras_tuner.tuners.RandomSearch):\n",
        "  def run_trial(self, trial, *args, **kwargs):\n",
        "\n",
        "    kwargs['batch_size'] = trial.hyperparameters.Choice('batch_size',\n",
        "                                         values=[4,8,16],\n",
        "                                         default=8)\n",
        "    return super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
        "\n",
        "\n",
        "tuner = MyTuner(\n",
        "    tuner_make_model,\n",
        "    max_trials=20,\n",
        "    # Do not resume the previous search in the same directory.\n",
        "    overwrite=False,\n",
        "    # Pick the best model based on validation accuracy\n",
        "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
        "    # Set a directory to store the intermediate results.\n",
        "    directory=\"/tuner\",\n",
        "    )\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=100, validation_split=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P8ZNUZ00nfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb71506e-2476-452d-8a34-bbc22c12a045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'units': 128, 'learning_rate': 0.0004, 'batch_size': 4}\n"
          ]
        }
      ],
      "source": [
        "best_hparams = tuner.get_best_hyperparameters()[0]\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_hparams.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGkT3mW71AYR"
      },
      "outputs": [],
      "source": [
        "128 4e-4 4"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Q9vyzTKv2fP-",
        "LkEdOk1Rh2eN",
        "PzgenptsY9PN",
        "CsAWaiNmzp5Y"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0796c2c212d54ccc9e848e9ff3571a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738bba3ebe404537b385362f78d7e406",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3b4b6337ae44db0b164ea60d7ed2411",
            "value": 466062
          }
        },
        "09980d352bcf4d21a308033c61fe5c2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b0936e874f94f919497aa828f218537": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c8a7f4101a54c49addc7cd962d85e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d6b7adc761e4453aab722bb0c572ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21dfd979d7bd487ea0b166ee368b5b53",
            "placeholder": "​",
            "style": "IPY_MODEL_854b4be29d2349fab809fd3275454a7e",
            "value": "model.safetensors: 100%"
          }
        },
        "0ed9c46b6c1f4786ab5e69a5d103fd31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7294539b9bb147df8a6dfb0aa04d46f5",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6634b0b49fe04c339fbe68c2c8befd5b",
            "value": 570
          }
        },
        "1f133239855042639d728bcafc3bccea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82e5ceafc0114e7cbdc7e0f7fd2ca336",
              "IPY_MODEL_0ed9c46b6c1f4786ab5e69a5d103fd31",
              "IPY_MODEL_3ccc00adb4be4743852b7a0c183e0060"
            ],
            "layout": "IPY_MODEL_b0014a4658d94c489ab46039292804b8"
          }
        },
        "21dfd979d7bd487ea0b166ee368b5b53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2262221614f540348ab294a9684e832e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "380956e8f1f148be9e97a0cc5d75482e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382505f254cb4b64bbcd3bf7d280aab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d6b7adc761e4453aab722bb0c572ea0",
              "IPY_MODEL_e6ffbb32c00f4ebb9efc30027b180af9",
              "IPY_MODEL_600cdae8f12b4a158f1b13c812c246b7"
            ],
            "layout": "IPY_MODEL_630647fc03bf49eaa8f17899f78d8af1"
          }
        },
        "3ccc00adb4be4743852b7a0c183e0060": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a15c832ef942f3826c3505d033d5db",
            "placeholder": "​",
            "style": "IPY_MODEL_50ab56fb09104b18abc3b70cbbe5921e",
            "value": " 570/570 [00:00&lt;00:00, 8.89kB/s]"
          }
        },
        "3e584691f5d74b928230ed45dedd9dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f044c8f32dd47b181920dce9135e13a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4398d6f0d4864739849a0b3d95033ec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_380956e8f1f148be9e97a0cc5d75482e",
            "placeholder": "​",
            "style": "IPY_MODEL_fe08731cbc564fcca8ed73ae091586ac",
            "value": "tokenizer.json: 100%"
          }
        },
        "50ab56fb09104b18abc3b70cbbe5921e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51a9d988670d4651ab2a946aeb05b2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9281b17453d649e2bacd290923615d5e",
            "placeholder": "​",
            "style": "IPY_MODEL_a35a7079c82845b48b06ccd0d3b7dd94",
            "value": " 466k/466k [00:00&lt;00:00, 8.02MB/s]"
          }
        },
        "5224feb03e664f0ab4e5390a498a656c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5840dc03b27c4fe2b19223e09a8a0546": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ae2cb39327d409e9005e7a0d6ea617f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09980d352bcf4d21a308033c61fe5c2d",
            "placeholder": "​",
            "style": "IPY_MODEL_b89a3b5281bd412386345ac062790f48",
            "value": "vocab.txt: 100%"
          }
        },
        "5d556af4e189459bacb36b1ad6728a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "600cdae8f12b4a158f1b13c812c246b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2262221614f540348ab294a9684e832e",
            "placeholder": "​",
            "style": "IPY_MODEL_0b0936e874f94f919497aa828f218537",
            "value": " 440M/440M [00:04&lt;00:00, 112MB/s]"
          }
        },
        "613cca74045246bface5bc65234e8058": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5224feb03e664f0ab4e5390a498a656c",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d556af4e189459bacb36b1ad6728a13",
            "value": 48
          }
        },
        "630647fc03bf49eaa8f17899f78d8af1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6634b0b49fe04c339fbe68c2c8befd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "691043100e434396aa0b189a4519e8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3f542935c0412e915f488e4b3b15bf",
            "placeholder": "​",
            "style": "IPY_MODEL_3e584691f5d74b928230ed45dedd9dd1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7294539b9bb147df8a6dfb0aa04d46f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732e415e0cee44b68904ca4854673021": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f044c8f32dd47b181920dce9135e13a",
            "placeholder": "​",
            "style": "IPY_MODEL_5840dc03b27c4fe2b19223e09a8a0546",
            "value": " 232k/232k [00:00&lt;00:00, 3.55MB/s]"
          }
        },
        "738bba3ebe404537b385362f78d7e406": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed9fbe5a76747e9ae84d0b6bd098df1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82e5ceafc0114e7cbdc7e0f7fd2ca336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa429cd87dd442b0aac024c33af2129c",
            "placeholder": "​",
            "style": "IPY_MODEL_0c8a7f4101a54c49addc7cd962d85e82",
            "value": "config.json: 100%"
          }
        },
        "854b4be29d2349fab809fd3275454a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9281b17453d649e2bacd290923615d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b422e7db1c4acdb1b1ea3afc4b61ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "974fbb54b0ca4b999cdc355b4411cae5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a35a7079c82845b48b06ccd0d3b7dd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4228b7fe78b485fba61f2cf750e697e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_691043100e434396aa0b189a4519e8e3",
              "IPY_MODEL_613cca74045246bface5bc65234e8058",
              "IPY_MODEL_f94a5130c0034e3b9c26a18fcf3adb95"
            ],
            "layout": "IPY_MODEL_edc224522f444cb9a6600730aa6ee887"
          }
        },
        "a72a385a1827478ab1a70933598c1601": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa429cd87dd442b0aac024c33af2129c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad233a1e243640a9b79646dfc7ac1211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4398d6f0d4864739849a0b3d95033ec9",
              "IPY_MODEL_0796c2c212d54ccc9e848e9ff3571a5c",
              "IPY_MODEL_51a9d988670d4651ab2a946aeb05b2a4"
            ],
            "layout": "IPY_MODEL_e4b69d89c80c4a2181bfad42ebb97d59"
          }
        },
        "b0014a4658d94c489ab46039292804b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b431d9b05f174c2f92a7fdfd4149c601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b89a3b5281bd412386345ac062790f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc5c3b9e152142649f95ed595c57ff33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ae2cb39327d409e9005e7a0d6ea617f",
              "IPY_MODEL_e004276eaaaf4943bdd411ecaf43d529",
              "IPY_MODEL_732e415e0cee44b68904ca4854673021"
            ],
            "layout": "IPY_MODEL_93b422e7db1c4acdb1b1ea3afc4b61ed"
          }
        },
        "bf9ed3c0ca594fd3ae3c2e982950445d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd24e3b40af1484cb40b03312c57da31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a15c832ef942f3826c3505d033d5db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e004276eaaaf4943bdd411ecaf43d529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a72a385a1827478ab1a70933598c1601",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b431d9b05f174c2f92a7fdfd4149c601",
            "value": 231508
          }
        },
        "e4b69d89c80c4a2181bfad42ebb97d59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ffbb32c00f4ebb9efc30027b180af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ed9fbe5a76747e9ae84d0b6bd098df1",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf9ed3c0ca594fd3ae3c2e982950445d",
            "value": 440449768
          }
        },
        "edc224522f444cb9a6600730aa6ee887": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b4b6337ae44db0b164ea60d7ed2411": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f94a5130c0034e3b9c26a18fcf3adb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_974fbb54b0ca4b999cdc355b4411cae5",
            "placeholder": "​",
            "style": "IPY_MODEL_cd24e3b40af1484cb40b03312c57da31",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.02kB/s]"
          }
        },
        "fe08731cbc564fcca8ed73ae091586ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff3f542935c0412e915f488e4b3b15bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}